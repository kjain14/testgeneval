Instance ID: pydata__xarray-3239-16458

Baseline 1 (Pynguin):
Predicted Test Suite: # Test cases automatically generated by Pynguin (https://www.pynguin.eu).
# Please check them before you use them.
import pytest
import xarray.core.combine as module_0
import pyarrow as module_1
import sysconfig as module_2


@pytest.mark.xfail(strict=True)
def test_case_0():
    list_0 = []
    var_0 = module_0.combine_nested(list_0, list_0, fill_value=list_0)
    assert (
        f"{type(var_0).__module__}.{type(var_0).__qualname__}"
        == "xarray.core.dataset.Dataset"
    )
    assert len(var_0) == 0
    var_0.convert_field(var_0, var_0)


@pytest.mark.xfail(strict=True)
def test_case_1():
    none_type_0 = None
    module_0.combine_nested(none_type_0, none_type_0)


@pytest.mark.xfail(strict=True)
def test_case_2():
    none_type_0 = None
    module_0.auto_combine(none_type_0)


@pytest.mark.xfail(strict=True)
def test_case_3():
    none_type_0 = None
    module_0.auto_combine(
        none_type_0,
        none_type_0,
        none_type_0,
        coords=none_type_0,
        fill_value=none_type_0,
    )


@pytest.mark.xfail(strict=True)
def test_case_4():
    none_type_0 = None
    module_0.combine_by_coords(none_type_0, none_type_0, coords=none_type_0)


@pytest.mark.xfail(strict=True)
def test_case_5():
    none_type_0 = None
    module_0.vars_as_keys(none_type_0)


@pytest.mark.xfail(strict=True)
def test_case_6():
    var_0 = module_1.get_libraries()
    module_0.combine_nested(var_0, var_0, data_vars=var_0, join=var_0)


@pytest.mark.xfail(strict=True)
def test_case_7():
    list_0 = []
    module_0.auto_combine(list_0, compat=list_0, coords=list_0, join=list_0)


@pytest.mark.xfail(strict=True)
def test_case_8():
    list_0 = []
    none_type_0 = None
    module_0.auto_combine(list_0, none_type_0, join=list_0)


def test_case_9():
    list_0 = []
    none_type_0 = None
    with pytest.raises(ValueError):
        module_0.combine_by_coords(list_0, join=none_type_0)


@pytest.mark.xfail(strict=True)
def test_case_10():
    var_0 = module_2.get_platform()
    module_0.combine_nested(var_0, var_0, data_vars=var_0, join=var_0)


@pytest.mark.xfail(strict=True)
def test_case_11():
    str_0 = "5aW<@uDKvNr"
    bool_0 = False
    str_1 = "47\nu\\+"
    dict_0 = {str_0: str_0, str_0: bool_0, str_0: str_0, str_1: str_0}
    module_0.auto_combine(dict_0, data_vars=str_0)


@pytest.mark.xfail(strict=True)
def test_case_12():
    var_0 = module_2.get_config_h_filename()
    module_0.combine_by_coords(var_0, var_0, fill_value=var_0)


@pytest.mark.xfail(strict=True)
def test_case_13():
    var_0 = module_1.get_libraries()
    module_0.auto_combine(var_0, var_0, var_0, fill_value=var_0, from_openmfds=var_0)


@pytest.mark.xfail(strict=True)
def test_case_14():
    var_0 = module_1.get_libraries()
    none_type_0 = None
    module_0.combine_nested(var_0, none_type_0)


@pytest.mark.xfail(strict=True)
def test_case_15():
    list_0 = []
    none_type_0 = None
    module_0.auto_combine(
        list_0,
        list_0,
        data_vars=list_0,
        fill_value=list_0,
        join=list_0,
        from_openmfds=none_type_0,
    )

Coverage: 62.06896551724138
Mutation Score: 1.5699999999999932

Baseline 2 (CodaMosa):
Predicted Test Suite: import xarray.core.combine as module_0
import xarray.core.dataset as module_1

def test_case_1():
    pass


def test_case_2():
    str_0 = 'H\x0b_uE7'
    list_0 = []
    var_0 = module_0.combine_nested(str_0, list_0)
    assert var_0 == 'H\x0b_uE7'


def test_case_3():
    dict_0 = {}
    var_0 = module_0.auto_combine(dict_0)
    assert len(var_0) == 0
    assert module_1.Dataset.load_store is not None
    assert module_1.Dataset.from_dataframe is not None
    assert module_1.Dataset.from_dict is not None


def test_case_4():
    str_0 = 'temperature'
    str_1 = 'x'
    var_0 = {str_0: str_0, str_0: str_1}
    int_0 = 1
    int_1 = [int_0, int_0, int_0]
    int_2 = {str_1: int_1}
    dataset_0 = module_1.Dataset(var_0, int_2)
    var_1 = {str_0: str_0, str_0: str_0}
    int_3 = [int_0]
    int_4 = {str_1: int_3}
    dataset_1 = module_1.Dataset(var_1, int_4)
    dataset_2 = [dataset_0, dataset_1]
    var_2 = module_0.combine_by_coords(dataset_2)
    assert len(dataset_0) == 1
    assert len(dataset_1) == 1
    assert len(var_2) == 1
    assert module_1.TYPE_CHECKING is False
    assert module_1.OPTIONS == {'display_width': 80, 'arithmetic_join': 'inner', 'enable_cftimeindex': True, 'file_cache_maxsize': 128, 'warn_for_unclosed_files': False, 'cmap_sequential': 'viridis', 'cmap_divergent': 'RdBu_r', 'keep_attrs': 'default'}
    assert module_1.dask_array_type == ()
    assert module_1.Dataset.load_store is not None
    assert module_1.Dataset.from_dataframe is not None
    assert module_1.Dataset.from_dict is not None


def test_case_5():
    str_0 = 'x'
    float_0 = 20.77
    float_1 = [float_0, float_0, float_0]
    var_0 = (str_0, float_1)
    var_1 = {str_0: var_0}
    str_1 = 'position'
    int_0 = 1
    int_1 = 2
    int_2 = [int_0, int_0, int_1]
    var_2 = (str_0, int_2)
    var_3 = {str_1: var_2}
    dataset_0 = module_1.Dataset(var_1, var_3)
    dataset_1 = [dataset_0, dataset_0]
    var_4 = module_0.auto_combine(dataset_1)
    assert len(dataset_0) == 0
    assert len(var_4) == 0
    assert module_1.TYPE_CHECKING is False
    assert module_1.OPTIONS == {'display_width': 80, 'arithmetic_join': 'inner', 'enable_cftimeindex': True, 'file_cache_maxsize': 128, 'warn_for_unclosed_files': False, 'cmap_sequential': 'viridis', 'cmap_divergent': 'RdBu_r', 'keep_attrs': 'default'}
    assert module_1.dask_array_type == ()
    assert module_1.Dataset.load_store is not None
    assert module_1.Dataset.from_dataframe is not None
    assert module_1.Dataset.from_dict is not None


def test_case_6():
    str_0 = 'temperature'
    str_1 = 'x'
    float_0 = 20.77
    float_1 = [float_0, float_0, float_0]
    var_0 = (str_1, float_1)
    var_1 = {str_0: var_0}
    str_2 = 'position'
    int_0 = 1
    int_1 = 2
    int_2 = [int_0, int_0, int_1]
    var_2 = (str_1, int_2)
    var_3 = {str_2: var_2}
    dataset_0 = module_1.Dataset(var_1, var_3)
    float_2 = 6.97
    var_4 = {str_0: float_2}
    int_3 = 3
    int_4 = 4
    int_5 = 5
    int_6 = [int_3, int_4, int_5]
    var_5 = (str_1, int_6)
    var_6 = {str_2: var_5}
    dataset_1 = module_1.Dataset(var_4, var_6)
    dataset_2 = [dataset_0, dataset_1]
    var_7 = module_0.auto_combine(dataset_2)
    assert len(dataset_0) == 1
    assert len(dataset_1) == 1
    assert len(var_7) == 1
    assert module_1.TYPE_CHECKING is False
    assert module_1.OPTIONS == {'display_width': 80, 'arithmetic_join': 'inner', 'enable_cftimeindex': True, 'file_cache_maxsize': 128, 'warn_for_unclosed_files': False, 'cmap_sequential': 'viridis', 'cmap_divergent': 'RdBu_r', 'keep_attrs': 'default'}
    assert module_1.dask_array_type == ()
    assert module_1.Dataset.load_store is not None
    assert module_1.Dataset.from_dataframe is not None
    assert module_1.Dataset.from_dict is not None# Automatically generated by Pynguin.


def test_case_7():
    try:
        bytes_0 = b'\x9cJmk \x1e\xcc\x8f\xadc\xa8\x00B\xa9\x8eM'
        int_0 = -522
        var_0 = module_0.combine_nested(bytes_0, int_0)
    except BaseException:
        pass


def test_case_8():
    try:
        float_0 = 1239.99988
        list_0 = [float_0]
        str_0 = '0_}'
        var_0 = module_0.combine_nested(list_0, list_0, str_0)
    except BaseException:
        pass


def test_case_9():
    try:
        str_0 = '5Qx'
        var_0 = module_0.combine_by_coords(str_0, str_0)
    except BaseException:
        pass


def test_case_10():
    try:
        bytes_0 = b'\x12#\xf922(D\xf5~\x16\xed\x14'
        dict_0 = None
        var_0 = module_0.combine_nested(bytes_0, bytes_0, dict_0)
    except BaseException:
        pass


def test_case_11():
    try:
        str_0 = 'Ek4W\nGN5+3+l'
        str_1 = 'zL{Vg,"[Jw'
        list_0 = [str_0, str_1, str_1]
        dict_0 = None
        set_0 = set()
        str_2 = 'fJ>\x0bH$#ohxg_\n'
        dict_1 = {str_2: dict_0}
        var_0 = module_0.combine_nested(list_0, dict_0, set_0, dict_1)
    except BaseException:
        pass


def test_case_12():
    try:
        float_0 = 1382.13981
        var_0 = module_0.vars_as_keys(float_0)
    except BaseException:
        pass


def test_case_13():
    try:
        list_0 = []
        str_0 = '<O8Y1,jR'
        bytes_0 = b'&\x81T\xb0\x8d\xadkm\x9f\xae'
        var_0 = module_0.combine_by_coords(list_0, list_0, str_0, bytes_0)
    except BaseException:
        pass


def test_case_14():
    try:
        int_0 = None
        str_0 = 'hWH'
        var_0 = module_0.auto_combine(int_0, str_0)
    except BaseException:
        pass


def test_case_15():
    try:
        str_0 = '\x0cLk6pj}6r>xo+cJq+'
        bool_0 = False
        var_0 = module_0.auto_combine(str_0, str_0, bool_0)
    except BaseException:
        pass


def test_case_16():
    try:
        str_0 = 'temperature'
        str_1 = 'x'
        int_0 = 34
        var_0 = (str_1, int_0)
        int_1 = 30
        int_2 = 40
        int_3 = [int_1, int_2, int_1]
        var_1 = (str_1, int_3)
        var_2 = {str_0: var_0, str_0: var_1}
        int_4 = 1
        int_5 = 2
        int_6 = [int_4, int_4, int_5]
        int_7 = {str_1: int_6}
        dataset_0 = module_1.Dataset(var_2, int_7)
        int_8 = [int_1, int_6, int_2]
        var_3 = (str_1, int_8)
        var_4 = {str_0: var_3, str_0: str_0}
        int_9 = -25
        int_10 = 4417
        int_11 = [int_9, int_4, int_10]
        int_12 = {str_1: int_11}
        dataset_1 = module_1.Dataset(var_4, int_12)
        dataset_2 = [dataset_0, dataset_1]
        iterator_0 = dataset_2.__iter__()
        hashable_0 = None
        dict_0 = {str_0: hashable_0, str_1: hashable_0, str_0: hashable_0, str_0: hashable_0, str_1: hashable_0}
        var_5 = module_0.combine_by_coords(iterator_0, dict_0)
    except BaseException:
        pass


def test_case_17():
    try:
        str_0 = "\x0b't"
        bytes_0 = b'~\xf6}\x12\xf1\xd2\xc5m\xc8'
        bool_0 = None
        list_0 = [bool_0, bytes_0, str_0]
        tuple_0 = (list_0,)
        str_1 = 'zMn]\n}OW#VZm'
        var_0 = module_0.combine_nested(tuple_0, str_1, list_0, tuple_0)
    except BaseException:
        pass


def test_case_18():
    try:
        str_0 = ' C]0Q-x'
        var_0 = module_0.vars_as_keys(str_0)
        dict_0 = {}
        str_1 = 'JHn'
        list_0 = [var_0, str_0, var_0, dict_0]
        hashable_0 = None
        dict_1 = {str_1: list_0, str_0: hashable_0}
        list_1 = [str_1]
        var_1 = module_0.auto_combine(dict_0, dict_1, list_1)
    except BaseException:
        pass


def test_case_19():
    try:
        bytes_0 = b'\xdfka\xcfr\xe2r\xa2'
        var_0 = module_0.vars_as_keys(bytes_0)
        float_0 = -606.35255
        tuple_0 = ()
        bool_0 = True
        var_1 = module_0.combine_nested(tuple_0, bool_0, float_0)
        assert var_0 == (97, 107, 114, 114, 162, 207, 223, 226)
        assert len(var_1) == 0
        assert module_1.Dataset.load_store is not None
        assert module_1.Dataset.from_dataframe is not None
        assert module_1.Dataset.from_dict is not None
        str_0 = 'M\\<\x0c;z|{$h'
        int_0 = -3526
        var_2 = module_0.auto_combine(str_0, int_0)
    except BaseException:
        pass


def test_case_20():
    try:
        bytes_0 = b'\x1d\xcf\xf59\xce\xcfN2.z\xa0\xa2a\xa8\xc5\xcc\xeb'
        list_0 = [bytes_0, bytes_0, bytes_0, bytes_0]
        var_0 = module_0.vars_as_keys(list_0)
        bytes_1 = b'\xe6\x080\x0e'
        list_1 = [bytes_1, bytes_0, list_0]
        dict_0 = {bytes_1: bytes_1, bytes_1: list_1}
        var_1 = module_0.combine_nested(list_1, dict_0, bytes_1)
    except BaseException:
        pass


def test_case_21():
    try:
        bytes_0 = b'\xec\xb1\xdd'
        list_0 = [bytes_0, bytes_0, bytes_0, bytes_0]
        var_0 = module_0.vars_as_keys(list_0)
        dict_0 = {}
        int_0 = None
        bytes_1 = b'y\x13*N7O\xbb\x9an:4\xad\xe3\xb1\xaa\xe7'
        var_1 = module_0.auto_combine(dict_0, int_0, bytes_1)
    except BaseException:
        pass


def test_case_22():
    try:
        str_0 = 'temperature'
        str_1 = 'x'
        int_0 = 14
        var_0 = {str_0: str_0, str_0: str_1}
        int_1 = [int_0, int_0, int_0]
        int_2 = {str_1: int_1}
        dataset_0 = module_1.Dataset(var_0, int_2)
        var_1 = {str_0: str_0, str_0: str_0}
        int_3 = [int_2, int_2, int_1]
        int_4 = {str_1: int_3}
        dataset_1 = module_1.Dataset(var_1, int_4)
        dataset_2 = [dataset_0, dataset_1]
        var_2 = module_0.combine_by_coords(dataset_2)
    except BaseException:
        pass


def test_case_23():
    try:
        set_0 = set()
        str_0 = 'SC":'
        var_0 = module_0.auto_combine(set_0, str_0)
        assert len(var_0) == 0
        assert module_1.Dataset.load_store is not None
        assert module_1.Dataset.from_dataframe is not None
        assert module_1.Dataset.from_dict is not None
        list_0 = [var_0]
        var_1 = module_0.auto_combine(list_0, str_0)
        assert len(var_1) == 0
        assert module_1.TYPE_CHECKING is False
        assert module_1.OPTIONS == {'display_width': 80, 'arithmetic_join': 'inner', 'enable_cftimeindex': True, 'file_cache_maxsize': 128, 'warn_for_unclosed_files': False, 'cmap_sequential': 'viridis', 'cmap_divergent': 'RdBu_r', 'keep_attrs': 'default'}
        assert module_1.dask_array_type == ()
        float_0 = -2765.2299
        var_2 = module_0.combine_by_coords(float_0, float_0)
    except BaseException:
        pass


def test_case_24():
    try:
        str_0 = 'temperature'
        str_1 = 'humidity'
        str_2 = 'x'
        int_0 = 15
        int_1 = 20
        int_2 = 25
        int_3 = [int_0, int_1, int_2]
        var_0 = (str_2, int_3)
        int_4 = 30
        int_5 = 40
        int_6 = 50
        int_7 = [int_4, int_5, int_6]
        var_1 = (str_2, int_7)
        var_2 = {str_0: var_0, str_1: var_1}
        int_8 = 0
        int_9 = 1
        int_10 = 2
        int_11 = [int_8, int_9, int_10]
        int_12 = {str_2: int_11}
        dataset_0 = module_1.Dataset(var_2, int_12)
        int_13 = 35
        int_14 = [int_4, int_13, int_5]
        var_3 = (str_2, int_14)
        int_15 = 52
        int_16 = 53
        int_17 = 80
        int_18 = [int_15, int_16, int_17]
        var_4 = (str_2, int_18)
        var_5 = {str_0: var_3, str_1: var_4}
        int_19 = {}
        dataset_1 = module_1.Dataset(var_5, int_19)
        dataset_2 = [dataset_0, dataset_1]
        var_6 = module_0.combine_by_coords(dataset_2)
    except BaseException:
        pass


def test_case_25():
    try:
        str_0 = 'temperature'
        str_1 = 'x'
        var_0 = {str_0: str_0, str_0: str_1}
        int_0 = 1
        int_1 = [int_0, int_0, int_0]
        int_2 = {str_1: int_1}
        dataset_0 = module_1.Dataset(var_0, int_2)
        var_1 = {str_0: str_0, str_0: str_0}
        int_3 = -25
        int_4 = 5
        int_5 = [int_3, int_0, int_4]
        int_6 = {str_1: int_5}
        dataset_1 = module_1.Dataset(var_1, int_6)
        dataset_2 = [dataset_0, dataset_1]
        var_2 = module_0.combine_by_coords(dataset_2)
    except BaseException:
        pass


def test_case_26():
    try:
        str_0 = 'temperature'
        float_0 = 11.04
        float_1 = 23.57
        float_2 = [float_0, float_1, float_0]
        var_0 = (str_0, float_2)
        var_1 = {str_0: var_0}
        str_1 = 'Hho+ition'
        var_2 = {str_1: float_2}
        dataset_0 = module_1.Dataset(var_1, var_2)
        float_3 = 8.13
        float_4 = 7.42
        float_5 = [float_2, float_3, float_4]
        var_3 = (str_0, float_5)
        var_4 = {str_0: var_3}
        int_0 = 3
        int_1 = 4
        int_2 = 5
        int_3 = [float_0, int_0, int_1, var_0, int_2]
        var_5 = {str_1: int_3}
        dataset_1 = module_1.Dataset(var_4, var_5)
        dataset_2 = [dataset_0, dataset_1]
        var_6 = module_0.auto_combine(dataset_2)
    except BaseException:
        pass


def test_case_27():
    try:
        str_0 = 'temperature'
        float_0 = 37.31685932463914
        str_1 = 'Hho+ition'
        float_1 = 8.13
        float_2 = [float_0, str_0, float_1, float_0]
        var_0 = (str_0, float_2)
        var_1 = {str_0: var_0}
        int_0 = 3
        int_1 = 4
        int_2 = 5
        int_3 = [int_0, int_1, int_2]
        var_2 = {str_1: int_3}
        dataset_0 = module_1.Dataset(var_1, var_2)
        dataset_1 = [dataset_0, dataset_0]
        var_3 = module_0.auto_combine(dataset_1)
    except BaseException:
        pass


def test_case_28():
    try:
        str_0 = 'temperature'
        float_0 = 11.04
        float_1 = 23.57
        float_2 = 20.77
        float_3 = [float_0, float_1, float_2]
        var_0 = (str_0, float_3)
        var_1 = {str_0: var_0}
        str_1 = 'position'
        int_0 = 0
        int_1 = 1
        int_2 = 2
        int_3 = [int_0, int_1, int_2]
        var_2 = (str_1, int_3)
        var_3 = {str_1: var_2}
        set_0 = set()
        dataset_0 = None
        str_2 = 'ALrA/\r)P5Viu.E]D'
        hashable_0 = None
        str_3 = ";2F@#\r= '0SEvy"
        dict_0 = {str_2: hashable_0, str_3: hashable_0, str_2: hashable_0}
        var_4 = module_0.combine_nested(set_0, dataset_0, dict_0)
        assert len(var_4) == 0
        assert module_1.Dataset.load_store is not None
        assert module_1.Dataset.from_dataframe is not None
        assert module_1.Dataset.from_dict is not None
        dataset_1 = module_1.Dataset(var_1, var_3)
        assert len(dataset_1) == 0
        assert module_1.TYPE_CHECKING is False
        assert module_1.OPTIONS == {'display_width': 80, 'arithmetic_join': 'inner', 'enable_cftimeindex': True, 'file_cache_maxsize': 128, 'warn_for_unclosed_files': False, 'cmap_sequential': 'viridis', 'cmap_divergent': 'RdBu_r', 'keep_attrs': 'default'}
        assert module_1.dask_array_type == ()
        float_4 = 6.97
        float_5 = 8.13
        float_6 = 7.42
        float_7 = [float_4, float_5, float_6]
        var_5 = (str_3, float_7)
        var_6 = {str_0: var_5}
        int_4 = 3
        int_5 = 4
        int_6 = 5
        int_7 = [int_4, int_5, int_6]
        var_7 = (str_1, int_7)
        var_8 = {str_1: var_7}
        dataset_2 = module_1.Dataset(var_6, var_8)
        assert len(dataset_2) == 1
        dataset_3 = [dataset_1, dataset_2]
        var_9 = module_0.auto_combine(dataset_3)
    except BaseException:
        pass


def test_case_29():
    try:
        str_0 = 'temperature'
        str_1 = 'x'
        var_0 = {str_0: str_0, str_0: str_1}
        int_0 = 1
        int_1 = [int_0, int_0, int_0]
        int_2 = {str_1: int_1}
        dataset_0 = module_1.Dataset(var_0, int_2)
        var_1 = {str_0: str_0, str_0: str_0}
        int_3 = []
        int_4 = {str_1: int_3}
        dataset_1 = module_1.Dataset(var_1, int_4)
        dataset_2 = [dataset_0, dataset_1]
        var_2 = module_0.combine_by_coords(dataset_2)
    except BaseException:
        pass

Coverage: 91.37931034482759
Mutation Score: 9.060000000000002

Baseline 3 (GPT-4o):
Predicted Test Suite: 
import pytest
import pandas as pd
from collections import OrderedDict
from xarray import DataArray, Dataset, concat, merge
from xarray.core.combine import (
    _infer_concat_order_from_positions,
    _infer_tile_ids_from_nested_list,
    _infer_concat_order_from_coords,
    _check_shape_tile_ids,
    _combine_nd,
    _combine_all_along_first_dim,
    _combine_1d,
    _new_tile_id,
    _nested_combine,
    combine_nested,
    combine_by_coords,
    auto_combine,
)

@pytest.fixture
def sample_datasets():
    ds1 = Dataset({"var": ("x", [1, 2])}, coords={"x": [0, 1]})
    ds2 = Dataset({"var": ("x", [3, 4])}, coords={"x": [2, 3]})
    return [ds1, ds2]

def test_infer_concat_order_from_positions():
    datasets = [[1, 2], [3, 4]]
    result = _infer_concat_order_from_positions(datasets)
    expected = OrderedDict({(0, 0): 1, (0, 1): 2, (1, 0): 3, (1, 1): 4})
    assert result == expected

def test_infer_tile_ids_from_nested_list():
    entry = [[1, 2], [3, 4]]
    result = list(_infer_tile_ids_from_nested_list(entry, ()))
    expected = [((0, 0), 1), ((0, 1), 2), ((1, 0), 3), ((1, 1), 4)]
    assert result == expected

def test_infer_concat_order_from_coords(sample_datasets):
    result, concat_dims = _infer_concat_order_from_coords(sample_datasets)
    expected = OrderedDict({(0,): sample_datasets[0], (1,): sample_datasets[1]})
    assert result == expected
    assert concat_dims == ["x"]

def test_check_shape_tile_ids():
    combined_tile_ids = OrderedDict({(0, 0): 1, (0, 1): 2, (1, 0): 3, (1, 1): 4})
    # Should not raise an error
    _check_shape_tile_ids(combined_tile_ids)

    with pytest.raises(ValueError):
        combined_tile_ids = OrderedDict({(0,): 1, (0, 1): 2, (1, 0): 3})
        _check_shape_tile_ids(combined_tile_ids)

def test_combine_nd(sample_datasets):
    combined_ids = OrderedDict({(0,): sample_datasets[0], (1,): sample_datasets[1]})
    concat_dims = ["x"]
    result = _combine_nd(combined_ids, concat_dims)
    expected = concat(sample_datasets, dim="x")
    assert result.equals(expected)

def test_combine_all_along_first_dim(sample_datasets):
    combined_ids = OrderedDict({(0,): sample_datasets[0], (1,): sample_datasets[1]})
    result = _combine_all_along_first_dim(combined_ids, "x", "all", "different", "no_conflicts")
    expected = {(): concat(sample_datasets, dim="x")}
    assert list(result.values())[0].equals(list(expected.values())[0])

def test_combine_1d(sample_datasets):
    result = _combine_1d(sample_datasets, "x")
    expected = concat(sample_datasets, dim="x")
    assert result.equals(expected)

def test_new_tile_id():
    single_id_ds_pair = ((0, 1), "dataset")
    result = _new_tile_id(single_id_ds_pair)
    expected = (1,)
    assert result == expected

def test_nested_combine(sample_datasets):
    result = _nested_combine([sample_datasets], ["x"], "no_conflicts", "all", "different", ids=False)
    expected = concat(sample_datasets, dim="x")
    assert result.equals(expected)

def test_combine_nested(sample_datasets):
    result = combine_nested([sample_datasets], concat_dim="x")
    expected = concat(sample_datasets, dim="x")
    assert result.equals(expected)

def test_combine_by_coords(sample_datasets):
    result = combine_by_coords(sample_datasets)
    expected = concat(sample_datasets, dim="x")
    assert result.equals(expected)

def test_auto_combine(sample_datasets):
    result = auto_combine(sample_datasets, concat_dim="x")
    expected = concat(sample_datasets, dim="x")
    assert result.equals(expected)

Coverage: 72.41379310344827
Mutation Score: 31.5

Approach:
Predicted Test Suite: import pytest
import pandas as pd
from collections import OrderedDict
from xarray import DataArray, Dataset, concat, merge
from xarray.core.combine import (
    _infer_concat_order_from_positions,
    _infer_tile_ids_from_nested_list,
    _infer_concat_order_from_coords,
    _check_shape_tile_ids,
    _combine_nd,
    _combine_all_along_first_dim,
    _combine_1d,
    _new_tile_id,
    _nested_combine,
    combine_nested,
    combine_by_coords,
    auto_combine,
)

@pytest.fixture
def sample_datasets():
    ds1 = Dataset({"var": ("x", [1, 2])}, coords={"x": [0, 1]})
    ds2 = Dataset({"var": ("x", [3, 4])}, coords={"x": [2, 3]})
    return [ds1, ds2]

def test_infer_concat_order_from_positions():
    datasets = [[1, 2], [3, 4]]
    result = _infer_concat_order_from_positions(datasets)
    expected = OrderedDict({(0, 0): 1, (0, 1): 2, (1, 0): 3, (1, 1): 4})
    assert result == expected

def test_infer_tile_ids_from_nested_list():
    entry = [[1, 2], [3, 4]]
    result = list(_infer_tile_ids_from_nested_list(entry, ()))
    expected = [((0, 0), 1), ((0, 1), 2), ((1, 0), 3), ((1, 1), 4)]
    assert result == expected

def test_infer_concat_order_from_coords(sample_datasets):
    result, concat_dims = _infer_concat_order_from_coords(sample_datasets)
    expected = OrderedDict({(0,): sample_datasets[0], (1,): sample_datasets[1]})
    assert result == expected
    assert concat_dims == ["x"]

def test_check_shape_tile_ids():
    combined_tile_ids = OrderedDict({(0, 0): 1, (0, 1): 2, (1, 0): 3, (1, 1): 4})
    # Should not raise an error
    _check_shape_tile_ids(combined_tile_ids)

    with pytest.raises(ValueError):
        combined_tile_ids = OrderedDict({(0,): 1, (0, 1): 2, (1, 0): 3})
        _check_shape_tile_ids(combined_tile_ids)

def test_combine_nd(sample_datasets):
    combined_ids = OrderedDict({(0,): sample_datasets[0], (1,): sample_datasets[1]})
    concat_dims = ["x"]
    result = _combine_nd(combined_ids, concat_dims)
    expected = concat(sample_datasets, dim="x")
    assert result.equals(expected)

def test_combine_all_along_first_dim(sample_datasets):
    combined_ids = OrderedDict({(0,): sample_datasets[0], (1,): sample_datasets[1]})
    result = _combine_all_along_first_dim(combined_ids, "x", "all", "different", "no_conflicts")
    expected = {(): concat(sample_datasets, dim="x")}
    assert list(result.values())[0].equals(list(expected.values())[0])

def test_combine_1d(sample_datasets):
    result = _combine_1d(sample_datasets, "x")
    expected = concat(sample_datasets, dim="x")
    assert result.equals(expected)

def test_new_tile_id():
    single_id_ds_pair = ((0, 1), "dataset")
    result = _new_tile_id(single_id_ds_pair)
    expected = (1,)
    assert result == expected

def test_nested_combine(sample_datasets):
    result = _nested_combine([sample_datasets], ["x", None], "no_conflicts", "all", "different", ids=False)
    expected = concat(sample_datasets, dim="x")
    assert result.equals(expected)

def test_combine_nested(sample_datasets):
    result = combine_nested([sample_datasets], concat_dim=["x", None])
    expected = concat(sample_datasets, dim="x")
    assert result.equals(expected)

def test_combine_by_coords(sample_datasets):
    result = combine_by_coords(sample_datasets)
    expected = concat(sample_datasets, dim="x")
    assert result.equals(expected)

def test_auto_combine(sample_datasets):
    result = auto_combine(sample_datasets, concat_dim="x")
    expected = concat(sample_datasets, dim="x")
    assert result.equals(expected)
Coverage: 77.58620689655173
Mutation Score: 36.22
Output: On branch main
Untracked files:
  (use "git add <file>..." to include in what will be committed)
        xarray/tests/test_combine.py

nothing added to commit but untracked files present (use "git add" to track)
commit 2c8ba46b3dc47f2c4e488d40d4d5a855ddefc99d
Author: TestGenEval <>
Date:   Wed Dec 11 12:05:16 2024 +0000

    Testing fixes

diff --git a/xarray/backends/api.py b/xarray/backends/api.py
index a20d3c2a..1f0869cf 100644
--- a/xarray/backends/api.py
+++ b/xarray/backends/api.py
@@ -761,7 +761,7 @@ def open_mfdataset(
         `xarray.auto_combine` is used, but in the future this behavior will
         switch to use `xarray.combine_by_coords` by default.
     compat : {'identical', 'equals', 'broadcast_equals',
-              'no_conflicts'}, optional
+              'no_conflicts', 'override'}, optional
         String indicating how to compare variables of the same name for
         potential conflicts when merging:
          * 'broadcast_equals': all values must be equal when variables are
@@ -772,6 +772,7 @@ def open_mfdataset(
          * 'no_conflicts': only values which are not null in both datasets
            must be equal. The returned dataset then contains the combination
            of all non-null values.
+         * 'override': skip comparing and pick variable from first dataset
     preprocess : callable, optional
         If provided, call this function on each dataset prior to concatenation.
         You can find the file-name from which each dataset was loaded in
diff --git a/xarray/core/combine.py b/xarray/core/combine.py
index c24be88b..e35bb51e 100644
--- a/xarray/core/combine.py
+++ b/xarray/core/combine.py
@@ -243,6 +243,7 @@ def _combine_1d(
                 dim=concat_dim,
                 data_vars=data_vars,
                 coords=coords,
+                compat=compat,
                 fill_value=fill_value,
                 join=join,
             )
@@ -351,7 +352,7 @@ def combine_nested(
         Must be the same length as the depth of the list passed to
         ``datasets``.
     compat : {'identical', 'equals', 'broadcast_equals',
-              'no_conflicts'}, optional
+              'no_conflicts', 'override'}, optional
         String indicating how to compare variables of the same name for
         potential merge conflicts:

@@ -363,6 +364,7 @@ def combine_nested(
         - 'no_conflicts': only values which are not null in both datasets
           must be equal. The returned dataset then contains the combination
           of all non-null values.
+        - 'override': skip comparing and pick variable from first dataset
     data_vars : {'minimal', 'different', 'all' or list of str}, optional
         Details are in the documentation of concat
     coords : {'minimal', 'different', 'all' or list of str}, optional
@@ -504,7 +506,7 @@ def combine_by_coords(
     datasets : sequence of xarray.Dataset
         Dataset objects to combine.
     compat : {'identical', 'equals', 'broadcast_equals',
-              'no_conflicts'}, optional
+              'no_conflicts', 'override'}, optional
         String indicating how to compare variables of the same name for
         potential conflicts:

@@ -516,6 +518,7 @@ def combine_by_coords(
         - 'no_conflicts': only values which are not null in both datasets
           must be equal. The returned dataset then contains the combination
           of all non-null values.
+        - 'override': skip comparing and pick variable from first dataset
     data_vars : {'minimal', 'different', 'all' or list of str}, optional
         Details are in the documentation of concat
     coords : {'minimal', 'different', 'all' or list of str}, optional
@@ -598,6 +601,7 @@ def combine_by_coords(
             concat_dims=concat_dims,
             data_vars=data_vars,
             coords=coords,
+            compat=compat,
             fill_value=fill_value,
             join=join,
         )
@@ -667,7 +671,7 @@ def auto_combine(
         component files. Set ``concat_dim=None`` explicitly to disable
         concatenation.
     compat : {'identical', 'equals', 'broadcast_equals',
-             'no_conflicts'}, optional
+             'no_conflicts', 'override'}, optional
         String indicating how to compare variables of the same name for
         potential conflicts:
         - 'broadcast_equals': all values must be equal when variables are
@@ -678,6 +682,7 @@ def auto_combine(
         - 'no_conflicts': only values which are not null in both datasets
           must be equal. The returned dataset then contains the combination
           of all non-null values.
+        - 'override': skip comparing and pick variable from first dataset
     data_vars : {'minimal', 'different', 'all' or list of str}, optional
         Details are in the documentation of concat
     coords : {'minimal', 'different', 'all' o list of str}, optional
@@ -832,6 +837,7 @@ def _old_auto_combine(
                 dim=dim,
                 data_vars=data_vars,
                 coords=coords,
+                compat=compat,
                 fill_value=fill_value,
                 join=join,
             )
@@ -850,6 +856,7 @@ def _auto_concat(
     coords="different",
     fill_value=dtypes.NA,
     join="outer",
+    compat="no_conflicts",
 ):
     if len(datasets) == 1 and dim is None:
         # There is nothing more to combine, so kick out early.
@@ -876,5 +883,10 @@ def _auto_concat(
                 )
             dim, = concat_dims
         return concat(
-            datasets, dim=dim, data_vars=data_vars, coords=coords, fill_value=fill_value
+            datasets,
+            dim=dim,
+            data_vars=data_vars,
+            coords=coords,
+            fill_value=fill_value,
+            compat=compat,
         )
diff --git a/xarray/core/concat.py b/xarray/core/concat.py
index d5dfa49a..e68c247d 100644
--- a/xarray/core/concat.py
+++ b/xarray/core/concat.py
@@ -4,6 +4,7 @@ import pandas as pd

 from . import dtypes, utils
 from .alignment import align
+from .merge import unique_variable, _VALID_COMPAT
 from .variable import IndexVariable, Variable, as_variable
 from .variable import concat as concat_vars

@@ -59,12 +60,19 @@ def concat(
             those corresponding to other dimensions.
           * list of str: The listed coordinate variables will be concatenated,
             in addition to the 'minimal' coordinates.
-    compat : {'equals', 'identical'}, optional
-        String indicating how to compare non-concatenated variables and
-        dataset global attributes for potential conflicts. 'equals' means
-        that all variable values and dimensions must be the same;
-        'identical' means that variable attributes and global attributes
-        must also be equal.
+    compat : {'identical', 'equals', 'broadcast_equals', 'no_conflicts', 'override'}, optional
+        String indicating how to compare non-concatenated variables of the same name for
+        potential conflicts. This is passed down to merge.
+
+        - 'broadcast_equals': all values must be equal when variables are
+          broadcast against each other to ensure common dimensions.
+        - 'equals': all values and dimensions must be the same.
+        - 'identical': all values, dimensions and attributes must be the
+          same.
+        - 'no_conflicts': only values which are not null in both datasets
+          must be equal. The returned dataset then contains the combination
+          of all non-null values.
+        - 'override': skip comparing and pick variable from first dataset
     positions : None or list of integer arrays, optional
         List of integer arrays which specifies the integer positions to which
         to assign each dataset along the concatenated dimension. If not
@@ -107,6 +115,12 @@ def concat(
     except StopIteration:
         raise ValueError("must supply at least one object to concatenate")

+    if compat not in _VALID_COMPAT:
+        raise ValueError(
+            "compat=%r invalid: must be 'broadcast_equals', 'equals', 'identical', 'no_conflicts' or 'override'"
+            % compat
+        )
+
     if isinstance(first_obj, DataArray):
         f = _dataarray_concat
     elif isinstance(first_obj, Dataset):
@@ -143,23 +157,39 @@ def _calc_concat_dim_coord(dim):
     return dim, coord


-def _calc_concat_over(datasets, dim, data_vars, coords):
+def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):
     """
     Determine which dataset variables need to be concatenated in the result,
-    and which can simply be taken from the first dataset.
     """
     # Return values
     concat_over = set()
     equals = {}

-    if dim in datasets[0]:
+    if dim in dim_names:
+        concat_over_existing_dim = True
         concat_over.add(dim)
+    else:
+        concat_over_existing_dim = False
+
+    concat_dim_lengths = []
     for ds in datasets:
+        if concat_over_existing_dim:
+            if dim not in ds.dims:
+                if dim in ds:
+                    ds = ds.set_coords(dim)
+                else:
+                    raise ValueError("%r is not present in all datasets" % dim)
         concat_over.update(k for k, v in ds.variables.items() if dim in v.dims)
+        concat_dim_lengths.append(ds.dims.get(dim, 1))

     def process_subset_opt(opt, subset):
         if isinstance(opt, str):
             if opt == "different":
+                if compat == "override":
+                    raise ValueError(
+                        "Cannot specify both %s='different' and compat='override'."
+                        % subset
+                    )
                 # all nonindexes that are not the same in each dataset
                 for k in getattr(datasets[0], subset):
                     if k not in concat_over:
@@ -173,7 +203,7 @@ def _calc_concat_over(datasets, dim, data_vars, coords):
                         for ds_rhs in datasets[1:]:
                             v_rhs = ds_rhs.variables[k].compute()
                             computed.append(v_rhs)
-                            if not v_lhs.equals(v_rhs):
+                            if not getattr(v_lhs, compat)(v_rhs):
                                 concat_over.add(k)
                                 equals[k] = False
                                 # computed variables are not to be re-computed
@@ -209,7 +239,29 @@ def _calc_concat_over(datasets, dim, data_vars, coords):

     process_subset_opt(data_vars, "data_vars")
     process_subset_opt(coords, "coords")
-    return concat_over, equals
+    return concat_over, equals, concat_dim_lengths
+
+
+# determine dimensional coordinate names and a dict mapping name to DataArray
+def _parse_datasets(datasets):
+
+    dims = set()
+    all_coord_names = set()
+    data_vars = set()  # list of data_vars
+    dim_coords = dict()  # maps dim name to variable
+    dims_sizes = {}  # shared dimension sizes to expand variables
+
+    for ds in datasets:
+        dims_sizes.update(ds.dims)
+        all_coord_names.update(ds.coords)
+        data_vars.update(ds.data_vars)
+
+        for dim in set(ds.dims) - dims:
+            if dim not in dim_coords:
+                dim_coords[dim] = ds.coords[dim].variable
+        dims = dims | set(ds.dims)
+
+    return dim_coords, dims_sizes, all_coord_names, data_vars


 def _dataset_concat(
@@ -227,11 +279,6 @@ def _dataset_concat(
     """
     from .dataset import Dataset

-    if compat not in ["equals", "identical"]:
-        raise ValueError(
-            "compat=%r invalid: must be 'equals' " "or 'identical'" % compat
-        )
-
     dim, coord = _calc_concat_dim_coord(dim)
     # Make sure we're working on a copy (we'll be loading variables)
     datasets = [ds.copy() for ds in datasets]
@@ -239,62 +286,65 @@ def _dataset_concat(
         *datasets, join=join, copy=False, exclude=[dim], fill_value=fill_value
     )

-    concat_over, equals = _calc_concat_over(datasets, dim, data_vars, coords)
+    dim_coords, dims_sizes, coord_names, data_names = _parse_datasets(datasets)
+    dim_names = set(dim_coords)
+    unlabeled_dims = dim_names - coord_names
+
+    both_data_and_coords = coord_names & data_names
+    if both_data_and_coords:
+        raise ValueError(
+            "%r is a coordinate in some datasets but not others." % both_data_and_coords
+        )
+    # we don't want the concat dimension in the result dataset yet
+    dim_coords.pop(dim, None)
+    dims_sizes.pop(dim, None)
+
+    # case where concat dimension is a coordinate or data_var but not a dimension
+    if (dim in coord_names or dim in data_names) and dim not in dim_names:
+        datasets = [ds.expand_dims(dim) for ds in datasets]
+
+    # determine which variables to concatentate
+    concat_over, equals, concat_dim_lengths = _calc_concat_over(
+        datasets, dim, dim_names, data_vars, coords, compat
+    )
+
+    # determine which variables to merge, and then merge them according to compat
+    variables_to_merge = (coord_names | data_names) - concat_over - dim_names
+
+    result_vars = {}
+    if variables_to_merge:
+        to_merge = {var: [] for var in variables_to_merge}
+
+        for ds in datasets:
+            absent_merge_vars = variables_to_merge - set(ds.variables)
+            if absent_merge_vars:
+                raise ValueError(
+                    "variables %r are present in some datasets but not others. "
+                    % absent_merge_vars
+                )

-    def insert_result_variable(k, v):
-        assert isinstance(v, Variable)
-        if k in datasets[0].coords:
-            result_coord_names.add(k)
-        result_vars[k] = v
+            for var in variables_to_merge:
+                to_merge[var].append(ds.variables[var])

-    # create the new dataset and add constant variables
-    result_vars = OrderedDict()
-    result_coord_names = set(datasets[0].coords)
+        for var in variables_to_merge:
+            result_vars[var] = unique_variable(
+                var, to_merge[var], compat=compat, equals=equals.get(var, None)
+            )
+    else:
+        result_vars = OrderedDict()
+    result_vars.update(dim_coords)
+
+    # assign attrs and encoding from first dataset
     result_attrs = datasets[0].attrs
     result_encoding = datasets[0].encoding

-    for k, v in datasets[0].variables.items():
-        if k not in concat_over:
-            insert_result_variable(k, v)
-
-    # check that global attributes and non-concatenated variables are fixed
-    # across all datasets
+    # check that global attributes are fixed across all datasets if necessary
     for ds in datasets[1:]:
         if compat == "identical" and not utils.dict_equiv(ds.attrs, result_attrs):
-            raise ValueError("dataset global attributes not equal")
-        for k, v in ds.variables.items():
-            if k not in result_vars and k not in concat_over:
-                raise ValueError("encountered unexpected variable %r" % k)
-            elif (k in result_coord_names) != (k in ds.coords):
-                raise ValueError(
-                    "%r is a coordinate in some datasets but not " "others" % k
-                )
-            elif k in result_vars and k != dim:
-                # Don't use Variable.identical as it internally invokes
-                # Variable.equals, and we may already know the answer
-                if compat == "identical" and not utils.dict_equiv(
-                    v.attrs, result_vars[k].attrs
-                ):
-                    raise ValueError("variable %s not identical across datasets" % k)
-
-                # Proceed with equals()
-                try:
-                    # May be populated when using the "different" method
-                    is_equal = equals[k]
-                except KeyError:
-                    result_vars[k].load()
-                    is_equal = v.equals(result_vars[k])
-                if not is_equal:
-                    raise ValueError("variable %s not equal across datasets" % k)
+            raise ValueError("Dataset global attributes not equal.")

     # we've already verified everything is consistent; now, calculate
     # shared dimension sizes so we can expand the necessary variables
-    dim_lengths = [ds.dims.get(dim, 1) for ds in datasets]
-    non_concat_dims = {}
-    for ds in datasets:
-        non_concat_dims.update(ds.dims)
-    non_concat_dims.pop(dim, None)
-
     def ensure_common_dims(vars):
         # ensure each variable with the given name shares the same
         # dimensions and the same shape for all of them except along the
@@ -302,25 +352,27 @@ def _dataset_concat(
         common_dims = tuple(pd.unique([d for v in vars for d in v.dims]))
         if dim not in common_dims:
             common_dims = (dim,) + common_dims
-        for var, dim_len in zip(vars, dim_lengths):
+        for var, dim_len in zip(vars, concat_dim_lengths):
             if var.dims != common_dims:
-                common_shape = tuple(
-                    non_concat_dims.get(d, dim_len) for d in common_dims
-                )
+                common_shape = tuple(dims_sizes.get(d, dim_len) for d in common_dims)
                 var = var.set_dims(common_dims, common_shape)
             yield var

     # stack up each variable to fill-out the dataset (in order)
+    # n.b. this loop preserves variable order, needed for groupby.
     for k in datasets[0].variables:
         if k in concat_over:
             vars = ensure_common_dims([ds.variables[k] for ds in datasets])
             combined = concat_vars(vars, dim, positions)
-            insert_result_variable(k, combined)
+            assert isinstance(combined, Variable)
+            result_vars[k] = combined

     result = Dataset(result_vars, attrs=result_attrs)
-    result = result.set_coords(result_coord_names)
+    result = result.set_coords(coord_names)
     result.encoding = result_encoding

+    result = result.drop(unlabeled_dims, errors="ignore")
+
     if coord is not None:
         # add concat dimension last to ensure that its in the final Dataset
         result[coord.name] = coord
@@ -342,7 +394,7 @@ def _dataarray_concat(

     if data_vars != "all":
         raise ValueError(
-            "data_vars is not a valid argument when " "concatenating DataArray objects"
+            "data_vars is not a valid argument when concatenating DataArray objects"
         )

     datasets = []
diff --git a/xarray/core/dataarray.py b/xarray/core/dataarray.py
index 807badde..e35b067c 100644
--- a/xarray/core/dataarray.py
+++ b/xarray/core/dataarray.py
@@ -1549,8 +1549,8 @@ class DataArray(AbstractArray, DataWithCoords):
         obj : DataArray
             Another DataArray, with this data but replaced coordinates.

-        Example
-        -------
+        Examples
+        --------
         >>> arr = xr.DataArray(data=np.ones((2, 3)),
         ...                    dims=['x', 'y'],
         ...                    coords={'x':
diff --git a/xarray/core/merge.py b/xarray/core/merge.py
index 225507b9..6dba659f 100644
--- a/xarray/core/merge.py
+++ b/xarray/core/merge.py
@@ -44,6 +44,7 @@ _VALID_COMPAT = Frozen(
         "broadcast_equals": 2,
         "minimal": 3,
         "no_conflicts": 4,
+        "override": 5,
     }
 )

@@ -70,8 +71,8 @@ class MergeError(ValueError):
     # TODO: move this to an xarray.exceptions module?


-def unique_variable(name, variables, compat="broadcast_equals"):
-    # type: (Any, List[Variable], str) -> Variable
+def unique_variable(name, variables, compat="broadcast_equals", equals=None):
+    # type: (Any, List[Variable], str, bool) -> Variable
     """Return the unique variable from a list of variables or raise MergeError.

     Parameters
@@ -81,8 +82,10 @@ def unique_variable(name, variables, compat="broadcast_equals"):
     variables : list of xarray.Variable
         List of Variable objects, all of which go by the same name in different
         inputs.
-    compat : {'identical', 'equals', 'broadcast_equals', 'no_conflicts'}, optional
+    compat : {'identical', 'equals', 'broadcast_equals', 'no_conflicts', 'override'}, optional
         Type of equality check to use.
+    equals: None or bool,
+        corresponding to result of compat test

     Returns
     -------
@@ -93,30 +96,38 @@ def unique_variable(name, variables, compat="broadcast_equals"):
     MergeError: if any of the variables are not equal.
     """  # noqa
     out = variables[0]
-    if len(variables) > 1:
-        combine_method = None

-        if compat == "minimal":
-            compat = "broadcast_equals"
+    if len(variables) == 1 or compat == "override":
+        return out
+
+    combine_method = None
+
+    if compat == "minimal":
+        compat = "broadcast_equals"
+
+    if compat == "broadcast_equals":
+        dim_lengths = broadcast_dimension_size(variables)
+        out = out.set_dims(dim_lengths)
+
+    if compat == "no_conflicts":
+        combine_method = "fillna"

-        if compat == "broadcast_equals":
-            dim_lengths = broadcast_dimension_size(variables)
-            out = out.set_dims(dim_lengths)
+    if equals is None:
+        out = out.compute()
+        for var in variables[1:]:
+            equals = getattr(out, compat)(var)
+            if not equals:
+                break

-        if compat == "no_conflicts":
-            combine_method = "fillna"
+    if not equals:
+        raise MergeError(
+            "conflicting values for variable %r on objects to be combined. You can skip this check by specifying compat='override'."
+            % (name)
+        )

+    if combine_method:
         for var in variables[1:]:
-            if not getattr(out, compat)(var):
-                raise MergeError(
-                    "conflicting values for variable %r on "
-                    "objects to be combined:\n"
-                    "first value: %r\nsecond value: %r" % (name, out, var)
-                )
-            if combine_method:
-                # TODO: add preservation of attrs into fillna
-                out = getattr(out, combine_method)(var)
-                out.attrs = var.attrs
+            out = getattr(out, combine_method)(var)

     return out

@@ -152,7 +163,7 @@ def merge_variables(
     priority_vars : mapping with Variable or None values, optional
         If provided, variables are always taken from this dict in preference to
         the input variable dictionaries, without checking for conflicts.
-    compat : {'identical', 'equals', 'broadcast_equals', 'minimal', 'no_conflicts'}, optional
+    compat : {'identical', 'equals', 'broadcast_equals', 'minimal', 'no_conflicts', 'override'}, optional
         Type of equality check to use when checking for conflicts.

     Returns
@@ -449,7 +460,7 @@ def merge_core(
     ----------
     objs : list of mappings
         All values must be convertable to labeled arrays.
-    compat : {'identical', 'equals', 'broadcast_equals', 'no_conflicts'}, optional
+    compat : {'identical', 'equals', 'broadcast_equals', 'no_conflicts', 'override'}, optional
         Compatibility checks to use when merging variables.
     join : {'outer', 'inner', 'left', 'right'}, optional
         How to combine objects with different indexes.
@@ -519,7 +530,7 @@ def merge(objects, compat="no_conflicts", join="outer", fill_value=dtypes.NA):
     objects : Iterable[Union[xarray.Dataset, xarray.DataArray, dict]]
         Merge together all variables from these objects. If any of them are
         DataArray objects, they must have a name.
-    compat : {'identical', 'equals', 'broadcast_equals', 'no_conflicts'}, optional
+    compat : {'identical', 'equals', 'broadcast_equals', 'no_conflicts', 'override'}, optional
         String indicating how to compare variables of the same name for
         potential conflicts:

@@ -531,6 +542,7 @@ def merge(objects, compat="no_conflicts", join="outer", fill_value=dtypes.NA):
         - 'no_conflicts': only values which are not null in both datasets
           must be equal. The returned dataset then contains the combination
           of all non-null values.
+        - 'override': skip comparing and pick variable from first dataset
     join : {'outer', 'inner', 'left', 'right', 'exact'}, optional
         String indicating how to combine differing indexes in objects.

diff --git a/xarray/tests/test_combine.py b/xarray/tests/test_combine.py
deleted file mode 100644
index f786a851..00000000
--- a/xarray/tests/test_combine.py
+++ /dev/null
@@ -1,886 +0,0 @@
-from collections import OrderedDict
-from datetime import datetime
-from itertools import product
-
-import numpy as np
-import pytest
-
-from xarray import (
-    DataArray,
-    Dataset,
-    auto_combine,
-    combine_by_coords,
-    combine_nested,
-    concat,
-)
-from xarray.core import dtypes
-from xarray.core.combine import (
-    _check_shape_tile_ids,
-    _combine_all_along_first_dim,
-    _combine_nd,
-    _infer_concat_order_from_coords,
-    _infer_concat_order_from_positions,
-    _new_tile_id,
-)
-
-from . import assert_equal, assert_identical, raises_regex
-from .test_dataset import create_test_data
-
-
-def assert_combined_tile_ids_equal(dict1, dict2):
-    assert len(dict1) == len(dict2)
-    for k, v in dict1.items():
-        assert k in dict2.keys()
-        assert_equal(dict1[k], dict2[k])
-
-
-class TestTileIDsFromNestedList:
-    def test_1d(self):
-        ds = create_test_data
-        input = [ds(0), ds(1)]
-
-        expected = {(0,): ds(0), (1,): ds(1)}
-        actual = _infer_concat_order_from_positions(input)
-        assert_combined_tile_ids_equal(expected, actual)
-
-    def test_2d(self):
-        ds = create_test_data
-        input = [[ds(0), ds(1)], [ds(2), ds(3)], [ds(4), ds(5)]]
-
-        expected = {
-            (0, 0): ds(0),
-            (0, 1): ds(1),
-            (1, 0): ds(2),
-            (1, 1): ds(3),
-            (2, 0): ds(4),
-            (2, 1): ds(5),
-        }
-        actual = _infer_concat_order_from_positions(input)
-        assert_combined_tile_ids_equal(expected, actual)
-
-    def test_3d(self):
-        ds = create_test_data
-        input = [
-            [[ds(0), ds(1)], [ds(2), ds(3)], [ds(4), ds(5)]],
-            [[ds(6), ds(7)], [ds(8), ds(9)], [ds(10), ds(11)]],
-        ]
-
-        expected = {
-            (0, 0, 0): ds(0),
-            (0, 0, 1): ds(1),
-            (0, 1, 0): ds(2),
-            (0, 1, 1): ds(3),
-            (0, 2, 0): ds(4),
-            (0, 2, 1): ds(5),
-            (1, 0, 0): ds(6),
-            (1, 0, 1): ds(7),
-            (1, 1, 0): ds(8),
-            (1, 1, 1): ds(9),
-            (1, 2, 0): ds(10),
-            (1, 2, 1): ds(11),
-        }
-        actual = _infer_concat_order_from_positions(input)
-        assert_combined_tile_ids_equal(expected, actual)
-
-    def test_single_dataset(self):
-        ds = create_test_data(0)
-        input = [ds]
-
-        expected = {(0,): ds}
-        actual = _infer_concat_order_from_positions(input)
-        assert_combined_tile_ids_equal(expected, actual)
-
-    def test_redundant_nesting(self):
-        ds = create_test_data
-        input = [[ds(0)], [ds(1)]]
-
-        expected = {(0, 0): ds(0), (1, 0): ds(1)}
-        actual = _infer_concat_order_from_positions(input)
-        assert_combined_tile_ids_equal(expected, actual)
-
-    def test_ignore_empty_list(self):
-        ds = create_test_data(0)
-        input = [ds, []]
-        expected = {(0,): ds}
-        actual = _infer_concat_order_from_positions(input)
-        assert_combined_tile_ids_equal(expected, actual)
-
-    def test_uneven_depth_input(self):
-        # Auto_combine won't work on ragged input
-        # but this is just to increase test coverage
-        ds = create_test_data
-        input = [ds(0), [ds(1), ds(2)]]
-
-        expected = {(0,): ds(0), (1, 0): ds(1), (1, 1): ds(2)}
-        actual = _infer_concat_order_from_positions(input)
-        assert_combined_tile_ids_equal(expected, actual)
-
-    def test_uneven_length_input(self):
-        # Auto_combine won't work on ragged input
-        # but this is just to increase test coverage
-        ds = create_test_data
-        input = [[ds(0)], [ds(1), ds(2)]]
-
-        expected = {(0, 0): ds(0), (1, 0): ds(1), (1, 1): ds(2)}
-        actual = _infer_concat_order_from_positions(input)
-        assert_combined_tile_ids_equal(expected, actual)
-
-    def test_infer_from_datasets(self):
-        ds = create_test_data
-        input = [ds(0), ds(1)]
-
-        expected = {(0,): ds(0), (1,): ds(1)}
-        actual = _infer_concat_order_from_positions(input)
-        assert_combined_tile_ids_equal(expected, actual)
-
-
-class TestTileIDsFromCoords:
-    def test_1d(self):
-        ds0 = Dataset({"x": [0, 1]})
-        ds1 = Dataset({"x": [2, 3]})
-
-        expected = {(0,): ds0, (1,): ds1}
-        actual, concat_dims = _infer_concat_order_from_coords([ds1, ds0])
-        assert_combined_tile_ids_equal(expected, actual)
-        assert concat_dims == ["x"]
-
-    def test_2d(self):
-        ds0 = Dataset({"x": [0, 1], "y": [10, 20, 30]})
-        ds1 = Dataset({"x": [2, 3], "y": [10, 20, 30]})
-        ds2 = Dataset({"x": [0, 1], "y": [40, 50, 60]})
-        ds3 = Dataset({"x": [2, 3], "y": [40, 50, 60]})
-        ds4 = Dataset({"x": [0, 1], "y": [70, 80, 90]})
-        ds5 = Dataset({"x": [2, 3], "y": [70, 80, 90]})
-
-        expected = {
-            (0, 0): ds0,
-            (1, 0): ds1,
-            (0, 1): ds2,
-            (1, 1): ds3,
-            (0, 2): ds4,
-            (1, 2): ds5,
-        }
-        actual, concat_dims = _infer_concat_order_from_coords(
-            [ds1, ds0, ds3, ds5, ds2, ds4]
-        )
-        assert_combined_tile_ids_equal(expected, actual)
-        assert concat_dims == ["x", "y"]
-
-    def test_no_dimension_coords(self):
-        ds0 = Dataset({"foo": ("x", [0, 1])})
-        ds1 = Dataset({"foo": ("x", [2, 3])})
-        with raises_regex(ValueError, "Could not find any dimension"):
-            _infer_concat_order_from_coords([ds1, ds0])
-
-    def test_coord_not_monotonic(self):
-        ds0 = Dataset({"x": [0, 1]})
-        ds1 = Dataset({"x": [3, 2]})
-        with raises_regex(
-            ValueError,
-            "Coordinate variable x is neither " "monotonically increasing nor",
-        ):
-            _infer_concat_order_from_coords([ds1, ds0])
-
-    def test_coord_monotonically_decreasing(self):
-        ds0 = Dataset({"x": [3, 2]})
-        ds1 = Dataset({"x": [1, 0]})
-
-        expected = {(0,): ds0, (1,): ds1}
-        actual, concat_dims = _infer_concat_order_from_coords([ds1, ds0])
-        assert_combined_tile_ids_equal(expected, actual)
-        assert concat_dims == ["x"]
-
-    def test_no_concatenation_needed(self):
-        ds = Dataset({"foo": ("x", [0, 1])})
-        expected = {(): ds}
-        actual, concat_dims = _infer_concat_order_from_coords([ds])
-        assert_combined_tile_ids_equal(expected, actual)
-        assert concat_dims == []
-
-    def test_2d_plus_bystander_dim(self):
-        ds0 = Dataset({"x": [0, 1], "y": [10, 20, 30], "t": [0.1, 0.2]})
-        ds1 = Dataset({"x": [2, 3], "y": [10, 20, 30], "t": [0.1, 0.2]})
-        ds2 = Dataset({"x": [0, 1], "y": [40, 50, 60], "t": [0.1, 0.2]})
-        ds3 = Dataset({"x": [2, 3], "y": [40, 50, 60], "t": [0.1, 0.2]})
-
-        expected = {(0, 0): ds0, (1, 0): ds1, (0, 1): ds2, (1, 1): ds3}
-        actual, concat_dims = _infer_concat_order_from_coords([ds1, ds0, ds3, ds2])
-        assert_combined_tile_ids_equal(expected, actual)
-        assert concat_dims == ["x", "y"]
-
-    def test_string_coords(self):
-        ds0 = Dataset({"person": ["Alice", "Bob"]})
-        ds1 = Dataset({"person": ["Caroline", "Daniel"]})
-
-        expected = {(0,): ds0, (1,): ds1}
-        actual, concat_dims = _infer_concat_order_from_coords([ds1, ds0])
-        assert_combined_tile_ids_equal(expected, actual)
-        assert concat_dims == ["person"]
-
-    # Decided against natural sorting of string coords GH #2616
-    def test_lexicographic_sort_string_coords(self):
-        ds0 = Dataset({"simulation": ["run8", "run9"]})
-        ds1 = Dataset({"simulation": ["run10", "run11"]})
-
-        expected = {(0,): ds1, (1,): ds0}
-        actual, concat_dims = _infer_concat_order_from_coords([ds1, ds0])
-        assert_combined_tile_ids_equal(expected, actual)
-        assert concat_dims == ["simulation"]
-
-    def test_datetime_coords(self):
-        ds0 = Dataset({"time": [datetime(2000, 3, 6), datetime(2001, 3, 7)]})
-        ds1 = Dataset({"time": [datetime(1999, 1, 1), datetime(1999, 2, 4)]})
-
-        expected = {(0,): ds1, (1,): ds0}
-        actual, concat_dims = _infer_concat_order_from_coords([ds0, ds1])
-        assert_combined_tile_ids_equal(expected, actual)
-        assert concat_dims == ["time"]
-
-
-@pytest.fixture(scope="module")
-def create_combined_ids():
-    return _create_combined_ids
-
-
-def _create_combined_ids(shape):
-    tile_ids = _create_tile_ids(shape)
-    nums = range(len(tile_ids))
-    return {tile_id: create_test_data(num) for tile_id, num in zip(tile_ids, nums)}
-
-
-def _create_tile_ids(shape):
-    tile_ids = product(*(range(i) for i in shape))
-    return list(tile_ids)
-
-
-class TestNewTileIDs:
-    @pytest.mark.parametrize(
-        "old_id, new_id",
-        [((3, 0, 1), (0, 1)), ((0, 0), (0,)), ((1,), ()), ((0,), ()), ((1, 0), (0,))],
-    )
-    def test_new_tile_id(self, old_id, new_id):
-        ds = create_test_data
-        assert _new_tile_id((old_id, ds)) == new_id
-
-    def test_get_new_tile_ids(self, create_combined_ids):
-        shape = (1, 2, 3)
-        combined_ids = create_combined_ids(shape)
-
-        expected_tile_ids = sorted(combined_ids.keys())
-        actual_tile_ids = _create_tile_ids(shape)
-        assert expected_tile_ids == actual_tile_ids
-
-
-class TestCombineND:
-    @pytest.mark.parametrize("concat_dim", ["dim1", "new_dim"])
-    def test_concat_once(self, create_combined_ids, concat_dim):
-        shape = (2,)
-        combined_ids = create_combined_ids(shape)
-        ds = create_test_data
-        result = _combine_all_along_first_dim(
-            combined_ids,
-            dim=concat_dim,
-            data_vars="all",
-            coords="different",
-            compat="no_conflicts",
-        )
-
-        expected_ds = concat([ds(0), ds(1)], dim=concat_dim)
-        assert_combined_tile_ids_equal(result, {(): expected_ds})
-
-    def test_concat_only_first_dim(self, create_combined_ids):
-        shape = (2, 3)
-        combined_ids = create_combined_ids(shape)
-        result = _combine_all_along_first_dim(
-            combined_ids,
-            dim="dim1",
-            data_vars="all",
-            coords="different",
-            compat="no_conflicts",
-        )
-
-        ds = create_test_data
-        partway1 = concat([ds(0), ds(3)], dim="dim1")
-        partway2 = concat([ds(1), ds(4)], dim="dim1")
-        partway3 = concat([ds(2), ds(5)], dim="dim1")
-        expected_datasets = [partway1, partway2, partway3]
-        expected = {(i,): ds for i, ds in enumerate(expected_datasets)}
-
-        assert_combined_tile_ids_equal(result, expected)
-
-    @pytest.mark.parametrize("concat_dim", ["dim1", "new_dim"])
-    def test_concat_twice(self, create_combined_ids, concat_dim):
-        shape = (2, 3)
-        combined_ids = create_combined_ids(shape)
-        result = _combine_nd(combined_ids, concat_dims=["dim1", concat_dim])
-
-        ds = create_test_data
-        partway1 = concat([ds(0), ds(3)], dim="dim1")
-        partway2 = concat([ds(1), ds(4)], dim="dim1")
-        partway3 = concat([ds(2), ds(5)], dim="dim1")
-        expected = concat([partway1, partway2, partway3], dim=concat_dim)
-
-        assert_equal(result, expected)
-
-
-class TestCheckShapeTileIDs:
-    def test_check_depths(self):
-        ds = create_test_data(0)
-        combined_tile_ids = {(0,): ds, (0, 1): ds}
-        with raises_regex(ValueError, "sub-lists do not have " "consistent depths"):
-            _check_shape_tile_ids(combined_tile_ids)
-
-    def test_check_lengths(self):
-        ds = create_test_data(0)
-        combined_tile_ids = {(0, 0): ds, (0, 1): ds, (0, 2): ds, (1, 0): ds, (1, 1): ds}
-        with raises_regex(ValueError, "sub-lists do not have " "consistent lengths"):
-            _check_shape_tile_ids(combined_tile_ids)
-
-
-class TestNestedCombine:
-    def test_nested_concat(self):
-        objs = [Dataset({"x": [0]}), Dataset({"x": [1]})]
-        expected = Dataset({"x": [0, 1]})
-        actual = combine_nested(objs, concat_dim="x")
-        assert_identical(expected, actual)
-        actual = combine_nested(objs, concat_dim=["x"])
-        assert_identical(expected, actual)
-
-        actual = combine_nested([actual], concat_dim=None)
-        assert_identical(expected, actual)
-
-        actual = combine_nested([actual], concat_dim="x")
-        assert_identical(expected, actual)
-
-        objs = [Dataset({"x": [0, 1]}), Dataset({"x": [2]})]
-        actual = combine_nested(objs, concat_dim="x")
-        expected = Dataset({"x": [0, 1, 2]})
-        assert_identical(expected, actual)
-
-        # ensure combine_nested handles non-sorted variables
-        objs = [
-            Dataset(OrderedDict([("x", ("a", [0])), ("y", ("a", [0]))])),
-            Dataset(OrderedDict([("y", ("a", [1])), ("x", ("a", [1]))])),
-        ]
-        actual = combine_nested(objs, concat_dim="a")
-        expected = Dataset({"x": ("a", [0, 1]), "y": ("a", [0, 1])})
-        assert_identical(expected, actual)
-
-        objs = [Dataset({"x": [0], "y": [0]}), Dataset({"x": [0]})]
-        with pytest.raises(KeyError):
-            combine_nested(objs, concat_dim="x")
-
-    @pytest.mark.parametrize(
-        "join, expected",
-        [
-            ("outer", Dataset({"x": [0, 1], "y": [0, 1]})),
-            ("inner", Dataset({"x": [0, 1], "y": []})),
-            ("left", Dataset({"x": [0, 1], "y": [0]})),
-            ("right", Dataset({"x": [0, 1], "y": [1]})),
-        ],
-    )
-    def test_combine_nested_join(self, join, expected):
-        objs = [Dataset({"x": [0], "y": [0]}), Dataset({"x": [1], "y": [1]})]
-        actual = combine_nested(objs, concat_dim="x", join=join)
-        assert_identical(expected, actual)
-
-    def test_combine_nested_join_exact(self):
-        objs = [Dataset({"x": [0], "y": [0]}), Dataset({"x": [1], "y": [1]})]
-        with raises_regex(ValueError, "indexes along dimension"):
-            combine_nested(objs, concat_dim="x", join="exact")
-
-    def test_empty_input(self):
-        assert_identical(Dataset(), combine_nested([], concat_dim="x"))
-
-    # Fails because of concat's weird treatment of dimension coords, see #2975
-    @pytest.mark.xfail
-    def test_nested_concat_too_many_dims_at_once(self):
-        objs = [Dataset({"x": [0], "y": [1]}), Dataset({"y": [0], "x": [1]})]
-        with pytest.raises(ValueError, match="not equal across datasets"):
-            combine_nested(objs, concat_dim="x", coords="minimal")
-
-    def test_nested_concat_along_new_dim(self):
-        objs = [
-            Dataset({"a": ("x", [10]), "x": [0]}),
-            Dataset({"a": ("x", [20]), "x": [0]}),
-        ]
-        expected = Dataset({"a": (("t", "x"), [[10], [20]]), "x": [0]})
-        actual = combine_nested(objs, concat_dim="t")
-        assert_identical(expected, actual)
-
-        # Same but with a DataArray as new dim, see GH #1988 and #2647
-        dim = DataArray([100, 150], name="baz", dims="baz")
-        expected = Dataset(
-            {"a": (("baz", "x"), [[10], [20]]), "x": [0], "baz": [100, 150]}
-        )
-        actual = combine_nested(objs, concat_dim=dim)
-        assert_identical(expected, actual)
-
-    def test_nested_merge(self):
-        data = Dataset({"x": 0})
-        actual = combine_nested([data, data, data], concat_dim=None)
-        assert_identical(data, actual)
-
-        ds1 = Dataset({"a": ("x", [1, 2]), "x": [0, 1]})
-        ds2 = Dataset({"a": ("x", [2, 3]), "x": [1, 2]})
-        expected = Dataset({"a": ("x", [1, 2, 3]), "x": [0, 1, 2]})
-        actual = combine_nested([ds1, ds2], concat_dim=None)
-        assert_identical(expected, actual)
-        actual = combine_nested([ds1, ds2], concat_dim=[None])
-        assert_identical(expected, actual)
-
-        tmp1 = Dataset({"x": 0})
-        tmp2 = Dataset({"x": np.nan})
-        actual = combine_nested([tmp1, tmp2], concat_dim=None)
-        assert_identical(tmp1, actual)
-        actual = combine_nested([tmp1, tmp2], concat_dim=[None])
-        assert_identical(tmp1, actual)
-
-        # Single object, with a concat_dim explicitly provided
-        # Test the issue reported in GH #1988
-        objs = [Dataset({"x": 0, "y": 1})]
-        dim = DataArray([100], name="baz", dims="baz")
-        actual = combine_nested(objs, concat_dim=[dim])
-        expected = Dataset({"x": ("baz", [0]), "y": ("baz", [1])}, {"baz": [100]})
-        assert_identical(expected, actual)
-
-        # Just making sure that auto_combine is doing what is
-        # expected for non-scalar values, too.
-        objs = [Dataset({"x": ("z", [0, 1]), "y": ("z", [1, 2])})]
-        dim = DataArray([100], name="baz", dims="baz")
-        actual = combine_nested(objs, concat_dim=[dim])
-        expected = Dataset(
-            {"x": (("baz", "z"), [[0, 1]]), "y": (("baz", "z"), [[1, 2]])},
-            {"baz": [100]},
-        )
-        assert_identical(expected, actual)
-
-    def test_concat_multiple_dims(self):
-        objs = [
-            [Dataset({"a": (("x", "y"), [[0]])}), Dataset({"a": (("x", "y"), [[1]])})],
-            [Dataset({"a": (("x", "y"), [[2]])}), Dataset({"a": (("x", "y"), [[3]])})],
-        ]
-        actual = combine_nested(objs, concat_dim=["x", "y"])
-        expected = Dataset({"a": (("x", "y"), [[0, 1], [2, 3]])})
-        assert_identical(expected, actual)
-
-    def test_concat_name_symmetry(self):
-        """Inspired by the discussion on GH issue #2777"""
-
-        da1 = DataArray(name="a", data=[[0]], dims=["x", "y"])
-        da2 = DataArray(name="b", data=[[1]], dims=["x", "y"])
-        da3 = DataArray(name="a", data=[[2]], dims=["x", "y"])
-        da4 = DataArray(name="b", data=[[3]], dims=["x", "y"])
-
-        x_first = combine_nested([[da1, da2], [da3, da4]], concat_dim=["x", "y"])
-        y_first = combine_nested([[da1, da3], [da2, da4]], concat_dim=["y", "x"])
-
-        assert_identical(x_first, y_first)
-
-    def test_concat_one_dim_merge_another(self):
-        data = create_test_data()
-        data1 = data.copy(deep=True)
-        data2 = data.copy(deep=True)
-
-        objs = [
-            [data1.var1.isel(dim2=slice(4)), data2.var1.isel(dim2=slice(4, 9))],
-            [data1.var2.isel(dim2=slice(4)), data2.var2.isel(dim2=slice(4, 9))],
-        ]
-
-        expected = data[["var1", "var2"]]
-        actual = combine_nested(objs, concat_dim=[None, "dim2"])
-        assert expected.identical(actual)
-
-    def test_auto_combine_2d(self):
-        ds = create_test_data
-
-        partway1 = concat([ds(0), ds(3)], dim="dim1")
-        partway2 = concat([ds(1), ds(4)], dim="dim1")
-        partway3 = concat([ds(2), ds(5)], dim="dim1")
-        expected = concat([partway1, partway2, partway3], dim="dim2")
-
-        datasets = [[ds(0), ds(1), ds(2)], [ds(3), ds(4), ds(5)]]
-        result = combine_nested(datasets, concat_dim=["dim1", "dim2"])
-        assert_equal(result, expected)
-
-    def test_combine_nested_missing_data_new_dim(self):
-        # Your data includes "time" and "station" dimensions, and each year's
-        # data has a different set of stations.
-        datasets = [
-            Dataset({"a": ("x", [2, 3]), "x": [1, 2]}),
-            Dataset({"a": ("x", [1, 2]), "x": [0, 1]}),
-        ]
-        expected = Dataset(
-            {"a": (("t", "x"), [[np.nan, 2, 3], [1, 2, np.nan]])}, {"x": [0, 1, 2]}
-        )
-        actual = combine_nested(datasets, concat_dim="t")
-        assert_identical(expected, actual)
-
-    def test_invalid_hypercube_input(self):
-        ds = create_test_data
-
-        datasets = [[ds(0), ds(1), ds(2)], [ds(3), ds(4)]]
-        with raises_regex(ValueError, "sub-lists do not have " "consistent lengths"):
-            combine_nested(datasets, concat_dim=["dim1", "dim2"])
-
-        datasets = [[ds(0), ds(1)], [[ds(3), ds(4)]]]
-        with raises_regex(ValueError, "sub-lists do not have " "consistent depths"):
-            combine_nested(datasets, concat_dim=["dim1", "dim2"])
-
-        datasets = [[ds(0), ds(1)], [ds(3), ds(4)]]
-        with raises_regex(ValueError, "concat_dims has length"):
-            combine_nested(datasets, concat_dim=["dim1"])
-
-    def test_merge_one_dim_concat_another(self):
-        objs = [
-            [Dataset({"foo": ("x", [0, 1])}), Dataset({"bar": ("x", [10, 20])})],
-            [Dataset({"foo": ("x", [2, 3])}), Dataset({"bar": ("x", [30, 40])})],
-        ]
-        expected = Dataset({"foo": ("x", [0, 1, 2, 3]), "bar": ("x", [10, 20, 30, 40])})
-
-        actual = combine_nested(objs, concat_dim=["x", None], compat="equals")
-        assert_identical(expected, actual)
-
-        # Proving it works symmetrically
-        objs = [
-            [Dataset({"foo": ("x", [0, 1])}), Dataset({"foo": ("x", [2, 3])})],
-            [Dataset({"bar": ("x", [10, 20])}), Dataset({"bar": ("x", [30, 40])})],
-        ]
-        actual = combine_nested(objs, concat_dim=[None, "x"], compat="equals")
-        assert_identical(expected, actual)
-
-    def test_combine_concat_over_redundant_nesting(self):
-        objs = [[Dataset({"x": [0]}), Dataset({"x": [1]})]]
-        actual = combine_nested(objs, concat_dim=[None, "x"])
-        expected = Dataset({"x": [0, 1]})
-        assert_identical(expected, actual)
-
-        objs = [[Dataset({"x": [0]})], [Dataset({"x": [1]})]]
-        actual = combine_nested(objs, concat_dim=["x", None])
-        expected = Dataset({"x": [0, 1]})
-        assert_identical(expected, actual)
-
-        objs = [[Dataset({"x": [0]})]]
-        actual = combine_nested(objs, concat_dim=[None, None])
-        expected = Dataset({"x": [0]})
-        assert_identical(expected, actual)
-
-    def test_combine_nested_but_need_auto_combine(self):
-        objs = [Dataset({"x": [0, 1]}), Dataset({"x": [2], "wall": [0]})]
-        with raises_regex(ValueError, "cannot be combined"):
-            combine_nested(objs, concat_dim="x")
-
-    @pytest.mark.parametrize("fill_value", [dtypes.NA, 2, 2.0])
-    def test_combine_nested_fill_value(self, fill_value):
-        datasets = [
-            Dataset({"a": ("x", [2, 3]), "x": [1, 2]}),
-            Dataset({"a": ("x", [1, 2]), "x": [0, 1]}),
-        ]
-        if fill_value == dtypes.NA:
-            # if we supply the default, we expect the missing value for a
-            # float array
-            fill_value = np.nan
-        expected = Dataset(
-            {"a": (("t", "x"), [[fill_value, 2, 3], [1, 2, fill_value]])},
-            {"x": [0, 1, 2]},
-        )
-        actual = combine_nested(datasets, concat_dim="t", fill_value=fill_value)
-        assert_identical(expected, actual)
-
-
-class TestCombineAuto:
-    def test_combine_by_coords(self):
-        objs = [Dataset({"x": [0]}), Dataset({"x": [1]})]
-        actual = combine_by_coords(objs)
-        expected = Dataset({"x": [0, 1]})
-        assert_identical(expected, actual)
-
-        actual = combine_by_coords([actual])
-        assert_identical(expected, actual)
-
-        objs = [Dataset({"x": [0, 1]}), Dataset({"x": [2]})]
-        actual = combine_by_coords(objs)
-        expected = Dataset({"x": [0, 1, 2]})
-        assert_identical(expected, actual)
-
-        # ensure auto_combine handles non-sorted variables
-        objs = [
-            Dataset({"x": ("a", [0]), "y": ("a", [0]), "a": [0]}),
-            Dataset({"x": ("a", [1]), "y": ("a", [1]), "a": [1]}),
-        ]
-        actual = combine_by_coords(objs)
-        expected = Dataset({"x": ("a", [0, 1]), "y": ("a", [0, 1]), "a": [0, 1]})
-        assert_identical(expected, actual)
-
-        objs = [Dataset({"x": [0], "y": [0]}), Dataset({"y": [1], "x": [1]})]
-        actual = combine_by_coords(objs)
-        expected = Dataset({"x": [0, 1], "y": [0, 1]})
-        assert_equal(actual, expected)
-
-        objs = [Dataset({"x": 0}), Dataset({"x": 1})]
-        with raises_regex(ValueError, "Could not find any dimension " "coordinates"):
-            combine_by_coords(objs)
-
-        objs = [Dataset({"x": [0], "y": [0]}), Dataset({"x": [0]})]
-        with raises_regex(ValueError, "Every dimension needs a coordinate"):
-            combine_by_coords(objs)
-
-        def test_empty_input(self):
-            assert_identical(Dataset(), combine_by_coords([]))
-
-    @pytest.mark.parametrize(
-        "join, expected",
-        [
-            ("outer", Dataset({"x": [0, 1], "y": [0, 1]})),
-            ("inner", Dataset({"x": [0, 1], "y": []})),
-            ("left", Dataset({"x": [0, 1], "y": [0]})),
-            ("right", Dataset({"x": [0, 1], "y": [1]})),
-        ],
-    )
-    def test_combine_coords_join(self, join, expected):
-        objs = [Dataset({"x": [0], "y": [0]}), Dataset({"x": [1], "y": [1]})]
-        actual = combine_nested(objs, concat_dim="x", join=join)
-        assert_identical(expected, actual)
-
-    def test_combine_coords_join_exact(self):
-        objs = [Dataset({"x": [0], "y": [0]}), Dataset({"x": [1], "y": [1]})]
-        with raises_regex(ValueError, "indexes along dimension"):
-            combine_nested(objs, concat_dim="x", join="exact")
-
-    def test_infer_order_from_coords(self):
-        data = create_test_data()
-        objs = [data.isel(dim2=slice(4, 9)), data.isel(dim2=slice(4))]
-        actual = combine_by_coords(objs)
-        expected = data
-        assert expected.broadcast_equals(actual)
-
-    def test_combine_leaving_bystander_dimensions(self):
-        # Check non-monotonic bystander dimension coord doesn't raise
-        # ValueError on combine (https://github.com/pydata/xarray/issues/3150)
-        ycoord = ["a", "c", "b"]
-
-        data = np.random.rand(7, 3)
-
-        ds1 = Dataset(
-            data_vars=dict(data=(["x", "y"], data[:3, :])),
-            coords=dict(x=[1, 2, 3], y=ycoord),
-        )
-
-        ds2 = Dataset(
-            data_vars=dict(data=(["x", "y"], data[3:, :])),
-            coords=dict(x=[4, 5, 6, 7], y=ycoord),
-        )
-
-        expected = Dataset(
-            data_vars=dict(data=(["x", "y"], data)),
-            coords=dict(x=[1, 2, 3, 4, 5, 6, 7], y=ycoord),
-        )
-
-        actual = combine_by_coords((ds1, ds2))
-        assert_identical(expected, actual)
-
-    def test_combine_by_coords_previously_failed(self):
-        # In the above scenario, one file is missing, containing the data for
-        # one year's data for one variable.
-        datasets = [
-            Dataset({"a": ("x", [0]), "x": [0]}),
-            Dataset({"b": ("x", [0]), "x": [0]}),
-            Dataset({"a": ("x", [1]), "x": [1]}),
-        ]
-        expected = Dataset({"a": ("x", [0, 1]), "b": ("x", [0, np.nan])}, {"x": [0, 1]})
-        actual = combine_by_coords(datasets)
-        assert_identical(expected, actual)
-
-    def test_combine_by_coords_still_fails(self):
-        # concat can't handle new variables (yet):
-        # https://github.com/pydata/xarray/issues/508
-        datasets = [Dataset({"x": 0}, {"y": 0}), Dataset({"x": 1}, {"y": 1, "z": 1})]
-        with pytest.raises(ValueError):
-            combine_by_coords(datasets, "y")
-
-    def test_combine_by_coords_no_concat(self):
-        objs = [Dataset({"x": 0}), Dataset({"y": 1})]
-        actual = combine_by_coords(objs)
-        expected = Dataset({"x": 0, "y": 1})
-        assert_identical(expected, actual)
-
-        objs = [Dataset({"x": 0, "y": 1}), Dataset({"y": np.nan, "z": 2})]
-        actual = combine_by_coords(objs)
-        expected = Dataset({"x": 0, "y": 1, "z": 2})
-        assert_identical(expected, actual)
-
-    def test_check_for_impossible_ordering(self):
-        ds0 = Dataset({"x": [0, 1, 5]})
-        ds1 = Dataset({"x": [2, 3]})
-        with raises_regex(
-            ValueError, "does not have monotonic global indexes" " along dimension x"
-        ):
-            combine_by_coords([ds1, ds0])
-
-
-@pytest.mark.filterwarnings(
-    "ignore:In xarray version 0.13 `auto_combine` " "will be deprecated"
-)
-@pytest.mark.filterwarnings("ignore:Also `open_mfdataset` will no longer")
-@pytest.mark.filterwarnings("ignore:The datasets supplied")
-class TestAutoCombineOldAPI:
-    """
-    Set of tests which check that old 1-dimensional auto_combine behaviour is
-    still satisfied. #2616
-    """
-
-    def test_auto_combine(self):
-        objs = [Dataset({"x": [0]}), Dataset({"x": [1]})]
-        actual = auto_combine(objs)
-        expected = Dataset({"x": [0, 1]})
-        assert_identical(expected, actual)
-
-        actual = auto_combine([actual])
-        assert_identical(expected, actual)
-
-        objs = [Dataset({"x": [0, 1]}), Dataset({"x": [2]})]
-        actual = auto_combine(objs)
-        expected = Dataset({"x": [0, 1, 2]})
-        assert_identical(expected, actual)
-
-        # ensure auto_combine handles non-sorted variables
-        objs = [
-            Dataset(OrderedDict([("x", ("a", [0])), ("y", ("a", [0]))])),
-            Dataset(OrderedDict([("y", ("a", [1])), ("x", ("a", [1]))])),
-        ]
-        actual = auto_combine(objs)
-        expected = Dataset({"x": ("a", [0, 1]), "y": ("a", [0, 1])})
-        assert_identical(expected, actual)
-
-        objs = [Dataset({"x": [0], "y": [0]}), Dataset({"y": [1], "x": [1]})]
-        with raises_regex(ValueError, "too many .* dimensions"):
-            auto_combine(objs)
-
-        objs = [Dataset({"x": 0}), Dataset({"x": 1})]
-        with raises_regex(ValueError, "cannot infer dimension"):
-            auto_combine(objs)
-
-        objs = [Dataset({"x": [0], "y": [0]}), Dataset({"x": [0]})]
-        with pytest.raises(KeyError):
-            auto_combine(objs)
-
-    def test_auto_combine_previously_failed(self):
-        # In the above scenario, one file is missing, containing the data for
-        # one year's data for one variable.
-        datasets = [
-            Dataset({"a": ("x", [0]), "x": [0]}),
-            Dataset({"b": ("x", [0]), "x": [0]}),
-            Dataset({"a": ("x", [1]), "x": [1]}),
-        ]
-        expected = Dataset({"a": ("x", [0, 1]), "b": ("x", [0, np.nan])}, {"x": [0, 1]})
-        actual = auto_combine(datasets)
-        assert_identical(expected, actual)
-
-        # Your data includes "time" and "station" dimensions, and each year's
-        # data has a different set of stations.
-        datasets = [
-            Dataset({"a": ("x", [2, 3]), "x": [1, 2]}),
-            Dataset({"a": ("x", [1, 2]), "x": [0, 1]}),
-        ]
-        expected = Dataset(
-            {"a": (("t", "x"), [[np.nan, 2, 3], [1, 2, np.nan]])}, {"x": [0, 1, 2]}
-        )
-        actual = auto_combine(datasets, concat_dim="t")
-        assert_identical(expected, actual)
-
-    def test_auto_combine_still_fails(self):
-        # concat can't handle new variables (yet):
-        # https://github.com/pydata/xarray/issues/508
-        datasets = [Dataset({"x": 0}, {"y": 0}), Dataset({"x": 1}, {"y": 1, "z": 1})]
-        with pytest.raises(ValueError):
-            auto_combine(datasets, "y")
-
-    def test_auto_combine_no_concat(self):
-        objs = [Dataset({"x": 0}), Dataset({"y": 1})]
-        actual = auto_combine(objs)
-        expected = Dataset({"x": 0, "y": 1})
-        assert_identical(expected, actual)
-
-        objs = [Dataset({"x": 0, "y": 1}), Dataset({"y": np.nan, "z": 2})]
-        actual = auto_combine(objs)
-        expected = Dataset({"x": 0, "y": 1, "z": 2})
-        assert_identical(expected, actual)
-
-        data = Dataset({"x": 0})
-        actual = auto_combine([data, data, data], concat_dim=None)
-        assert_identical(data, actual)
-
-        # Single object, with a concat_dim explicitly provided
-        # Test the issue reported in GH #1988
-        objs = [Dataset({"x": 0, "y": 1})]
-        dim = DataArray([100], name="baz", dims="baz")
-        actual = auto_combine(objs, concat_dim=dim)
-        expected = Dataset({"x": ("baz", [0]), "y": ("baz", [1])}, {"baz": [100]})
-        assert_identical(expected, actual)
-
-        # Just making sure that auto_combine is doing what is
-        # expected for non-scalar values, too.
-        objs = [Dataset({"x": ("z", [0, 1]), "y": ("z", [1, 2])})]
-        dim = DataArray([100], name="baz", dims="baz")
-        actual = auto_combine(objs, concat_dim=dim)
-        expected = Dataset(
-            {"x": (("baz", "z"), [[0, 1]]), "y": (("baz", "z"), [[1, 2]])},
-            {"baz": [100]},
-        )
-        assert_identical(expected, actual)
-
-    def test_auto_combine_order_by_appearance_not_coords(self):
-        objs = [
-            Dataset({"foo": ("x", [0])}, coords={"x": ("x", [1])}),
-            Dataset({"foo": ("x", [1])}, coords={"x": ("x", [0])}),
-        ]
-        actual = auto_combine(objs)
-        expected = Dataset({"foo": ("x", [0, 1])}, coords={"x": ("x", [1, 0])})
-        assert_identical(expected, actual)
-
-    @pytest.mark.parametrize("fill_value", [dtypes.NA, 2, 2.0])
-    def test_auto_combine_fill_value(self, fill_value):
-        datasets = [
-            Dataset({"a": ("x", [2, 3]), "x": [1, 2]}),
-            Dataset({"a": ("x", [1, 2]), "x": [0, 1]}),
-        ]
-        if fill_value == dtypes.NA:
-            # if we supply the default, we expect the missing value for a
-            # float array
-            fill_value = np.nan
-        expected = Dataset(
-            {"a": (("t", "x"), [[fill_value, 2, 3], [1, 2, fill_value]])},
-            {"x": [0, 1, 2]},
-        )
-        actual = auto_combine(datasets, concat_dim="t", fill_value=fill_value)
-        assert_identical(expected, actual)
-
-
-class TestAutoCombineDeprecation:
-    """
-    Set of tests to check that FutureWarnings are correctly raised until the
-    deprecation cycle is complete. #2616
-    """
-
-    def test_auto_combine_with_concat_dim(self):
-        objs = [Dataset({"x": [0]}), Dataset({"x": [1]})]
-        with pytest.warns(FutureWarning, match="`concat_dim`"):
-            auto_combine(objs, concat_dim="x")
-
-    def test_auto_combine_with_merge_and_concat(self):
-        objs = [Dataset({"x": [0]}), Dataset({"x": [1]}), Dataset({"z": ((), 99)})]
-        with pytest.warns(FutureWarning, match="require both concatenation"):
-            auto_combine(objs)
-
-    def test_auto_combine_with_coords(self):
-        objs = [
-            Dataset({"foo": ("x", [0])}, coords={"x": ("x", [0])}),
-            Dataset({"foo": ("x", [1])}, coords={"x": ("x", [1])}),
-        ]
-        with pytest.warns(FutureWarning, match="supplied have global"):
-            auto_combine(objs)
-
-    def test_auto_combine_without_coords(self):
-        objs = [Dataset({"foo": ("x", [0])}), Dataset({"foo": ("x", [1])})]
-        with pytest.warns(FutureWarning, match="supplied do not have global"):
-            auto_combine(objs)
diff --git a/xarray/tests/test_concat.py b/xarray/tests/test_concat.py
index ee99ca02..00428f70 100644
--- a/xarray/tests/test_concat.py
+++ b/xarray/tests/test_concat.py
@@ -5,8 +5,7 @@ import pandas as pd
 import pytest

 from xarray import DataArray, Dataset, Variable, concat
-from xarray.core import dtypes
-
+from xarray.core import dtypes, merge
 from . import (
     InaccessibleArray,
     assert_array_equal,
@@ -18,6 +17,34 @@ from . import (
 from .test_dataset import create_test_data


+def test_concat_compat():
+    ds1 = Dataset(
+        {
+            "has_x_y": (("y", "x"), [[1, 2]]),
+            "has_x": ("x", [1, 2]),
+            "no_x_y": ("z", [1, 2]),
+        },
+        coords={"x": [0, 1], "y": [0], "z": [-1, -2]},
+    )
+    ds2 = Dataset(
+        {
+            "has_x_y": (("y", "x"), [[3, 4]]),
+            "has_x": ("x", [1, 2]),
+            "no_x_y": (("q", "z"), [[1, 2]]),
+        },
+        coords={"x": [0, 1], "y": [1], "z": [-1, -2], "q": [0]},
+    )
+
+    result = concat([ds1, ds2], dim="y", data_vars="minimal", compat="broadcast_equals")
+    assert_equal(ds2.no_x_y, result.no_x_y.transpose())
+
+    for var in ["has_x", "no_x_y"]:
+        assert "y" not in result[var]
+
+    with raises_regex(ValueError, "'q' is not present in all datasets"):
+        concat([ds1, ds2], dim="q", data_vars="all", compat="broadcast_equals")
+
+
 class TestConcatDataset:
     @pytest.fixture
     def data(self):
@@ -92,7 +119,7 @@ class TestConcatDataset:
             actual = concat(objs, dim="x", coords=coords)
             assert_identical(expected, actual)
         for coords in ["minimal", []]:
-            with raises_regex(ValueError, "not equal across"):
+            with raises_regex(merge.MergeError, "conflicting values"):
                 concat(objs, dim="x", coords=coords)

     def test_concat_constant_index(self):
@@ -103,8 +130,10 @@ class TestConcatDataset:
         for mode in ["different", "all", ["foo"]]:
             actual = concat([ds1, ds2], "y", data_vars=mode)
             assert_identical(expected, actual)
-        with raises_regex(ValueError, "not equal across datasets"):
-            concat([ds1, ds2], "y", data_vars="minimal")
+        with raises_regex(merge.MergeError, "conflicting values"):
+            # previously dim="y", and raised error which makes no sense.
+            # "foo" has dimension "y" so minimal should concatenate it?
+            concat([ds1, ds2], "new_dim", data_vars="minimal")

     def test_concat_size0(self):
         data = create_test_data()
@@ -134,6 +163,14 @@ class TestConcatDataset:
         data = create_test_data()
         split_data = [data.isel(dim1=slice(3)), data.isel(dim1=slice(3, None))]

+        with raises_regex(ValueError, "must supply at least one"):
+            concat([], "dim1")
+
+        with raises_regex(ValueError, "Cannot specify both .*='different'"):
+            concat(
+                [data, data], dim="concat_dim", data_vars="different", compat="override"
+            )
+
         with raises_regex(ValueError, "must supply at least one"):
             concat([], "dim1")

@@ -146,7 +183,7 @@ class TestConcatDataset:
             concat([data0, data1], "dim1", compat="identical")
         assert_identical(data, concat([data0, data1], "dim1", compat="equals"))

-        with raises_regex(ValueError, "encountered unexpected"):
+        with raises_regex(ValueError, "present in some datasets"):
             data0, data1 = deepcopy(split_data)
             data1["foo"] = ("bar", np.random.randn(10))
             concat([data0, data1], "dim1")
diff --git a/xarray/tests/test_dask.py b/xarray/tests/test_dask.py
index d1057654..76b3ed1a 100644
--- a/xarray/tests/test_dask.py
+++ b/xarray/tests/test_dask.py
@@ -825,7 +825,6 @@ def kernel(name):
     """Dask kernel to test pickling/unpickling and __repr__.
     Must be global to make it pickleable.
     """
-    print("kernel(%s)" % name)
     global kernel_call_count
     kernel_call_count += 1
     return np.ones(1, dtype=np.int64)
diff --git a/xarray/tests/test_merge.py b/xarray/tests/test_merge.py
index ed1453ce..c1e6c7a5 100644
--- a/xarray/tests/test_merge.py
+++ b/xarray/tests/test_merge.py
@@ -196,6 +196,8 @@ class TestMergeMethod:
         with raises_regex(ValueError, "compat=.* invalid"):
             ds1.merge(ds2, compat="foobar")

+        assert ds1.identical(ds1.merge(ds2, compat="override"))
+
     def test_merge_auto_align(self):
         ds1 = xr.Dataset({"a": ("x", [1, 2]), "x": [0, 1]})
         ds2 = xr.Dataset({"b": ("x", [3, 4]), "x": [1, 2]})
Obtaining file:///testbed
  Preparing metadata (setup.py): started
  Preparing metadata (setup.py): finished with status 'done'
Requirement already satisfied: numpy>=1.12 in /opt/miniconda3/envs/testbed/lib/python3.10/site-packages (from xarray==0.12.3+93.g2c8ba46b) (1.23.0)
Requirement already satisfied: pandas>=0.19.2 in /opt/miniconda3/envs/testbed/lib/python3.10/site-packages (from xarray==0.12.3+93.g2c8ba46b) (1.5.3)
Requirement already satisfied: python-dateutil>=2.8.1 in /opt/miniconda3/envs/testbed/lib/python3.10/site-packages (from pandas>=0.19.2->xarray==0.12.3+93.g2c8ba46b) (2.8.2)
Requirement already satisfied: pytz>=2020.1 in /opt/miniconda3/envs/testbed/lib/python3.10/site-packages (from pandas>=0.19.2->xarray==0.12.3+93.g2c8ba46b) (2023.3)
Requirement already satisfied: six>=1.5 in /opt/miniconda3/envs/testbed/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas>=0.19.2->xarray==0.12.3+93.g2c8ba46b) (1.16.0)
Installing collected packages: xarray
  Attempting uninstall: xarray
    Found existing installation: xarray 0.12.3+92.ge90e8bc0
    Uninstalling xarray-0.12.3+92.ge90e8bc0:
      Successfully uninstalled xarray-0.12.3+92.ge90e8bc0
  DEPRECATION: Legacy editable install of xarray==0.12.3+93.g2c8ba46b from file:///testbed (setup.py develop) is deprecated. pip 25.0 will enforce this behaviour change. A possible replacement is to add a pyproject.toml or enable --use-pep517, and use setuptools >= 64. If the resulting installation is not behaving as expected, try using --config-settings editable_mode=compat. Please consult the setuptools documentation for more information. Discussion can be found at https://github.com/pypa/pip/issues/11457
  Running setup.py develop for xarray
Successfully installed xarray
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.
============================= test session starts ==============================
platform linux -- Python 3.10.14, pytest-7.4.0, pluggy-1.5.0
rootdir: /testbed
configfile: setup.cfg
plugins: env-1.1.3, cov-5.0.0, hypothesis-6.108.5, xdist-3.6.1
collected 12 items

xarray/tests/test_combine.py ............                                [100%]

=============================== warnings summary ===============================
xarray/core/dask_array_ops.py:12
xarray/core/dask_array_ops.py:12
  /testbed/xarray/core/dask_array_ops.py:12: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    if LooseVersion(dask.__version__) <= LooseVersion("0.18.2"):

xarray/core/npcompat.py:136
xarray/core/npcompat.py:136
  /testbed/xarray/core/npcompat.py:136: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    if LooseVersion(np.__version__) >= LooseVersion("1.13"):

xarray/core/dask_array_compat.py:45
xarray/core/dask_array_compat.py:45
  /testbed/xarray/core/dask_array_compat.py:45: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    if LooseVersion(dask_version) > LooseVersion("0.19.2"):

xarray/core/pdcompat.py:46
  /testbed/xarray/core/pdcompat.py:46: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    if LooseVersion(pd.__version__) < "0.25.0":

../opt/miniconda3/envs/testbed/lib/python3.10/site-packages/setuptools/_distutils/version.py:345
../opt/miniconda3/envs/testbed/lib/python3.10/site-packages/setuptools/_distutils/version.py:345
  /opt/miniconda3/envs/testbed/lib/python3.10/site-packages/setuptools/_distutils/version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    other = LooseVersion(other)

xarray/plot/utils.py:18
xarray/plot/utils.py:18
  /testbed/xarray/plot/utils.py:18: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    if LooseVersion(nc_time_axis.__version__) < LooseVersion("1.2.0"):

xarray/plot/plot.py:287
  /testbed/xarray/plot/plot.py:287: SyntaxWarning: "is" with a literal. Did you mean "=="?
    if args is ():

xarray/tests/__init__.py:60: 15 warnings
  /testbed/xarray/tests/__init__.py:60: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    return version.LooseVersion(vstring)

../opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pydap/lib.py:5
  /opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pydap/lib.py:5: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html
    from pkg_resources import get_distribution

../opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pkg_resources/__init__.py:2871
  /opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`.
  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
    declare_namespace(pkg)

../opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pkg_resources/__init__.py:2871
  /opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap.responses')`.
  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
    declare_namespace(pkg)

../opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pkg_resources/__init__.py:2350
../opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pkg_resources/__init__.py:2350
../opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pkg_resources/__init__.py:2350
  /opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pkg_resources/__init__.py:2350: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`.
  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
    declare_namespace(parent)

../opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pkg_resources/__init__.py:2871
  /opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap.handlers')`.
  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
    declare_namespace(pkg)

../opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pkg_resources/__init__.py:2871
  /opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap.tests')`.
  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
    declare_namespace(pkg)

xarray/tests/test_combine.py::test_nested_combine
xarray/tests/test_combine.py::test_combine_nested
  /testbed/xarray/core/alignment.py:170: FutureWarning: Index.__or__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__or__.  Use index.union(other) instead.
    index = joiner(matching_indexes)

xarray/tests/test_combine.py::test_auto_combine
  /testbed/xarray/tests/test_combine.py:93: FutureWarning: In xarray version 0.13 `auto_combine` will be deprecated. See
  http://xarray.pydata.org/en/stable/combining.html#combining-multi
    result = auto_combine(sample_datasets, concat_dim="x")

xarray/tests/test_combine.py::test_auto_combine
  /testbed/xarray/tests/test_combine.py:93: FutureWarning: Also `open_mfdataset` will no longer accept a `concat_dim` argument.
  To get equivalent behaviour from now on please use the new
  `combine_nested` function instead (or the `combine='nested'` option to
  `open_mfdataset`).The datasets supplied have global dimension coordinates. You may want
  to use the new `combine_by_coords` function (or the
  `combine='by_coords'` option to `open_mfdataset`) to order the datasets
  before concatenation. Alternatively, to continue concatenating based
  on the order the datasets are supplied in future, please use the new
  `combine_nested` function (or the `combine='nested'` option to
  open_mfdataset).
    result = auto_combine(sample_datasets, concat_dim="x")

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
==================================== PASSES ====================================
=========================== short test summary info ============================
PASSED xarray/tests/test_combine.py::test_infer_concat_order_from_positions
PASSED xarray/tests/test_combine.py::test_infer_tile_ids_from_nested_list
PASSED xarray/tests/test_combine.py::test_infer_concat_order_from_coords
PASSED xarray/tests/test_combine.py::test_check_shape_tile_ids
PASSED xarray/tests/test_combine.py::test_combine_nd
PASSED xarray/tests/test_combine.py::test_combine_all_along_first_dim
PASSED xarray/tests/test_combine.py::test_combine_1d
PASSED xarray/tests/test_combine.py::test_new_tile_id
PASSED xarray/tests/test_combine.py::test_nested_combine
PASSED xarray/tests/test_combine.py::test_combine_nested
PASSED xarray/tests/test_combine.py::test_combine_by_coords
PASSED xarray/tests/test_combine.py::test_auto_combine
======================= 12 passed, 39 warnings in 4.78s ========================

