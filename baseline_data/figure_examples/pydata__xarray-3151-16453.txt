Instance ID: pydata__xarray-3151-16453

Baseline 1 (Pynguin):
Predicted Test Suite: # Test cases automatically generated by Pynguin (https://www.pynguin.eu).
# Please check them before you use them.
import pytest
import xarray.core.combine as module_0
import pandas.io.json._json as module_1
import numpy as module_2
import pyarrow.util as module_3
import pyarrow.compute as module_4


@pytest.mark.xfail(strict=True)
def test_case_0():
    str_0 = 'Wrk92{\t"$NO~ Z\x0b"EhUc'
    module_0.combine_nested(str_0, str_0, fill_value=str_0)


@pytest.mark.xfail(strict=True)
def test_case_1():
    dict_0 = {}
    var_0 = module_0.auto_combine(dict_0)
    assert (
        f"{type(var_0).__module__}.{type(var_0).__qualname__}"
        == "xarray.core.dataset.Dataset"
    )
    assert len(var_0) == 0
    var_1 = var_0.min(axis=var_0, **dict_0)
    var_2 = module_0.combine_nested(var_0, dict_0, fill_value=dict_0)
    none_type_0 = None
    module_1.read_json(
        dict_0,
        orient=dict_0,
        dtype=none_type_0,
        convert_axes=dict_0,
        encoding_errors=none_type_0,
        lines=dict_0,
    )


@pytest.mark.xfail(strict=True)
def test_case_2():
    none_type_0 = None
    module_0.combine_nested(none_type_0, none_type_0)


@pytest.mark.xfail(strict=True)
def test_case_3():
    none_type_0 = None
    module_0.auto_combine(
        none_type_0,
        compat=none_type_0,
        data_vars=none_type_0,
        coords=none_type_0,
        fill_value=none_type_0,
    )


@pytest.mark.xfail(strict=True)
def test_case_4():
    none_type_0 = None
    module_0.auto_combine(none_type_0, none_type_0, none_type_0, none_type_0)


def test_case_5():
    dict_0 = {}
    var_0 = module_0.auto_combine(dict_0)
    assert (
        f"{type(var_0).__module__}.{type(var_0).__qualname__}"
        == "xarray.core.dataset.Dataset"
    )
    assert len(var_0) == 0


@pytest.mark.xfail(strict=True)
def test_case_6():
    none_type_0 = None
    module_0.vars_as_keys(none_type_0)


def test_case_7():
    list_0 = []
    var_0 = module_0.vars_as_keys(list_0)
    var_1 = module_0.vars_as_keys(list_0)
    var_2 = module_0.combine_by_coords(var_0)
    assert (
        f"{type(var_2).__module__}.{type(var_2).__qualname__}"
        == "xarray.core.dataset.Dataset"
    )
    assert len(var_2) == 0


@pytest.mark.xfail(strict=True)
def test_case_8():
    dict_0 = module_2.__dir__()
    module_0.auto_combine(dict_0)


@pytest.mark.xfail(strict=True)
def test_case_9():
    str_0 = 'Wrk92{\t"$NO~ Z\x0b"EhUc'
    module_0.combine_by_coords(str_0, data_vars=str_0)


@pytest.mark.xfail(strict=True)
def test_case_10():
    dict_0 = {}
    none_type_0 = None
    var_0 = module_3.guid()
    module_0.auto_combine(
        dict_0,
        compat=dict_0,
        data_vars=none_type_0,
        coords=none_type_0,
        from_openmfds=var_0,
    )


@pytest.mark.xfail(strict=True)
def test_case_11():
    dict_0 = {}
    none_type_0 = None
    module_0.auto_combine(dict_0, none_type_0, none_type_0, fill_value=dict_0)


@pytest.mark.xfail(strict=True)
def test_case_12():
    dict_0 = {}
    var_0 = module_0.auto_combine(dict_0)
    assert (
        f"{type(var_0).__module__}.{type(var_0).__qualname__}"
        == "xarray.core.dataset.Dataset"
    )
    assert len(var_0) == 0
    var_1 = var_0.min(axis=var_0, **dict_0)
    var_2 = module_0.combine_nested(var_0, dict_0, fill_value=dict_0)
    var_3 = module_0.auto_combine(var_0, dict_0, data_vars=var_1, coords=var_0)
    var_4 = module_0.combine_by_coords(var_0, coords=var_2, fill_value=var_3)
    assert (
        f"{type(var_4).__module__}.{type(var_4).__qualname__}"
        == "xarray.core.dataset.Dataset"
    )
    assert len(var_4) == 0
    module_0.combine_by_coords(var_2, var_0)


@pytest.mark.xfail(strict=True)
def test_case_13():
    dict_0 = {}
    var_0 = module_0.auto_combine(dict_0, data_vars=dict_0)
    assert (
        f"{type(var_0).__module__}.{type(var_0).__qualname__}"
        == "xarray.core.dataset.Dataset"
    )
    assert len(var_0) == 0
    var_1 = var_0.min(skipna=var_0)
    tuple_0 = (var_0, var_0)
    var_2 = module_0.combine_nested(tuple_0, var_1, coords=var_0)
    module_4.log2(dict_0)


@pytest.mark.xfail(strict=True)
def test_case_14():
    dict_0 = {}
    var_0 = module_0.auto_combine(dict_0, data_vars=dict_0)
    assert (
        f"{type(var_0).__module__}.{type(var_0).__qualname__}"
        == "xarray.core.dataset.Dataset"
    )
    assert len(var_0) == 0
    var_1 = var_0.transpose()
    var_2 = var_0.min(skipna=var_0)
    tuple_0 = (var_0, var_0)
    module_0.auto_combine(tuple_0, compat=var_1, fill_value=var_1)


@pytest.mark.xfail(strict=True)
def test_case_15():
    dict_0 = {}
    var_0 = module_0.auto_combine(dict_0, data_vars=dict_0)
    assert (
        f"{type(var_0).__module__}.{type(var_0).__qualname__}"
        == "xarray.core.dataset.Dataset"
    )
    assert len(var_0) == 0
    var_1 = var_0.min(skipna=var_0)
    tuple_0 = (var_0, var_0)
    var_2 = module_0.combine_nested(tuple_0, var_1, coords=var_0)
    module_0.auto_combine(var_2, dict_0, var_1, coords=dict_0)

Coverage: 62.857142857142854
Mutation Score: 1.9599999999999937

Baseline 2 (CodaMosa):
Predicted Test Suite: import builtins as module_2
import xarray.core.combine as module_0
import xarray.core.dataset as module_1

def test_case_1():
    pass


def test_case_2():
    tuple_0 = ()
    var_0 = module_0.auto_combine(tuple_0)
    assert len(var_0) == 0
    assert module_1.Dataset.load_store is not None
    assert module_1.Dataset.from_dataframe is not None
    assert module_1.Dataset.from_dict is not None


def test_case_3():
    str_0 = 'mperature'
    str_1 = '\x0b+JP>p~c:#9<iLDrg '
    var_0 = {str_0: str_1}
    int_0 = 2
    int_1 = [int_0, int_0, int_0]
    int_2 = {str_1: int_1}
    dataset_0 = module_1.Dataset(var_0, int_2)
    var_1 = {str_0: str_1}
    int_3 = [int_0, int_0, int_2]
    int_4 = {str_1: int_3}
    dataset_1 = module_1.Dataset(var_1, int_4)
    dataset_2 = [dataset_0, dataset_1]
    var_2 = module_0.auto_combine(dataset_2)
    assert len(dataset_0) == 1
    assert len(dataset_1) == 1
    assert len(var_2) == 1
    assert module_1.TYPE_CHECKING is False
    assert module_1.ALL_DIMS is not None
    assert module_1.OPTIONS == {'display_width': 80, 'arithmetic_join': 'inner', 'enable_cftimeindex': True, 'file_cache_maxsize': 128, 'warn_for_unclosed_files': False, 'cmap_sequential': 'viridis', 'cmap_divergent': 'RdBu_r', 'keep_attrs': 'default'}
    assert module_1.dask_array_type == ()
    assert module_1.Dataset.load_store is not None
    assert module_1.Dataset.from_dataframe is not None
    assert module_1.Dataset.from_dict is not None


def test_case_4():
    str_0 = '.PDqQf#]'
    var_0 = {}
    int_0 = 0
    int_1 = [int_0, int_0, int_0]
    int_2 = {str_0: int_1}
    dataset_0 = module_1.Dataset(var_0, int_2)
    dataset_1 = [dataset_0, dataset_0]
    var_1 = module_0.auto_combine(dataset_1)
    assert len(dataset_0) == 0
    assert len(var_1) == 0
    assert module_1.TYPE_CHECKING is False
    assert module_1.ALL_DIMS is not None
    assert module_1.OPTIONS == {'display_width': 80, 'arithmetic_join': 'inner', 'enable_cftimeindex': True, 'file_cache_maxsize': 128, 'warn_for_unclosed_files': False, 'cmap_sequential': 'viridis', 'cmap_divergent': 'RdBu_r', 'keep_attrs': 'default'}
    assert module_1.dask_array_type == ()
    assert module_1.Dataset.load_store is not None
    assert module_1.Dataset.from_dataframe is not None
    assert module_1.Dataset.from_dict is not None


def test_case_5():
    str_0 = 'emperature'
    str_1 = '\x0b+JP>p~c:#9<iLDrg '
    var_0 = {}
    int_0 = 1
    int_1 = {str_1: int_0}
    dataset_0 = module_1.Dataset(var_0, int_1)
    var_1 = {str_0: str_0}
    int_2 = {str_1: int_0}
    dataset_1 = module_1.Dataset(var_1, int_2)
    dataset_2 = [dataset_0, dataset_1]
    var_2 = module_0.auto_combine(dataset_2)
    assert len(dataset_0) == 0
    assert len(dataset_1) == 1
    assert len(var_2) == 1
    assert module_1.TYPE_CHECKING is False
    assert module_1.ALL_DIMS is not None
    assert module_1.OPTIONS == {'display_width': 80, 'arithmetic_join': 'inner', 'enable_cftimeindex': True, 'file_cache_maxsize': 128, 'warn_for_unclosed_files': False, 'cmap_sequential': 'viridis', 'cmap_divergent': 'RdBu_r', 'keep_attrs': 'default'}
    assert module_1.dask_array_type == ()
    assert module_1.Dataset.load_store is not None
    assert module_1.Dataset.from_dataframe is not None
    assert module_1.Dataset.from_dict is not None


def test_case_6():
    str_0 = 'emperature'
    str_1 = '\x0b+JP>p~c:#9<iLDrg '
    float_0 = 23.57
    float_1 = 20.77
    float_2 = [float_0, float_0, float_1]
    var_0 = (str_1, float_2)
    var_1 = {str_0: var_0}
    int_0 = 1
    int_1 = 2
    int_2 = [int_0, int_0, int_1]
    int_3 = {str_1: int_2}
    dataset_0 = module_1.Dataset(var_1, int_3)
    var_2 = {str_0: str_0}
    int_4 = {str_1: int_1}
    dataset_1 = module_1.Dataset(var_2, int_4)
    dataset_2 = [dataset_0, dataset_1]
    var_3 = module_0.auto_combine(dataset_2)
    assert len(dataset_0) == 1
    assert len(dataset_1) == 1
    assert len(var_3) == 1
    assert module_1.TYPE_CHECKING is False
    assert module_1.ALL_DIMS is not None
    assert module_1.OPTIONS == {'display_width': 80, 'arithmetic_join': 'inner', 'enable_cftimeindex': True, 'file_cache_maxsize': 128, 'warn_for_unclosed_files': False, 'cmap_sequential': 'viridis', 'cmap_divergent': 'RdBu_r', 'keep_attrs': 'default'}
    assert module_1.dask_array_type == ()
    assert module_1.Dataset.load_store is not None
    assert module_1.Dataset.from_dataframe is not None
    assert module_1.Dataset.from_dict is not None


def test_case_7():
    str_0 = 'temperature'
    str_1 = 'humidity'
    str_2 = 'x'
    int_0 = 15
    int_1 = 18
    int_2 = 20
    int_3 = [int_0, int_1, int_2]
    var_0 = (str_2, int_3)
    var_1 = {str_0: var_0, str_1: int_2}
    int_4 = 0
    int_5 = 1
    int_6 = 2
    int_7 = [int_4, int_5, int_6]
    int_8 = {str_2: int_7}
    dataset_0 = module_1.Dataset(var_1, int_8)
    int_9 = 22
    int_10 = 25
    int_11 = [int_9, int_10, int_2]
    var_2 = (str_2, int_11)
    float_0 = -1224.3473645371853
    float_1 = 0.9
    float_2 = [float_0, float_1, float_1]
    var_3 = (str_2, float_2)
    var_4 = {str_0: var_2, str_1: var_3}
    int_12 = 3
    int_13 = 4
    int_14 = 5
    int_15 = [int_12, int_13, int_14]
    int_16 = {str_2: int_15}
    dataset_1 = module_1.Dataset(var_4, int_16)
    dataset_2 = [dataset_0, dataset_1]
    var_5 = module_0.combine_by_coords(dataset_2)
    assert len(dataset_0) == 2
    assert len(dataset_1) == 2
    assert len(var_5) == 2
    assert module_1.TYPE_CHECKING is False
    assert module_1.ALL_DIMS is not None
    assert module_1.OPTIONS == {'display_width': 80, 'arithmetic_join': 'inner', 'enable_cftimeindex': True, 'file_cache_maxsize': 128, 'warn_for_unclosed_files': False, 'cmap_sequential': 'viridis', 'cmap_divergent': 'RdBu_r', 'keep_attrs': 'default'}
    assert module_1.dask_array_type == ()
    assert module_1.Dataset.load_store is not None
    assert module_1.Dataset.from_dataframe is not None
    assert module_1.Dataset.from_dict is not None# Automatically generated by Pynguin.


def test_case_8():
    try:
        str_0 = '$|4lpKKinoxM0k;rP4'
        bool_0 = False
        var_0 = module_0.combine_nested(str_0, bool_0)
    except BaseException:
        pass


def test_case_9():
    try:
        str_0 = '[\x0b\'F){BF0"yP'
        var_0 = module_0.auto_combine(str_0)
    except BaseException:
        pass


def test_case_10():
    try:
        str_0 = '+afL\\dc)>'
        complex_0 = None
        var_0 = module_0.combine_nested(str_0, complex_0)
    except BaseException:
        pass


def test_case_11():
    try:
        bool_0 = True
        float_0 = 3215.51
        list_0 = []
        int_0 = None
        float_1 = 1712.514
        var_0 = module_0.combine_nested(list_0, int_0, float_1)
        assert len(var_0) == 0
        assert module_1.Dataset.load_store is not None
        assert module_1.Dataset.from_dataframe is not None
        assert module_1.Dataset.from_dict is not None
        set_0 = {bool_0, bool_0, float_0, float_0}
        tuple_0 = ()
        list_1 = [float_0, bool_0, tuple_0, tuple_0]
        var_1 = module_0.auto_combine(bool_0, set_0, set_0, list_1)
    except BaseException:
        pass


def test_case_12():
    try:
        str_0 = '|?r/82hoK>\x0b>\\J,8'
        int_0 = 15
        var_0 = module_0.combine_by_coords(str_0, str_0, int_0)
    except BaseException:
        pass


def test_case_13():
    try:
        int_0 = 997
        var_0 = module_0.combine_by_coords(int_0)
    except BaseException:
        pass


def test_case_14():
    try:
        bytes_0 = b'\xe3\xd7'
        tuple_0 = None
        var_0 = module_0.auto_combine(bytes_0, tuple_0)
    except BaseException:
        pass


def test_case_15():
    try:
        bytes_0 = b'\xf9\xc8B\xb1\xc0\xcb\x81\x8b\x06\xbfh'
        str_0 = 'A{%'
        float_0 = 1818.58076
        var_0 = module_0.combine_nested(bytes_0, str_0, float_0, str_0)
    except BaseException:
        pass


def test_case_16():
    try:
        list_0 = []
        tuple_0 = ()
        var_0 = module_0.auto_combine(list_0, tuple_0, tuple_0)
    except BaseException:
        pass


def test_case_17():
    try:
        dict_0 = None
        list_0 = [dict_0, dict_0, dict_0]
        list_1 = []
        var_0 = module_0.combine_nested(list_0, dict_0, list_1)
    except BaseException:
        pass


def test_case_18():
    try:
        tuple_0 = ()
        var_0 = module_0.combine_by_coords(tuple_0)
        assert len(var_0) == 0
        assert module_1.Dataset.load_store is not None
        assert module_1.Dataset.from_dataframe is not None
        assert module_1.Dataset.from_dict is not None
        str_0 = 'V97wgK'
        str_1 = 'J<`'
        str_2 = '$[MV!/\x0b\\]<DE'
        dict_0 = {str_0: str_1, str_2: str_0}
        object_0 = module_2.object(**dict_0)
    except BaseException:
        pass


def test_case_19():
    try:
        dict_0 = {}
        bool_0 = None
        var_0 = module_0.auto_combine(dict_0, bool_0)
        assert len(var_0) == 0
        assert module_1.Dataset.load_store is not None
        assert module_1.Dataset.from_dataframe is not None
        assert module_1.Dataset.from_dict is not None
        set_0 = None
        float_0 = None
        bool_1 = False
        var_1 = module_0.combine_by_coords(set_0, float_0, bool_1)
    except BaseException:
        pass


def test_case_20():
    try:
        str_0 = '/n.~!}y5!&~1'
        list_0 = [str_0, str_0, str_0, str_0]
        list_1 = [str_0, list_0, str_0]
        var_0 = module_0.combine_nested(list_0, str_0, list_1)
    except BaseException:
        pass


def test_case_21():
    try:
        str_0 = 'p%6Rc\t)H?-'
        dataset_0 = module_1.Dataset()
        dataset_1 = dataset_0.compute()
        var_0 = module_0.combine_by_coords(dataset_1)
        assert len(dataset_0) == 0
        assert len(dataset_1) == 0
        assert len(var_0) == 0
        assert module_1.TYPE_CHECKING is False
        assert module_1.ALL_DIMS is not None
        assert module_1.OPTIONS == {'display_width': 80, 'arithmetic_join': 'inner', 'enable_cftimeindex': True, 'file_cache_maxsize': 128, 'warn_for_unclosed_files': False, 'cmap_sequential': 'viridis', 'cmap_divergent': 'RdBu_r', 'keep_attrs': 'default'}
        assert module_1.dask_array_type == ()
        assert module_1.Dataset.load_store is not None
        assert module_1.Dataset.from_dataframe is not None
        assert module_1.Dataset.from_dict is not None
        var_1 = module_0.combine_nested(str_0, dataset_0, dataset_1)
        assert var_1 == 'p%6Rc\t)H?-'
        iterator_0 = dataset_1.__iter__()
        dataset_1.close()
        str_1 = 'x.;m#q?2VmQ+dce8sl/'
        var_2 = module_0.combine_by_coords(str_1)
    except BaseException:
        pass


def test_case_22():
    try:
        str_0 = "B_\r?'6WK\n1Ss'o2}.LoZ"
        str_1 = '\x0b+JP>p~c:#9<iLDrg '
        float_0 = 23.57
        var_0 = {str_0: str_1, str_0: float_0}
        int_0 = -5
        int_1 = [int_0, int_0, int_0]
        int_2 = {str_1: int_1}
        dataset_0 = module_1.Dataset(var_0, int_2)
        float_1 = [float_0, float_0, float_0]
        var_1 = (str_1, float_1)
        var_2 = {str_0: var_1}
        int_3 = 4
        int_4 = 63
        int_5 = [int_0, int_3, int_4]
        int_6 = {str_1: int_5}
        dataset_1 = module_1.Dataset(var_2, int_6)
        dataset_2 = [dataset_0, dataset_1]
        var_3 = module_0.auto_combine(dataset_2)
        assert len(dataset_0) == 1
        assert len(dataset_1) == 1
        assert len(var_3) == 1
        assert module_1.TYPE_CHECKING is False
        assert module_1.ALL_DIMS is not None
        assert module_1.OPTIONS == {'display_width': 80, 'arithmetic_join': 'inner', 'enable_cftimeindex': True, 'file_cache_maxsize': 128, 'warn_for_unclosed_files': False, 'cmap_sequential': 'viridis', 'cmap_divergent': 'RdBu_r', 'keep_attrs': 'default'}
        assert module_1.dask_array_type == ()
        assert module_1.Dataset.load_store is not None
        assert module_1.Dataset.from_dataframe is not None
        assert module_1.Dataset.from_dict is not None
        var_4 = module_0.combine_by_coords(str_1)
    except BaseException:
        pass


def test_case_23():
    try:
        str_0 = '.PDqQ!f#]t'
        var_0 = {}
        int_0 = 0
        int_1 = {str_0: int_0}
        dataset_0 = module_1.Dataset(var_0, int_1)
        int_2 = {str_0: int_0}
        dataset_1 = [dataset_0, int_2, dataset_0, str_0]
        var_1 = module_0.auto_combine(dataset_1)
    except BaseException:
        pass


def test_case_24():
    try:
        str_0 = 'emperature'
        str_1 = '\x0b+JP>p~c:#9<iLDrg '
        float_0 = 23.57
        float_1 = 20.77
        float_2 = [float_0, float_0, float_1]
        var_0 = (str_1, float_2)
        var_1 = {str_0: var_0}
        int_0 = 2
        int_1 = {}
        dataset_0 = module_1.Dataset(var_1, int_1)
        var_2 = {}
        int_2 = 54
        int_3 = [int_0, int_2, int_2]
        int_4 = {str_1: int_3}
        dataset_1 = module_1.Dataset(var_2, int_4)
        dataset_2 = [dataset_0]
        var_3 = module_0.auto_combine(dataset_2)
        assert len(dataset_0) == 1
        assert len(dataset_1) == 0
        assert len(var_3) == 1
        assert module_1.TYPE_CHECKING is False
        assert module_1.ALL_DIMS is not None
        assert module_1.OPTIONS == {'display_width': 80, 'arithmetic_join': 'inner', 'enable_cftimeindex': True, 'file_cache_maxsize': 128, 'warn_for_unclosed_files': False, 'cmap_sequential': 'viridis', 'cmap_divergent': 'RdBu_r', 'keep_attrs': 'default'}
        assert module_1.dask_array_type == ()
        assert module_1.Dataset.load_store is not None
        assert module_1.Dataset.from_dataframe is not None
        assert module_1.Dataset.from_dict is not None
        str_2 = 'x.;m>q?2VmQ+dce8sl~'
        var_4 = module_0.combine_by_coords(str_2)
    except BaseException:
        pass


def test_case_25():
    try:
        str_0 = '7+'
        str_1 = 'humidity'
        str_2 = 'A<'
        int_0 = 18
        var_0 = {str_0: str_0, str_1: int_0}
        int_1 = 0
        int_2 = 1
        int_3 = 14
        int_4 = [int_1, int_2, int_3]
        int_5 = {str_2: int_4}
        dataset_0 = module_1.Dataset(var_0, int_5)
        int_6 = 3
        int_7 = 4
        int_8 = 5
        int_9 = [int_6, int_7, int_8]
        int_10 = {str_2: int_9}
        dataset_1 = module_1.Dataset(dataset_0, int_10)
        dataset_2 = [dataset_0, dataset_1]
        var_1 = module_0.combine_by_coords(dataset_2)
    except BaseException:
        pass


def test_case_26():
    try:
        dataset_0 = module_1.Dataset()
        dataset_1 = dataset_0.compute()
        var_0 = module_0.combine_by_coords(dataset_1)
        assert len(dataset_0) == 0
        assert len(dataset_1) == 0
        assert len(var_0) == 0
        assert module_1.TYPE_CHECKING is False
        assert module_1.ALL_DIMS is not None
        assert module_1.OPTIONS == {'display_width': 80, 'arithmetic_join': 'inner', 'enable_cftimeindex': True, 'file_cache_maxsize': 128, 'warn_for_unclosed_files': False, 'cmap_sequential': 'viridis', 'cmap_divergent': 'RdBu_r', 'keep_attrs': 'default'}
        assert module_1.dask_array_type == ()
        assert module_1.Dataset.load_store is not None
        assert module_1.Dataset.from_dataframe is not None
        assert module_1.Dataset.from_dict is not None
        list_0 = [var_0]
        var_1 = module_0.combine_by_coords(list_0, dataset_1)
    except BaseException:
        pass


def test_case_27():
    try:
        str_0 = 'Wf]&[A"kp\r?5thhDw5~`'
        str_1 = '\x0b+JP>p~c:#9<iLDrg '
        dataset_0 = module_1.Dataset()
        dataset_1 = dataset_0.compute()
        var_0 = module_0.combine_by_coords(dataset_1)
        assert len(dataset_0) == 0
        assert len(dataset_1) == 0
        assert len(var_0) == 0
        assert module_1.TYPE_CHECKING is False
        assert module_1.ALL_DIMS is not None
        assert module_1.OPTIONS == {'display_width': 80, 'arithmetic_join': 'inner', 'enable_cftimeindex': True, 'file_cache_maxsize': 128, 'warn_for_unclosed_files': False, 'cmap_sequential': 'viridis', 'cmap_divergent': 'RdBu_r', 'keep_attrs': 'default'}
        assert module_1.dask_array_type == ()
        assert module_1.Dataset.load_store is not None
        assert module_1.Dataset.from_dataframe is not None
        assert module_1.Dataset.from_dict is not None
        var_1 = module_0.combine_nested(str_0, dataset_0, dataset_1)
        assert var_1 == 'Wf]&[A"kp\r?5thhDw5~`'
        float_0 = 23.57
        int_0 = 0
        int_1 = [int_0, int_0, int_0]
        int_2 = {str_1: int_1}
        dataset_2 = dataset_0.__copy__()
        assert len(dataset_2) == 0
        float_1 = 6.97
        dataset_1.close()
        float_2 = [float_1, float_0, float_0]
        var_2 = (str_1, float_2)
        var_3 = {str_0: var_2}
        int_3 = 4
        int_4 = 11
        int_5 = [int_2, int_3, int_4]
        int_6 = {str_1: int_5}
        dataset_3 = module_1.Dataset(var_3, int_6)
        assert len(dataset_3) == 1
        dataset_4 = [dataset_2, dataset_3]
        var_4 = module_0.auto_combine(dataset_4)
        assert len(var_4) == 1
        bool_0 = True
        list_0 = [int_5, bool_0, var_4, int_0]
        list_1 = [list_0, int_0]
        dict_0 = {}
        iterator_0 = dataset_3.__iter__()
        var_5 = module_0.combine_nested(list_0, list_1, dict_0, dataset_0, iterator_0)
    except BaseException:
        pass


def test_case_28():
    try:
        list_0 = []
        list_1 = [list_0]
        str_0 = 'Wf]&[A"kp\r?5thhDw5~`'
        int_0 = 566
        dict_0 = {str_0: int_0}
        var_0 = module_0.combine_nested(list_1, dict_0)
    except BaseException:
        pass


def test_case_29():
    try:
        str_0 = 'p%6Rc\t)H?-'
        str_1 = ''
        float_0 = 25.50677905964062
        float_1 = [float_0, float_0]
        var_0 = {str_0: float_1}
        int_0 = 0
        int_1 = [int_0, int_0, int_0]
        int_2 = {str_1: int_1}
        dataset_0 = module_1.Dataset(var_0, int_2)
        dataset_1 = [dataset_0, dataset_0]
        var_1 = module_0.auto_combine(dataset_1)
    except BaseException:
        pass


def test_case_30():
    try:
        dataset_0 = module_1.Dataset()
        bytes_0 = b'\x05\xd1\x11'
        var_0 = module_0.combine_nested(dataset_0, dataset_0, bytes_0)
        str_0 = "B_\r?6WK\n1Ss'o2}.LoZ"
        str_1 = '\x0b+JP>p~c:#9<iLDrg '
        dict_0 = {}
        var_1 = module_0.vars_as_keys(dict_0)
        float_0 = 23.57
        float_1 = 20.488599070581298
        float_2 = [float_0, float_0, float_1]
        var_2 = (str_1, float_2)
        var_3 = {str_0: var_2}
        int_0 = 1
        int_1 = -1
        dataset_1 = dataset_0.load()
        int_2 = [int_0, int_0, int_1]
        int_3 = {str_1: int_2}
        dataset_2 = module_1.Dataset(var_3, int_3)
        float_3 = 1449.0
        float_4 = [float_0, float_0, float_3]
        var_4 = (str_1, float_4)
        var_5 = {str_0: var_4}
        int_4 = -22
        int_5 = [int_1, int_4, int_3]
        int_6 = {str_1: int_5}
        dataset_3 = module_1.Dataset(var_5, int_6)
        list_0 = [dataset_3, dataset_1]
        set_0 = set()
        var_6 = module_0.combine_by_coords(set_0)
        assert len(dataset_0) == 0
        assert len(var_0) == 0
        assert var_1 == ()
        assert len(dataset_1) == 0
        assert len(dataset_2) == 1
        assert len(dataset_3) == 1
        assert len(var_6) == 0
        assert module_1.TYPE_CHECKING is False
        assert module_1.ALL_DIMS is not None
        assert module_1.OPTIONS == {'display_width': 80, 'arithmetic_join': 'inner', 'enable_cftimeindex': True, 'file_cache_maxsize': 128, 'warn_for_unclosed_files': False, 'cmap_sequential': 'viridis', 'cmap_divergent': 'RdBu_r', 'keep_attrs': 'default'}
        assert module_1.dask_array_type == ()
        assert module_1.Dataset.load_store is not None
        assert module_1.Dataset.from_dataframe is not None
        assert module_1.Dataset.from_dict is not None
        var_7 = module_0.auto_combine(list_0)
        assert len(var_7) == 1
        dataset_4 = [dataset_2, dataset_3]
        var_8 = module_0.auto_combine(dataset_4)
        assert len(var_8) == 1
        int_7 = 1003
        dict_1 = {str_0: int_7}
        int_8 = -2012
        var_9 = module_0.auto_combine(list_0, float_3, dict_1, int_8)
    except BaseException:
        pass

Coverage: 90.85714285714286
Mutation Score: 0.0

Baseline 3 (GPT-4o):
Predicted Test Suite: 
import pytest
import pandas as pd
from collections import OrderedDict
from xarray import DataArray, Dataset, concat, combine_by_coords, combine_nested
from xarray.core.combine import (
    _new_tile_id, _check_shape_tile_ids, _combine_all_along_first_dim, 
    _combine_nd, _infer_concat_order_from_positions, _infer_concat_order_from_coords,
    combine_by_coords, combine_nested, auto_combine
)

def create_test_datasets():
    ds1 = Dataset({'temperature': ('x', [15, 20, 25])}, coords={'x': [0, 1, 2]})
    ds2 = Dataset({'temperature': ('x', [30, 35, 40])}, coords={'x': [3, 4, 5]})
    ds3 = Dataset({'temperature': ('x', [45, 50, 55])}, coords={'x': [6, 7, 8]})
    return [ds1, ds2, ds3]

def test_infer_concat_order_from_positions():
    datasets = [[1, 2], [3, 4]]
    expected = OrderedDict([((0, 0), 1), ((0, 1), 2), ((1, 0), 3), ((1, 1), 4)])
    result = _infer_concat_order_from_positions(datasets)
    assert result == expected

def test_infer_tile_ids_from_nested_list():
    entry = [[1, 2], [3, 4]]
    expected = [((0, 0), 1), ((0, 1), 2), ((1, 0), 3), ((1, 1), 4)]
    result = list(_infer_tile_ids_from_nested_list(entry, ()))
    assert result == expected

def test_infer_concat_order_from_coords():
    ds1 = Dataset({'temperature': ('x', [15, 20, 25])}, coords={'x': [0, 1, 2]})
    ds2 = Dataset({'temperature': ('x', [30, 35, 40])}, coords={'x': [3, 4, 5]})
    datasets = [ds1, ds2]
    combined_ids, concat_dims = _infer_concat_order_from_coords(datasets)
    assert concat_dims == ['x']
    assert list(combined_ids.keys()) == [(0,), (1,)]

def test_check_shape_tile_ids():
    combined_tile_ids = OrderedDict([((0, 0), 1), ((0, 1), 2), ((1, 0), 3), ((1, 1), 4)])
    _check_shape_tile_ids(combined_tile_ids)  # Should not raise an error

    with pytest.raises(ValueError):
        combined_tile_ids_invalid = OrderedDict([((0,), 1), ((0, 1), 2)])
        _check_shape_tile_ids(combined_tile_ids_invalid)

def test_combine_nd():
    datasets = create_test_datasets()
    combined_ids = OrderedDict({(0,): datasets[0], (1,): datasets[1], (2,): datasets[2]})
    concat_dims = ['x']
    combined_ds = _combine_nd(combined_ids, concat_dims)
    assert combined_ds.dims['x'] == 9

def test_combine_all_along_first_dim():
    datasets = create_test_datasets()
    combined_ids = OrderedDict({(0,): datasets[0], (1,): datasets[1]})
    new_combined_ids = _combine_all_along_first_dim(combined_ids, 'x', 'all', 'different', 'no_conflicts')
    assert len(new_combined_ids) == 1
    assert list(new_combined_ids.values())[0].dims['x'] == 6

def test_combine_1d():
    datasets = create_test_datasets()[:2]
    combined = _combine_1d(datasets, 'x')
    assert combined.dims['x'] == 6

def test_new_tile_id():
    tile_id = (0, 1, 2)
    ds = Dataset()
    result = _new_tile_id((tile_id, ds))
    assert result == (1, 2)

def test_nested_combine():
    datasets = create_test_datasets()
    combined = combine_nested(datasets, concat_dim='x')
    assert combined.dims['x'] == 9

def test_combine_by_coords():
    datasets = create_test_datasets()
    combined = combine_by_coords(datasets)
    assert combined.dims['x'] == 9

def test_auto_combine():
    datasets = create_test_datasets()
    combined = auto_combine(datasets, concat_dim='x')
    assert combined.dims['x'] == 9

Coverage: 77.71428571428571
Mutation Score: 35.69

Approach:
Predicted Test Suite: import pytest
import pandas as pd
from collections import OrderedDict
from xarray import DataArray, Dataset, concat, combine_by_coords, combine_nested
from xarray.core.combine import (
    _new_tile_id, _check_shape_tile_ids, _combine_all_along_first_dim,
    _combine_nd, _infer_concat_order_from_positions, _infer_concat_order_from_coords,
    _infer_tile_ids_from_nested_list, _combine_1d, combine_by_coords, combine_nested, auto_combine
)

def create_test_datasets():
    ds1 = Dataset({'temperature': ('x', [15, 20, 25])}, coords={'x': [0, 1, 2]})
    ds2 = Dataset({'temperature': ('x', [30, 35, 40])}, coords={'x': [3, 4, 5]})
    ds3 = Dataset({'temperature': ('x', [45, 50, 55])}, coords={'x': [6, 7, 8]})
    return [ds1, ds2, ds3]

def test_infer_concat_order_from_positions():
    datasets = [[1, 2], [3, 4]]
    expected = OrderedDict([((0, 0), 1), ((0, 1), 2), ((1, 0), 3), ((1, 1), 4)])
    result = _infer_concat_order_from_positions(datasets)
    assert result == expected

def test_infer_tile_ids_from_nested_list():
    entry = [[1, 2], [3, 4]]
    expected = [((0, 0), 1), ((0, 1), 2), ((1, 0), 3), ((1, 1), 4)]
    result = list(_infer_tile_ids_from_nested_list(entry, ()))
    assert result == expected

def test_infer_concat_order_from_coords():
    ds1 = Dataset({'temperature': ('x', [15, 20, 25])}, coords={'x': [0, 1, 2]})
    ds2 = Dataset({'temperature': ('x', [30, 35, 40])}, coords={'x': [3, 4, 5]})
    datasets = [ds1, ds2]
    combined_ids, concat_dims = _infer_concat_order_from_coords(datasets)
    assert concat_dims == ['x']
    assert list(combined_ids.keys()) == [(0,), (1,)]

def test_check_shape_tile_ids():
    combined_tile_ids = OrderedDict([((0, 0), 1), ((0, 1), 2), ((1, 0), 3), ((1, 1), 4)])
    _check_shape_tile_ids(combined_tile_ids)  # Should not raise an error

    with pytest.raises(ValueError):
        combined_tile_ids_invalid = OrderedDict([((0,), 1), ((0, 1), 2)])
        _check_shape_tile_ids(combined_tile_ids_invalid)

def test_combine_nd():
    datasets = create_test_datasets()
    combined_ids = OrderedDict({(0,): datasets[0], (1,): datasets[1], (2,): datasets[2]})
    concat_dims = ['x']
    combined_ds = _combine_nd(combined_ids, concat_dims)
    assert combined_ds.dims['x'] == 9

def test_combine_all_along_first_dim():
    datasets = create_test_datasets()
    combined_ids = OrderedDict({(0,): datasets[0], (1,): datasets[1]})
    new_combined_ids = _combine_all_along_first_dim(combined_ids, 'x', 'all', 'different', 'no_conflicts')
    assert len(new_combined_ids) == 1
    assert list(new_combined_ids.values())[0].dims['x'] == 6

def test_combine_1d():
    datasets = create_test_datasets()[:2]
    combined = _combine_1d(datasets, 'x')
    assert combined.dims['x'] == 6

def test_new_tile_id():
    tile_id = (0, 1, 2)
    ds = Dataset()
    result = _new_tile_id((tile_id, ds))
    assert result == (1, 2)

def test_nested_combine():
    datasets = create_test_datasets()
    combined = combine_nested(datasets, concat_dim='x')
    assert combined.dims['x'] == 9

def test_combine_by_coords():
    datasets = create_test_datasets()
    combined = combine_by_coords(datasets)
    assert combined.dims['x'] == 9

def test_auto_combine():
    datasets = create_test_datasets()
    combined = auto_combine(datasets, concat_dim='x')
    assert combined.dims['x'] == 9
Coverage: 77.71428571428571
Mutation Score: 35.69
Output: On branch main
Untracked files:
  (use "git add <file>..." to include in what will be committed)
        xarray/tests/test_combine.py

nothing added to commit but untracked files present (use "git add" to track)
commit b90907ca1e683ca93e7c4924db180b3615424821
Author: TestGenEval <>
Date:   Wed Dec 11 11:56:56 2024 +0000

    Testing fixes

diff --git a/xarray/core/combine.py b/xarray/core/combine.py
index 37ae903b..5718698f 100644
--- a/xarray/core/combine.py
+++ b/xarray/core/combine.py
@@ -501,14 +501,13 @@ def combine_by_coords(datasets, compat='no_conflicts', data_vars='all',
                                    fill_value=fill_value)

         # Check the overall coordinates are monotonically increasing
-        for dim in concatenated.dims:
-            if dim in concatenated:
-                indexes = concatenated.indexes.get(dim)
-                if not (indexes.is_monotonic_increasing
-                        or indexes.is_monotonic_decreasing):
-                    raise ValueError("Resulting object does not have monotonic"
-                                     " global indexes along dimension {}"
-                                     .format(dim))
+        for dim in concat_dims:
+            indexes = concatenated.indexes.get(dim)
+            if not (indexes.is_monotonic_increasing
+                    or indexes.is_monotonic_decreasing):
+                raise ValueError("Resulting object does not have monotonic"
+                                 " global indexes along dimension {}"
+                                 .format(dim))
         concatenated_grouped_by_data_vars.append(concatenated)

     return merge(concatenated_grouped_by_data_vars, compat=compat,
diff --git a/xarray/tests/test_combine.py b/xarray/tests/test_combine.py
deleted file mode 100644
index 77e2993b..00000000
--- a/xarray/tests/test_combine.py
+++ /dev/null
@@ -1,777 +0,0 @@
-from collections import OrderedDict
-from itertools import product
-from datetime import datetime
-
-import numpy as np
-import pytest
-
-from xarray import (DataArray, Dataset, concat, combine_by_coords,
-                    combine_nested)
-from xarray import auto_combine
-from xarray.core import dtypes
-from xarray.core.combine import (
-    _new_tile_id, _check_shape_tile_ids, _combine_all_along_first_dim,
-    _combine_nd, _infer_concat_order_from_positions,
-    _infer_concat_order_from_coords)
-
-from . import (assert_identical, assert_equal, raises_regex)
-from .test_dataset import create_test_data
-
-
-def assert_combined_tile_ids_equal(dict1, dict2):
-    assert len(dict1) == len(dict2)
-    for k, v in dict1.items():
-        assert k in dict2.keys()
-        assert_equal(dict1[k], dict2[k])
-
-
-class TestTileIDsFromNestedList:
-    def test_1d(self):
-        ds = create_test_data
-        input = [ds(0), ds(1)]
-
-        expected = {(0,): ds(0), (1,): ds(1)}
-        actual = _infer_concat_order_from_positions(input)
-        assert_combined_tile_ids_equal(expected, actual)
-
-    def test_2d(self):
-        ds = create_test_data
-        input = [[ds(0), ds(1)], [ds(2), ds(3)], [ds(4), ds(5)]]
-
-        expected = {(0, 0): ds(0), (0, 1): ds(1),
-                    (1, 0): ds(2), (1, 1): ds(3),
-                    (2, 0): ds(4), (2, 1): ds(5)}
-        actual = _infer_concat_order_from_positions(input)
-        assert_combined_tile_ids_equal(expected, actual)
-
-    def test_3d(self):
-        ds = create_test_data
-        input = [[[ds(0), ds(1)], [ds(2), ds(3)], [ds(4), ds(5)]],
-                 [[ds(6), ds(7)], [ds(8), ds(9)], [ds(10), ds(11)]]]
-
-        expected = {(0, 0, 0): ds(0), (0, 0, 1): ds(1),
-                    (0, 1, 0): ds(2), (0, 1, 1): ds(3),
-                    (0, 2, 0): ds(4), (0, 2, 1): ds(5),
-                    (1, 0, 0): ds(6), (1, 0, 1): ds(7),
-                    (1, 1, 0): ds(8), (1, 1, 1): ds(9),
-                    (1, 2, 0): ds(10), (1, 2, 1): ds(11)}
-        actual = _infer_concat_order_from_positions(input)
-        assert_combined_tile_ids_equal(expected, actual)
-
-    def test_single_dataset(self):
-        ds = create_test_data(0)
-        input = [ds]
-
-        expected = {(0,): ds}
-        actual = _infer_concat_order_from_positions(input)
-        assert_combined_tile_ids_equal(expected, actual)
-
-    def test_redundant_nesting(self):
-        ds = create_test_data
-        input = [[ds(0)], [ds(1)]]
-
-        expected = {(0, 0): ds(0), (1, 0): ds(1)}
-        actual = _infer_concat_order_from_positions(input)
-        assert_combined_tile_ids_equal(expected, actual)
-
-    def test_ignore_empty_list(self):
-        ds = create_test_data(0)
-        input = [ds, []]
-        expected = {(0,): ds}
-        actual = _infer_concat_order_from_positions(input)
-        assert_combined_tile_ids_equal(expected, actual)
-
-    def test_uneven_depth_input(self):
-        # Auto_combine won't work on ragged input
-        # but this is just to increase test coverage
-        ds = create_test_data
-        input = [ds(0), [ds(1), ds(2)]]
-
-        expected = {(0,): ds(0), (1, 0): ds(1), (1, 1): ds(2)}
-        actual = _infer_concat_order_from_positions(input)
-        assert_combined_tile_ids_equal(expected, actual)
-
-    def test_uneven_length_input(self):
-        # Auto_combine won't work on ragged input
-        # but this is just to increase test coverage
-        ds = create_test_data
-        input = [[ds(0)], [ds(1), ds(2)]]
-
-        expected = {(0, 0): ds(0), (1, 0): ds(1), (1, 1): ds(2)}
-        actual = _infer_concat_order_from_positions(input)
-        assert_combined_tile_ids_equal(expected, actual)
-
-    def test_infer_from_datasets(self):
-        ds = create_test_data
-        input = [ds(0), ds(1)]
-
-        expected = {(0,): ds(0), (1,): ds(1)}
-        actual = _infer_concat_order_from_positions(input)
-        assert_combined_tile_ids_equal(expected, actual)
-
-
-class TestTileIDsFromCoords:
-    def test_1d(self):
-        ds0 = Dataset({'x': [0, 1]})
-        ds1 = Dataset({'x': [2, 3]})
-
-        expected = {(0,): ds0, (1,): ds1}
-        actual, concat_dims = _infer_concat_order_from_coords([ds1, ds0])
-        assert_combined_tile_ids_equal(expected, actual)
-        assert concat_dims == ['x']
-
-    def test_2d(self):
-        ds0 = Dataset({'x': [0, 1], 'y': [10, 20, 30]})
-        ds1 = Dataset({'x': [2, 3], 'y': [10, 20, 30]})
-        ds2 = Dataset({'x': [0, 1], 'y': [40, 50, 60]})
-        ds3 = Dataset({'x': [2, 3], 'y': [40, 50, 60]})
-        ds4 = Dataset({'x': [0, 1], 'y': [70, 80, 90]})
-        ds5 = Dataset({'x': [2, 3], 'y': [70, 80, 90]})
-
-        expected = {(0, 0): ds0, (1, 0): ds1,
-                    (0, 1): ds2, (1, 1): ds3,
-                    (0, 2): ds4, (1, 2): ds5}
-        actual, concat_dims = _infer_concat_order_from_coords([ds1, ds0, ds3,
-                                                               ds5, ds2, ds4])
-        assert_combined_tile_ids_equal(expected, actual)
-        assert concat_dims == ['x', 'y']
-
-    def test_no_dimension_coords(self):
-        ds0 = Dataset({'foo': ('x', [0, 1])})
-        ds1 = Dataset({'foo': ('x', [2, 3])})
-        with raises_regex(ValueError, "Could not find any dimension"):
-            _infer_concat_order_from_coords([ds1, ds0])
-
-    def test_coord_not_monotonic(self):
-        ds0 = Dataset({'x': [0, 1]})
-        ds1 = Dataset({'x': [3, 2]})
-        with raises_regex(ValueError, "Coordinate variable x is neither "
-                                      "monotonically increasing nor"):
-            _infer_concat_order_from_coords([ds1, ds0])
-
-    def test_coord_monotonically_decreasing(self):
-        ds0 = Dataset({'x': [3, 2]})
-        ds1 = Dataset({'x': [1, 0]})
-
-        expected = {(0,): ds0, (1,): ds1}
-        actual, concat_dims = _infer_concat_order_from_coords([ds1, ds0])
-        assert_combined_tile_ids_equal(expected, actual)
-        assert concat_dims == ['x']
-
-    def test_no_concatenation_needed(self):
-        ds = Dataset({'foo': ('x', [0, 1])})
-        expected = {(): ds}
-        actual, concat_dims = _infer_concat_order_from_coords([ds])
-        assert_combined_tile_ids_equal(expected, actual)
-        assert concat_dims == []
-
-    def test_2d_plus_bystander_dim(self):
-        ds0 = Dataset({'x': [0, 1], 'y': [10, 20, 30], 't': [0.1, 0.2]})
-        ds1 = Dataset({'x': [2, 3], 'y': [10, 20, 30], 't': [0.1, 0.2]})
-        ds2 = Dataset({'x': [0, 1], 'y': [40, 50, 60], 't': [0.1, 0.2]})
-        ds3 = Dataset({'x': [2, 3], 'y': [40, 50, 60], 't': [0.1, 0.2]})
-
-        expected = {(0, 0): ds0, (1, 0): ds1,
-                    (0, 1): ds2, (1, 1): ds3}
-        actual, concat_dims = _infer_concat_order_from_coords([ds1, ds0,
-                                                               ds3, ds2])
-        assert_combined_tile_ids_equal(expected, actual)
-        assert concat_dims == ['x', 'y']
-
-    def test_string_coords(self):
-        ds0 = Dataset({'person': ['Alice', 'Bob']})
-        ds1 = Dataset({'person': ['Caroline', 'Daniel']})
-
-        expected = {(0,): ds0, (1,): ds1}
-        actual, concat_dims = _infer_concat_order_from_coords([ds1, ds0])
-        assert_combined_tile_ids_equal(expected, actual)
-        assert concat_dims == ['person']
-
-    # Decided against natural sorting of string coords GH #2616
-    def test_lexicographic_sort_string_coords(self):
-        ds0 = Dataset({'simulation': ['run8', 'run9']})
-        ds1 = Dataset({'simulation': ['run10', 'run11']})
-
-        expected = {(0,): ds1, (1,): ds0}
-        actual, concat_dims = _infer_concat_order_from_coords([ds1, ds0])
-        assert_combined_tile_ids_equal(expected, actual)
-        assert concat_dims == ['simulation']
-
-    def test_datetime_coords(self):
-        ds0 = Dataset({'time': [datetime(2000, 3, 6), datetime(2001, 3, 7)]})
-        ds1 = Dataset({'time': [datetime(1999, 1, 1), datetime(1999, 2, 4)]})
-
-        expected = {(0,): ds1, (1,): ds0}
-        actual, concat_dims = _infer_concat_order_from_coords([ds0, ds1])
-        assert_combined_tile_ids_equal(expected, actual)
-        assert concat_dims == ['time']
-
-
-@pytest.fixture(scope='module')
-def create_combined_ids():
-    return _create_combined_ids
-
-
-def _create_combined_ids(shape):
-    tile_ids = _create_tile_ids(shape)
-    nums = range(len(tile_ids))
-    return {tile_id: create_test_data(num)
-            for tile_id, num in zip(tile_ids, nums)}
-
-
-def _create_tile_ids(shape):
-    tile_ids = product(*(range(i) for i in shape))
-    return list(tile_ids)
-
-
-class TestNewTileIDs:
-    @pytest.mark.parametrize("old_id, new_id", [((3, 0, 1), (0, 1)),
-                                                ((0, 0), (0,)),
-                                                ((1,), ()),
-                                                ((0,), ()),
-                                                ((1, 0), (0,))])
-    def test_new_tile_id(self, old_id, new_id):
-        ds = create_test_data
-        assert _new_tile_id((old_id, ds)) == new_id
-
-    def test_get_new_tile_ids(self, create_combined_ids):
-        shape = (1, 2, 3)
-        combined_ids = create_combined_ids(shape)
-
-        expected_tile_ids = sorted(combined_ids.keys())
-        actual_tile_ids = _create_tile_ids(shape)
-        assert expected_tile_ids == actual_tile_ids
-
-
-class TestCombineND:
-    @pytest.mark.parametrize("concat_dim", ['dim1', 'new_dim'])
-    def test_concat_once(self, create_combined_ids, concat_dim):
-        shape = (2,)
-        combined_ids = create_combined_ids(shape)
-        ds = create_test_data
-        result = _combine_all_along_first_dim(combined_ids, dim=concat_dim,
-                                              data_vars='all',
-                                              coords='different',
-                                              compat='no_conflicts')
-
-        expected_ds = concat([ds(0), ds(1)], dim=concat_dim)
-        assert_combined_tile_ids_equal(result, {(): expected_ds})
-
-    def test_concat_only_first_dim(self, create_combined_ids):
-        shape = (2, 3)
-        combined_ids = create_combined_ids(shape)
-        result = _combine_all_along_first_dim(combined_ids, dim='dim1',
-                                              data_vars='all',
-                                              coords='different',
-                                              compat='no_conflicts')
-
-        ds = create_test_data
-        partway1 = concat([ds(0), ds(3)], dim='dim1')
-        partway2 = concat([ds(1), ds(4)], dim='dim1')
-        partway3 = concat([ds(2), ds(5)], dim='dim1')
-        expected_datasets = [partway1, partway2, partway3]
-        expected = {(i,): ds for i, ds in enumerate(expected_datasets)}
-
-        assert_combined_tile_ids_equal(result, expected)
-
-    @pytest.mark.parametrize("concat_dim", ['dim1', 'new_dim'])
-    def test_concat_twice(self, create_combined_ids, concat_dim):
-        shape = (2, 3)
-        combined_ids = create_combined_ids(shape)
-        result = _combine_nd(combined_ids, concat_dims=['dim1', concat_dim])
-
-        ds = create_test_data
-        partway1 = concat([ds(0), ds(3)], dim='dim1')
-        partway2 = concat([ds(1), ds(4)], dim='dim1')
-        partway3 = concat([ds(2), ds(5)], dim='dim1')
-        expected = concat([partway1, partway2, partway3], dim=concat_dim)
-
-        assert_equal(result, expected)
-
-
-class TestCheckShapeTileIDs:
-    def test_check_depths(self):
-        ds = create_test_data(0)
-        combined_tile_ids = {(0,): ds, (0, 1): ds}
-        with raises_regex(ValueError, 'sub-lists do not have '
-                                      'consistent depths'):
-            _check_shape_tile_ids(combined_tile_ids)
-
-    def test_check_lengths(self):
-        ds = create_test_data(0)
-        combined_tile_ids = {(0, 0): ds, (0, 1): ds, (0, 2): ds,
-                             (1, 0): ds, (1, 1): ds}
-        with raises_regex(ValueError, 'sub-lists do not have '
-                                      'consistent lengths'):
-            _check_shape_tile_ids(combined_tile_ids)
-
-
-class TestManualCombine:
-    def test_manual_concat(self):
-        objs = [Dataset({'x': [0]}), Dataset({'x': [1]})]
-        expected = Dataset({'x': [0, 1]})
-        actual = combine_nested(objs, concat_dim='x')
-        assert_identical(expected, actual)
-        actual = combine_nested(objs, concat_dim=['x'])
-        assert_identical(expected, actual)
-
-        actual = combine_nested([actual], concat_dim=None)
-        assert_identical(expected, actual)
-
-        actual = combine_nested([actual], concat_dim='x')
-        assert_identical(expected, actual)
-
-        objs = [Dataset({'x': [0, 1]}), Dataset({'x': [2]})]
-        actual = combine_nested(objs, concat_dim='x')
-        expected = Dataset({'x': [0, 1, 2]})
-        assert_identical(expected, actual)
-
-        # ensure manual_combine handles non-sorted variables
-        objs = [Dataset(OrderedDict([('x', ('a', [0])), ('y', ('a', [0]))])),
-                Dataset(OrderedDict([('y', ('a', [1])), ('x', ('a', [1]))]))]
-        actual = combine_nested(objs, concat_dim='a')
-        expected = Dataset({'x': ('a', [0, 1]), 'y': ('a', [0, 1])})
-        assert_identical(expected, actual)
-
-        objs = [Dataset({'x': [0], 'y': [0]}), Dataset({'x': [0]})]
-        with pytest.raises(KeyError):
-            combine_nested(objs, concat_dim='x')
-
-    def test_empty_input(self):
-        assert_identical(Dataset(), combine_nested([], concat_dim='x'))
-
-    # Fails because of concat's weird treatment of dimension coords, see #2975
-    @pytest.mark.xfail
-    def test_manual_concat_too_many_dims_at_once(self):
-        objs = [Dataset({'x': [0], 'y': [1]}), Dataset({'y': [0], 'x': [1]})]
-        with pytest.raises(ValueError, match="not equal across datasets"):
-            combine_nested(objs, concat_dim='x', coords='minimal')
-
-    def test_manual_concat_along_new_dim(self):
-        objs = [Dataset({'a': ('x', [10]), 'x': [0]}),
-                Dataset({'a': ('x', [20]), 'x': [0]})]
-        expected = Dataset({'a': (('t', 'x'), [[10], [20]]), 'x': [0]})
-        actual = combine_nested(objs, concat_dim='t')
-        assert_identical(expected, actual)
-
-        # Same but with a DataArray as new dim, see GH #1988 and #2647
-        dim = DataArray([100, 150], name='baz', dims='baz')
-        expected = Dataset({'a': (('baz', 'x'), [[10], [20]]),
-                            'x': [0], 'baz': [100, 150]})
-        actual = combine_nested(objs, concat_dim=dim)
-        assert_identical(expected, actual)
-
-    def test_manual_merge(self):
-        data = Dataset({'x': 0})
-        actual = combine_nested([data, data, data], concat_dim=None)
-        assert_identical(data, actual)
-
-        ds1 = Dataset({'a': ('x', [1, 2]), 'x': [0, 1]})
-        ds2 = Dataset({'a': ('x', [2, 3]), 'x': [1, 2]})
-        expected = Dataset({'a': ('x', [1, 2, 3]), 'x': [0, 1, 2]})
-        actual = combine_nested([ds1, ds2], concat_dim=None)
-        assert_identical(expected, actual)
-        actual = combine_nested([ds1, ds2], concat_dim=[None])
-        assert_identical(expected, actual)
-
-        tmp1 = Dataset({'x': 0})
-        tmp2 = Dataset({'x': np.nan})
-        actual = combine_nested([tmp1, tmp2], concat_dim=None)
-        assert_identical(tmp1, actual)
-        actual = combine_nested([tmp1, tmp2], concat_dim=[None])
-        assert_identical(tmp1, actual)
-
-        # Single object, with a concat_dim explicitly provided
-        # Test the issue reported in GH #1988
-        objs = [Dataset({'x': 0, 'y': 1})]
-        dim = DataArray([100], name='baz', dims='baz')
-        actual = combine_nested(objs, concat_dim=[dim])
-        expected = Dataset({'x': ('baz', [0]), 'y': ('baz', [1])},
-                           {'baz': [100]})
-        assert_identical(expected, actual)
-
-        # Just making sure that auto_combine is doing what is
-        # expected for non-scalar values, too.
-        objs = [Dataset({'x': ('z', [0, 1]), 'y': ('z', [1, 2])})]
-        dim = DataArray([100], name='baz', dims='baz')
-        actual = combine_nested(objs, concat_dim=[dim])
-        expected = Dataset({'x': (('baz', 'z'), [[0, 1]]),
-                            'y': (('baz', 'z'), [[1, 2]])},
-                           {'baz': [100]})
-        assert_identical(expected, actual)
-
-    def test_concat_multiple_dims(self):
-        objs = [[Dataset({'a': (('x', 'y'), [[0]])}),
-                 Dataset({'a': (('x', 'y'), [[1]])})],
-                [Dataset({'a': (('x', 'y'), [[2]])}),
-                 Dataset({'a': (('x', 'y'), [[3]])})]]
-        actual = combine_nested(objs, concat_dim=['x', 'y'])
-        expected = Dataset({'a': (('x', 'y'), [[0, 1], [2, 3]])})
-        assert_identical(expected, actual)
-
-    def test_concat_name_symmetry(self):
-        """Inspired by the discussion on GH issue #2777"""
-
-        da1 = DataArray(name='a', data=[[0]], dims=['x', 'y'])
-        da2 = DataArray(name='b', data=[[1]], dims=['x', 'y'])
-        da3 = DataArray(name='a', data=[[2]], dims=['x', 'y'])
-        da4 = DataArray(name='b', data=[[3]], dims=['x', 'y'])
-
-        x_first = combine_nested([[da1, da2], [da3, da4]],
-                                 concat_dim=['x', 'y'])
-        y_first = combine_nested([[da1, da3], [da2, da4]],
-                                 concat_dim=['y', 'x'])
-
-        assert_identical(x_first, y_first)
-
-    def test_concat_one_dim_merge_another(self):
-        data = create_test_data()
-        data1 = data.copy(deep=True)
-        data2 = data.copy(deep=True)
-
-        objs = [[data1.var1.isel(dim2=slice(4)),
-                 data2.var1.isel(dim2=slice(4, 9))],
-                [data1.var2.isel(dim2=slice(4)),
-                 data2.var2.isel(dim2=slice(4, 9))]]
-
-        expected = data[['var1', 'var2']]
-        actual = combine_nested(objs, concat_dim=[None, 'dim2'])
-        assert expected.identical(actual)
-
-    def test_auto_combine_2d(self):
-        ds = create_test_data
-
-        partway1 = concat([ds(0), ds(3)], dim='dim1')
-        partway2 = concat([ds(1), ds(4)], dim='dim1')
-        partway3 = concat([ds(2), ds(5)], dim='dim1')
-        expected = concat([partway1, partway2, partway3], dim='dim2')
-
-        datasets = [[ds(0), ds(1), ds(2)], [ds(3), ds(4), ds(5)]]
-        result = combine_nested(datasets, concat_dim=['dim1', 'dim2'])
-        assert_equal(result, expected)
-
-    def test_manual_combine_missing_data_new_dim(self):
-        # Your data includes "time" and "station" dimensions, and each year's
-        # data has a different set of stations.
-        datasets = [Dataset({'a': ('x', [2, 3]), 'x': [1, 2]}),
-                    Dataset({'a': ('x', [1, 2]), 'x': [0, 1]})]
-        expected = Dataset({'a': (('t', 'x'),
-                                  [[np.nan, 2, 3], [1, 2, np.nan]])},
-                           {'x': [0, 1, 2]})
-        actual = combine_nested(datasets, concat_dim='t')
-        assert_identical(expected, actual)
-
-    def test_invalid_hypercube_input(self):
-        ds = create_test_data
-
-        datasets = [[ds(0), ds(1), ds(2)], [ds(3), ds(4)]]
-        with raises_regex(ValueError, 'sub-lists do not have '
-                                      'consistent lengths'):
-            combine_nested(datasets, concat_dim=['dim1', 'dim2'])
-
-        datasets = [[ds(0), ds(1)], [[ds(3), ds(4)]]]
-        with raises_regex(ValueError, 'sub-lists do not have '
-                                      'consistent depths'):
-            combine_nested(datasets, concat_dim=['dim1', 'dim2'])
-
-        datasets = [[ds(0), ds(1)], [ds(3), ds(4)]]
-        with raises_regex(ValueError, 'concat_dims has length'):
-            combine_nested(datasets, concat_dim=['dim1'])
-
-    def test_merge_one_dim_concat_another(self):
-        objs = [[Dataset({'foo': ('x', [0, 1])}),
-                 Dataset({'bar': ('x', [10, 20])})],
-                [Dataset({'foo': ('x', [2, 3])}),
-                 Dataset({'bar': ('x', [30, 40])})]]
-        expected = Dataset({'foo': ('x', [0, 1, 2, 3]),
-                            'bar': ('x', [10, 20, 30, 40])})
-
-        actual = combine_nested(objs, concat_dim=['x', None], compat='equals')
-        assert_identical(expected, actual)
-
-        # Proving it works symmetrically
-        objs = [[Dataset({'foo': ('x', [0, 1])}),
-                 Dataset({'foo': ('x', [2, 3])})],
-                [Dataset({'bar': ('x', [10, 20])}),
-                 Dataset({'bar': ('x', [30, 40])})]]
-        actual = combine_nested(objs, concat_dim=[None, 'x'], compat='equals')
-        assert_identical(expected, actual)
-
-    def test_combine_concat_over_redundant_nesting(self):
-        objs = [[Dataset({'x': [0]}), Dataset({'x': [1]})]]
-        actual = combine_nested(objs, concat_dim=[None, 'x'])
-        expected = Dataset({'x': [0, 1]})
-        assert_identical(expected, actual)
-
-        objs = [[Dataset({'x': [0]})], [Dataset({'x': [1]})]]
-        actual = combine_nested(objs, concat_dim=['x', None])
-        expected = Dataset({'x': [0, 1]})
-        assert_identical(expected, actual)
-
-        objs = [[Dataset({'x': [0]})]]
-        actual = combine_nested(objs, concat_dim=[None, None])
-        expected = Dataset({'x': [0]})
-        assert_identical(expected, actual)
-
-    def test_manual_combine_but_need_auto_combine(self):
-        objs = [Dataset({'x': [0, 1]}), Dataset({'x': [2], 'wall': [0]})]
-        with raises_regex(ValueError, 'cannot be combined'):
-            combine_nested(objs, concat_dim='x')
-
-    @pytest.mark.parametrize('fill_value', [dtypes.NA, 2, 2.0])
-    def test_combine_nested_fill_value(self, fill_value):
-        datasets = [Dataset({'a': ('x', [2, 3]), 'x': [1, 2]}),
-                    Dataset({'a': ('x', [1, 2]), 'x': [0, 1]})]
-        if fill_value == dtypes.NA:
-            # if we supply the default, we expect the missing value for a
-            # float array
-            fill_value = np.nan
-        expected = Dataset({'a': (('t', 'x'),
-                                  [[fill_value, 2, 3], [1, 2, fill_value]])},
-                           {'x': [0, 1, 2]})
-        actual = combine_nested(datasets, concat_dim='t',
-                                fill_value=fill_value)
-        assert_identical(expected, actual)
-
-
-class TestCombineAuto:
-    def test_combine_by_coords(self):
-        objs = [Dataset({'x': [0]}), Dataset({'x': [1]})]
-        actual = combine_by_coords(objs)
-        expected = Dataset({'x': [0, 1]})
-        assert_identical(expected, actual)
-
-        actual = combine_by_coords([actual])
-        assert_identical(expected, actual)
-
-        objs = [Dataset({'x': [0, 1]}), Dataset({'x': [2]})]
-        actual = combine_by_coords(objs)
-        expected = Dataset({'x': [0, 1, 2]})
-        assert_identical(expected, actual)
-
-        # ensure auto_combine handles non-sorted variables
-        objs = [Dataset({'x': ('a', [0]), 'y': ('a', [0]), 'a': [0]}),
-                Dataset({'x': ('a', [1]), 'y': ('a', [1]), 'a': [1]})]
-        actual = combine_by_coords(objs)
-        expected = Dataset({'x': ('a', [0, 1]), 'y': ('a', [0, 1]),
-                            'a': [0, 1]})
-        assert_identical(expected, actual)
-
-        objs = [Dataset({'x': [0], 'y': [0]}), Dataset({'y': [1], 'x': [1]})]
-        actual = combine_by_coords(objs)
-        expected = Dataset({'x': [0, 1], 'y': [0, 1]})
-        assert_equal(actual, expected)
-
-        objs = [Dataset({'x': 0}), Dataset({'x': 1})]
-        with raises_regex(ValueError, 'Could not find any dimension '
-                                      'coordinates'):
-            combine_by_coords(objs)
-
-        objs = [Dataset({'x': [0], 'y': [0]}), Dataset({'x': [0]})]
-        with raises_regex(ValueError, 'Every dimension needs a coordinate'):
-            combine_by_coords(objs)
-
-        def test_empty_input(self):
-            assert_identical(Dataset(), combine_by_coords([]))
-
-    def test_infer_order_from_coords(self):
-        data = create_test_data()
-        objs = [data.isel(dim2=slice(4, 9)), data.isel(dim2=slice(4))]
-        actual = combine_by_coords(objs)
-        expected = data
-        assert expected.broadcast_equals(actual)
-
-    def test_combine_by_coords_previously_failed(self):
-        # In the above scenario, one file is missing, containing the data for
-        # one year's data for one variable.
-        datasets = [Dataset({'a': ('x', [0]), 'x': [0]}),
-                    Dataset({'b': ('x', [0]), 'x': [0]}),
-                    Dataset({'a': ('x', [1]), 'x': [1]})]
-        expected = Dataset({'a': ('x', [0, 1]), 'b': ('x', [0, np.nan])},
-                           {'x': [0, 1]})
-        actual = combine_by_coords(datasets)
-        assert_identical(expected, actual)
-
-    def test_combine_by_coords_still_fails(self):
-        # concat can't handle new variables (yet):
-        # https://github.com/pydata/xarray/issues/508
-        datasets = [Dataset({'x': 0}, {'y': 0}),
-                    Dataset({'x': 1}, {'y': 1, 'z': 1})]
-        with pytest.raises(ValueError):
-            combine_by_coords(datasets, 'y')
-
-    def test_combine_by_coords_no_concat(self):
-        objs = [Dataset({'x': 0}), Dataset({'y': 1})]
-        actual = combine_by_coords(objs)
-        expected = Dataset({'x': 0, 'y': 1})
-        assert_identical(expected, actual)
-
-        objs = [Dataset({'x': 0, 'y': 1}), Dataset({'y': np.nan, 'z': 2})]
-        actual = combine_by_coords(objs)
-        expected = Dataset({'x': 0, 'y': 1, 'z': 2})
-        assert_identical(expected, actual)
-
-    def test_check_for_impossible_ordering(self):
-        ds0 = Dataset({'x': [0, 1, 5]})
-        ds1 = Dataset({'x': [2, 3]})
-        with raises_regex(ValueError, "does not have monotonic global indexes"
-                                      " along dimension x"):
-            combine_by_coords([ds1, ds0])
-
-
-@pytest.mark.filterwarnings("ignore:In xarray version 0.13 `auto_combine` "
-                            "will be deprecated")
-@pytest.mark.filterwarnings("ignore:Also `open_mfdataset` will no longer")
-@pytest.mark.filterwarnings("ignore:The datasets supplied")
-class TestAutoCombineOldAPI:
-    """
-    Set of tests which check that old 1-dimensional auto_combine behaviour is
-    still satisfied. #2616
-    """
-    def test_auto_combine(self):
-        objs = [Dataset({'x': [0]}), Dataset({'x': [1]})]
-        actual = auto_combine(objs)
-        expected = Dataset({'x': [0, 1]})
-        assert_identical(expected, actual)
-
-        actual = auto_combine([actual])
-        assert_identical(expected, actual)
-
-        objs = [Dataset({'x': [0, 1]}), Dataset({'x': [2]})]
-        actual = auto_combine(objs)
-        expected = Dataset({'x': [0, 1, 2]})
-        assert_identical(expected, actual)
-
-        # ensure auto_combine handles non-sorted variables
-        objs = [Dataset(OrderedDict([('x', ('a', [0])), ('y', ('a', [0]))])),
-                Dataset(OrderedDict([('y', ('a', [1])), ('x', ('a', [1]))]))]
-        actual = auto_combine(objs)
-        expected = Dataset({'x': ('a', [0, 1]), 'y': ('a', [0, 1])})
-        assert_identical(expected, actual)
-
-        objs = [Dataset({'x': [0], 'y': [0]}), Dataset({'y': [1], 'x': [1]})]
-        with raises_regex(ValueError, 'too many .* dimensions'):
-            auto_combine(objs)
-
-        objs = [Dataset({'x': 0}), Dataset({'x': 1})]
-        with raises_regex(ValueError, 'cannot infer dimension'):
-            auto_combine(objs)
-
-        objs = [Dataset({'x': [0], 'y': [0]}), Dataset({'x': [0]})]
-        with pytest.raises(KeyError):
-            auto_combine(objs)
-
-    def test_auto_combine_previously_failed(self):
-        # In the above scenario, one file is missing, containing the data for
-        # one year's data for one variable.
-        datasets = [Dataset({'a': ('x', [0]), 'x': [0]}),
-                    Dataset({'b': ('x', [0]), 'x': [0]}),
-                    Dataset({'a': ('x', [1]), 'x': [1]})]
-        expected = Dataset({'a': ('x', [0, 1]), 'b': ('x', [0, np.nan])},
-                           {'x': [0, 1]})
-        actual = auto_combine(datasets)
-        assert_identical(expected, actual)
-
-        # Your data includes "time" and "station" dimensions, and each year's
-        # data has a different set of stations.
-        datasets = [Dataset({'a': ('x', [2, 3]), 'x': [1, 2]}),
-                    Dataset({'a': ('x', [1, 2]), 'x': [0, 1]})]
-        expected = Dataset({'a': (('t', 'x'),
-                                  [[np.nan, 2, 3], [1, 2, np.nan]])},
-                           {'x': [0, 1, 2]})
-        actual = auto_combine(datasets, concat_dim='t')
-        assert_identical(expected, actual)
-
-    def test_auto_combine_still_fails(self):
-        # concat can't handle new variables (yet):
-        # https://github.com/pydata/xarray/issues/508
-        datasets = [Dataset({'x': 0}, {'y': 0}),
-                    Dataset({'x': 1}, {'y': 1, 'z': 1})]
-        with pytest.raises(ValueError):
-            auto_combine(datasets, 'y')
-
-    def test_auto_combine_no_concat(self):
-        objs = [Dataset({'x': 0}), Dataset({'y': 1})]
-        actual = auto_combine(objs)
-        expected = Dataset({'x': 0, 'y': 1})
-        assert_identical(expected, actual)
-
-        objs = [Dataset({'x': 0, 'y': 1}), Dataset({'y': np.nan, 'z': 2})]
-        actual = auto_combine(objs)
-        expected = Dataset({'x': 0, 'y': 1, 'z': 2})
-        assert_identical(expected, actual)
-
-        data = Dataset({'x': 0})
-        actual = auto_combine([data, data, data], concat_dim=None)
-        assert_identical(data, actual)
-
-        # Single object, with a concat_dim explicitly provided
-        # Test the issue reported in GH #1988
-        objs = [Dataset({'x': 0, 'y': 1})]
-        dim = DataArray([100], name='baz', dims='baz')
-        actual = auto_combine(objs, concat_dim=dim)
-        expected = Dataset({'x': ('baz', [0]), 'y': ('baz', [1])},
-                           {'baz': [100]})
-        assert_identical(expected, actual)
-
-        # Just making sure that auto_combine is doing what is
-        # expected for non-scalar values, too.
-        objs = [Dataset({'x': ('z', [0, 1]), 'y': ('z', [1, 2])})]
-        dim = DataArray([100], name='baz', dims='baz')
-        actual = auto_combine(objs, concat_dim=dim)
-        expected = Dataset({'x': (('baz', 'z'), [[0, 1]]),
-                            'y': (('baz', 'z'), [[1, 2]])},
-                           {'baz': [100]})
-        assert_identical(expected, actual)
-
-    def test_auto_combine_order_by_appearance_not_coords(self):
-        objs = [Dataset({'foo': ('x', [0])}, coords={'x': ('x', [1])}),
-                Dataset({'foo': ('x', [1])}, coords={'x': ('x', [0])})]
-        actual = auto_combine(objs)
-        expected = Dataset({'foo': ('x', [0, 1])},
-                           coords={'x': ('x', [1, 0])})
-        assert_identical(expected, actual)
-
-    @pytest.mark.parametrize('fill_value', [dtypes.NA, 2, 2.0])
-    def test_auto_combine_fill_value(self, fill_value):
-        datasets = [Dataset({'a': ('x', [2, 3]), 'x': [1, 2]}),
-                    Dataset({'a': ('x', [1, 2]), 'x': [0, 1]})]
-        if fill_value == dtypes.NA:
-            # if we supply the default, we expect the missing value for a
-            # float array
-            fill_value = np.nan
-        expected = Dataset({'a': (('t', 'x'),
-                                  [[fill_value, 2, 3], [1, 2, fill_value]])},
-                           {'x': [0, 1, 2]})
-        actual = auto_combine(datasets, concat_dim='t', fill_value=fill_value)
-        assert_identical(expected, actual)
-
-
-class TestAutoCombineDeprecation:
-    """
-    Set of tests to check that FutureWarnings are correctly raised until the
-    deprecation cycle is complete. #2616
-    """
-    def test_auto_combine_with_concat_dim(self):
-        objs = [Dataset({'x': [0]}), Dataset({'x': [1]})]
-        with pytest.warns(FutureWarning, match="`concat_dim`"):
-            auto_combine(objs, concat_dim='x')
-
-    def test_auto_combine_with_merge_and_concat(self):
-        objs = [Dataset({'x': [0]}),
-                Dataset({'x': [1]}),
-                Dataset({'z': ((), 99)})]
-        with pytest.warns(FutureWarning, match="require both concatenation"):
-            auto_combine(objs)
-
-    def test_auto_combine_with_coords(self):
-        objs = [Dataset({'foo': ('x', [0])}, coords={'x': ('x', [0])}),
-                Dataset({'foo': ('x', [1])}, coords={'x': ('x', [1])})]
-        with pytest.warns(FutureWarning, match="supplied have global"):
-            auto_combine(objs)
-
-    def test_auto_combine_without_coords(self):
-        objs = [Dataset({'foo': ('x', [0])}),
-                Dataset({'foo': ('x', [1])})]
-        with pytest.warns(FutureWarning, match="supplied do not have global"):
-            auto_combine(objs)
Obtaining file:///testbed
  Preparing metadata (setup.py): started
  Preparing metadata (setup.py): finished with status 'done'
Requirement already satisfied: numpy>=1.12 in /opt/miniconda3/envs/testbed/lib/python3.10/site-packages (from xarray==0.12.3+22.gb90907ca) (1.23.0)
Requirement already satisfied: pandas>=0.19.2 in /opt/miniconda3/envs/testbed/lib/python3.10/site-packages (from xarray==0.12.3+22.gb90907ca) (1.5.3)
Requirement already satisfied: python-dateutil>=2.8.1 in /opt/miniconda3/envs/testbed/lib/python3.10/site-packages (from pandas>=0.19.2->xarray==0.12.3+22.gb90907ca) (2.8.2)
Requirement already satisfied: pytz>=2020.1 in /opt/miniconda3/envs/testbed/lib/python3.10/site-packages (from pandas>=0.19.2->xarray==0.12.3+22.gb90907ca) (2023.3)
Requirement already satisfied: six>=1.5 in /opt/miniconda3/envs/testbed/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas>=0.19.2->xarray==0.12.3+22.gb90907ca) (1.16.0)
Installing collected packages: xarray
  Attempting uninstall: xarray
    Found existing installation: xarray 0.12.3+21.g118f4d99
    Uninstalling xarray-0.12.3+21.g118f4d99:
      Successfully uninstalled xarray-0.12.3+21.g118f4d99
  DEPRECATION: Legacy editable install of xarray==0.12.3+22.gb90907ca from file:///testbed (setup.py develop) is deprecated. pip 25.0 will enforce this behaviour change. A possible replacement is to add a pyproject.toml or enable --use-pep517, and use setuptools >= 64. If the resulting installation is not behaving as expected, try using --config-settings editable_mode=compat. Please consult the setuptools documentation for more information. Discussion can be found at https://github.com/pypa/pip/issues/11457
  Running setup.py develop for xarray
Successfully installed xarray
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.
============================= test session starts ==============================
platform linux -- Python 3.10.14, pytest-7.4.0, pluggy-1.5.0
rootdir: /testbed
configfile: setup.cfg
plugins: env-1.1.3, cov-5.0.0, hypothesis-6.108.5, xdist-3.6.1
collected 11 items

xarray/tests/test_combine.py ...........                                 [100%]

=============================== warnings summary ===============================
xarray/core/dask_array_ops.py:11
xarray/core/dask_array_ops.py:11
  /testbed/xarray/core/dask_array_ops.py:11: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    if LooseVersion(dask.__version__) <= LooseVersion('0.18.2'):

xarray/core/npcompat.py:135
xarray/core/npcompat.py:135
  /testbed/xarray/core/npcompat.py:135: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    if LooseVersion(np.__version__) >= LooseVersion('1.13'):

xarray/core/dask_array_compat.py:43
xarray/core/dask_array_compat.py:43
  /testbed/xarray/core/dask_array_compat.py:43: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    if LooseVersion(dask_version) > LooseVersion('0.19.2'):

xarray/plot/utils.py:17
xarray/plot/utils.py:17
  /testbed/xarray/plot/utils.py:17: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    if LooseVersion(nc_time_axis.__version__) < LooseVersion('1.2.0'):

xarray/plot/plot.py:243
  /testbed/xarray/plot/plot.py:243: SyntaxWarning: "is" with a literal. Did you mean "=="?
    if args is ():

xarray/core/pdcompat.py:46
  /testbed/xarray/core/pdcompat.py:46: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    if LooseVersion(pd.__version__) < '0.25.0':

../opt/miniconda3/envs/testbed/lib/python3.10/site-packages/setuptools/_distutils/version.py:345
../opt/miniconda3/envs/testbed/lib/python3.10/site-packages/setuptools/_distutils/version.py:345
  /opt/miniconda3/envs/testbed/lib/python3.10/site-packages/setuptools/_distutils/version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    other = LooseVersion(other)

xarray/tests/__init__.py:57: 15 warnings
  /testbed/xarray/tests/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    return version.LooseVersion(vstring)

../opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pydap/lib.py:5
  /opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pydap/lib.py:5: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html
    from pkg_resources import get_distribution

../opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pkg_resources/__init__.py:2871
  /opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`.
  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
    declare_namespace(pkg)

../opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pkg_resources/__init__.py:2871
  /opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap.responses')`.
  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
    declare_namespace(pkg)

../opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pkg_resources/__init__.py:2350
../opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pkg_resources/__init__.py:2350
../opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pkg_resources/__init__.py:2350
  /opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pkg_resources/__init__.py:2350: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`.
  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
    declare_namespace(parent)

../opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pkg_resources/__init__.py:2871
  /opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap.handlers')`.
  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
    declare_namespace(pkg)

../opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pkg_resources/__init__.py:2871
  /opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap.tests')`.
  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
    declare_namespace(pkg)

xarray/tests/test_combine.py::test_auto_combine
  /testbed/xarray/tests/test_combine.py:82: FutureWarning: In xarray version 0.13 `auto_combine` will be deprecated. See
  http://xarray.pydata.org/en/stable/combining.html#combining-multi
    combined = auto_combine(datasets, concat_dim='x')

xarray/tests/test_combine.py::test_auto_combine
  /testbed/xarray/tests/test_combine.py:82: FutureWarning: Also `open_mfdataset` will no longer accept a `concat_dim` argument.
  To get equivalent behaviour from now on please use the new
  `combine_nested` function instead (or the `combine='nested'` option to
  `open_mfdataset`).The datasets supplied have global dimension coordinates. You may want
  to use the new `combine_by_coords` function (or the
  `combine='by_coords'` option to `open_mfdataset`) to order the datasets
  before concatenation. Alternatively, to continue concatenating based
  on the order the datasets are supplied in future, please use the new
  `combine_nested` function (or the `combine='nested'` option to
  open_mfdataset).
    combined = auto_combine(datasets, concat_dim='x')

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
==================================== PASSES ====================================
=========================== short test summary info ============================
PASSED xarray/tests/test_combine.py::test_infer_concat_order_from_positions
PASSED xarray/tests/test_combine.py::test_infer_tile_ids_from_nested_list
PASSED xarray/tests/test_combine.py::test_infer_concat_order_from_coords
PASSED xarray/tests/test_combine.py::test_check_shape_tile_ids
PASSED xarray/tests/test_combine.py::test_combine_nd
PASSED xarray/tests/test_combine.py::test_combine_all_along_first_dim
PASSED xarray/tests/test_combine.py::test_combine_1d
PASSED xarray/tests/test_combine.py::test_new_tile_id
PASSED xarray/tests/test_combine.py::test_nested_combine
PASSED xarray/tests/test_combine.py::test_combine_by_coords
PASSED xarray/tests/test_combine.py::test_auto_combine
======================= 11 passed, 37 warnings in 4.82s ========================

