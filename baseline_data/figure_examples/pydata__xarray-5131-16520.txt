Instance ID: pydata__xarray-5131-16520

Baseline 1 (Pynguin):
Predicted Test Suite: # Test cases automatically generated by Pynguin (https://www.pynguin.eu).
# Please check them before you use them.
import pytest
import scipy.io.matlab.mio5_params as module_0
import xarray.core.groupby as module_1
import numba.np.ufunc.parallel as module_2
import llvmlite.binding.targets as module_3


def test_case_0():
    var_0 = module_0.__dir__()
    var_1 = module_1.check_reduce_dims(var_0, var_0)


@pytest.mark.xfail(strict=True)
def test_case_1():
    none_type_0 = None
    module_1.DataArrayGroupBy(
        none_type_0, none_type_0, grouper=none_type_0, bins=none_type_0
    )


@pytest.mark.xfail(strict=True)
def test_case_2():
    none_type_0 = None
    module_1.unique_value_groups(none_type_0)


@pytest.mark.xfail(strict=True)
def test_case_3():
    none_type_0 = None
    module_1._DummyGroup(none_type_0, none_type_0, none_type_0)


@pytest.mark.xfail(strict=True)
def test_case_4():
    var_0 = module_2.gen_snt_check()
    module_1.check_reduce_dims(var_0, var_0)


def test_case_5():
    none_type_0 = None
    list_0 = []
    with pytest.raises(ValueError):
        module_1.check_reduce_dims(none_type_0, list_0)


@pytest.mark.xfail(strict=True)
def test_case_6():
    var_0 = module_0.__dir__()
    module_1.DataArrayGroupBy(var_0, var_0, var_0, cut_kwargs=var_0)


def test_case_7():
    var_0 = module_3.get_host_cpu_name()
    var_1 = module_1.unique_value_groups(var_0)
    var_2 = module_1.check_reduce_dims(var_0, var_0)
    with pytest.raises(TypeError):
        module_1.GroupBy(var_2, var_2, grouper=var_0, restore_coord_dims=var_0)


def test_case_8():
    var_0 = module_0.__dir__()
    with pytest.raises(TypeError):
        module_1.GroupBy(var_0, var_0, restore_coord_dims=var_0)


def test_case_9():
    var_0 = module_0.__dir__()
    with pytest.raises(TypeError):
        module_1.GroupBy(var_0, var_0, var_0, var_0, var_0)

Coverage: 29.834254143646408
Mutation Score: 5.980000000000004

Baseline 2 (CodaMosa):
Predicted Test Suite: import xarray.core.groupby as module_0

def test_case_1():
    pass


def test_case_2():
    str_0 = "0FJ'-\x0bsz"
    int_0 = 691
    dict_0 = {str_0: str_0, str_0: str_0, str_0: int_0, str_0: int_0}
    var_0 = module_0.check_reduce_dims(dict_0, str_0)
    assert var_0 is None
    assert len(module_0.integer_types) == 2


def test_case_3():
    bytes_0 = None
    tuple_0 = ()
    str_0 = 'iscomplex'
    dict_0 = {str_0: bytes_0, str_0: tuple_0, str_0: bytes_0, str_0: bytes_0}
    var_0 = module_0.check_reduce_dims(tuple_0, dict_0)
    assert var_0 is None
    assert len(module_0.integer_types) == 2
    int_0 = None
    list_0 = [var_0, int_0]
    var_1 = module_0.unique_value_groups(list_0)
    assert len(var_1) == 2# Automatically generated by Pynguin.


def test_case_4():
    try:
        bool_0 = True
        int_0 = 420
        var_0 = module_0.check_reduce_dims(bool_0, int_0)
    except BaseException:
        pass


def test_case_5():
    try:
        str_0 = "0FJ'-\x0bsz"
        int_0 = 691
        dict_0 = {str_0: str_0, str_0: str_0, str_0: int_0, str_0: int_0}
        str_1 = 'data_vars'
        var_0 = module_0.check_reduce_dims(dict_0, str_1)
    except BaseException:
        pass


def test_case_6():
    try:
        float_0 = -26.1634
        bool_0 = False
        str_0 = '5({!u}'
        dummy_group_0 = module_0._DummyGroup(float_0, bool_0, str_0)
    except BaseException:
        pass


def test_case_7():
    try:
        bytes_0 = None
        list_0 = [bytes_0, bytes_0, bytes_0, bytes_0]
        data_array_group_by_0 = module_0.DataArrayGroupBy(bytes_0, list_0)
    except BaseException:
        pass


def test_case_8():
    try:
        bool_0 = True
        str_0 = 'iscomplex'
        data_array_group_by_0 = module_0.DataArrayGroupBy(bool_0, str_0, str_0, str_0)
    except BaseException:
        pass


def test_case_9():
    try:
        str_0 = 'X4obl6_ad<s'
        bool_0 = True
        dataset_group_by_0 = module_0.DatasetGroupBy(str_0, bool_0, str_0)
    except BaseException:
        pass


def test_case_10():
    try:
        bytes_0 = None
        bool_0 = True
        tuple_0 = ()
        str_0 = 'iscomplex'
        dict_0 = {str_0: bytes_0, str_0: tuple_0, str_0: bytes_0, str_0: bytes_0}
        var_0 = module_0.check_reduce_dims(tuple_0, dict_0)
        assert var_0 is None
        assert len(module_0.integer_types) == 2
        str_1 = 'oY-I1L&>_@M'
        data_array_group_by_0 = module_0.DataArrayGroupBy(bool_0, str_1, str_1, bool_0, str_1)
    except BaseException:
        pass


def test_case_11():
    try:
        bool_0 = True
        set_0 = {bool_0, bool_0}
        var_0 = module_0.unique_value_groups(set_0)
    except BaseException:
        pass


def test_case_12():
    try:
        bytes_0 = None
        tuple_0 = ()
        str_0 = 'iscomplex'
        dict_0 = {str_0: tuple_0, str_0: bytes_0, str_0: tuple_0, str_0: bytes_0}
        var_0 = module_0.check_reduce_dims(tuple_0, dict_0)
        assert var_0 is None
        assert len(module_0.integer_types) == 2
        str_1 = 'oUY-I1L&>_@M'
        list_0 = [str_0, bytes_0, str_0]
        var_1 = module_0.unique_value_groups(list_0)
        assert len(var_1) == 2
        bool_0 = True
        str_2 = ',qib#uxSb\t+'
        dataset_group_by_0 = module_0.DatasetGroupBy(str_1, bool_0, str_2)
    except BaseException:
        pass


def test_case_13():
    try:
        bytes_0 = None
        str_0 = 'U/`vL/:Z(F:sqc\r.< '
        float_0 = -1212.83725
        list_0 = [str_0]
        bool_0 = False
        tuple_0 = ()
        dict_0 = {}
        group_by_0 = module_0.GroupBy(float_0, list_0, bytes_0, bytes_0, bool_0, tuple_0, dict_0)
    except BaseException:
        pass


def test_case_14():
    try:
        bytes_0 = b'\x15\xb5\x10'
        tuple_0 = ()
        list_0 = [bytes_0, tuple_0, bytes_0, tuple_0]
        bool_0 = True
        str_0 = ''
        dataset_group_by_0 = module_0.DatasetGroupBy(list_0, bool_0, str_0, tuple_0)
    except BaseException:
        pass

Coverage: 30.386740331491712
Mutation Score: 2.989999999999995

Baseline 3 (GPT-4o):
Predicted Test Suite: 
import pytest
import numpy as np
import pandas as pd
from xarray import DataArray, Dataset
from xarray.core.groupby import (
    check_reduce_dims,
    unique_value_groups,
    _dummy_copy,
    _is_one_or_none,
    _consolidate_slices,
    _inverse_permutation_indices,
    _DummyGroup,
    GroupBy,
    DataArrayGroupBy,
    DatasetGroupBy,
)

def test_check_reduce_dims():
    dimensions = ['x', 'y', 'z']
    check_reduce_dims('x', dimensions)
    check_reduce_dims(['x', 'y'], dimensions)
    with pytest.raises(ValueError):
        check_reduce_dims('a', dimensions)

def test_unique_value_groups():
    ar = np.array([1, 2, 2, 3, 1, 4, 4, 4])
    values, groups = unique_value_groups(ar)
    assert np.array_equal(values, np.array([1, 2, 3, 4]))
    assert groups == [[0, 4], [1, 2], [3], [5, 6, 7]]

def test_dummy_copy():
    da = DataArray([1, 2, 3], dims='x')
    dummy = _dummy_copy(da)
    assert isinstance(dummy, DataArray)
    assert dummy.shape == (0,)

def test_is_one_or_none():
    assert _is_one_or_none(1)
    assert _is_one_or_none(None)
    assert not _is_one_or_none(2)

def test_consolidate_slices():
    slices = [slice(0, 2), slice(2, 4), slice(4, 6)]
    consolidated = _consolidate_slices(slices)
    assert consolidated == [slice(0, 6)]

def test_inverse_permutation_indices():
    positions = [np.array([0, 1]), np.array([2, 3])]
    indices = _inverse_permutation_indices(positions)
    assert np.array_equal(indices, np.array([0, 1, 2, 3]))

def test_dummy_group():
    obj = Dataset(coords={'x': ('x', [0, 1, 2])})
    group = _DummyGroup(obj, 'x', obj.coords)
    assert group.name == 'x'
    assert group.size == 3
    assert group.dims == ('x',)
    assert group.ndim == 1
    assert list(group.values) == [0, 1, 2]

def test_groupby_init():
    da = DataArray([1, 2, 3], dims='x', coords={'x': [0, 1, 0]})
    groupby = GroupBy(da, da['x'])
    assert groupby._group.name == 'x'
    assert len(groupby) == 2

def test_groupby_iter():
    da = DataArray([1, 2, 3], dims='x', coords={'x': [0, 1, 0]})
    groupby = GroupBy(da, da['x'])
    groups = list(groupby)
    assert len(groups) == 2
    assert np.array_equal(groups[0][1].values, np.array([1, 3]))
    assert np.array_equal(groups[1][1].values, np.array([2]))

def test_dataarray_groupby_map():
    da = DataArray([1, 2, 3], dims='x', coords={'x': [0, 1, 0]})
    groupby = DataArrayGroupBy(da, da['x'])
    result = groupby.map(lambda x: x.mean())
    assert np.array_equal(result.values, np.array([2.0, 2.0]))

def test_dataset_groupby_map():
    ds = Dataset({'a': ('x', [1, 2, 3])}, coords={'x': [0, 1, 0]})
    groupby = DatasetGroupBy(ds, ds['x'])
    result = groupby.map(lambda x: x.mean())
    assert np.array_equal(result['a'].values, np.array([2.0, 2.0]))

Coverage: 57.4585635359116
Mutation Score: 23.64

Approach:
Predicted Test Suite: import pytest
import numpy as np
import pandas as pd
from xarray import DataArray, Dataset
from xarray.core.groupby import (
    check_reduce_dims,
    unique_value_groups,
    _dummy_copy,
    _is_one_or_none,
    _consolidate_slices,
    _inverse_permutation_indices,
    _DummyGroup,
    GroupBy,
    DataArrayGroupBy,
    DatasetGroupBy,
)

def test_check_reduce_dims():
    dimensions = ['x', 'y', 'z']
    check_reduce_dims('x', dimensions)
    check_reduce_dims(['x', 'y'], dimensions)
    with pytest.raises(ValueError):
        check_reduce_dims('a', dimensions)

def test_groupby_fillna():
    da = DataArray([1, np.nan, 3], dims='x', coords={'x': [0, 1, 0]})
    groupby = DataArrayGroupBy(da, da['x'])
    result = groupby.fillna(0)
    assert np.array_equal(result.values, np.array([1.0, 0.0, 3.0]))

def test_groupby_quantile():
    da = DataArray([1, 2, 3], dims='x', coords={'x': [0, 1, 0]})
    groupby = DataArrayGroupBy(da, da['x'])
    result = groupby.quantile(0.5)
    assert np.array_equal(result.values, np.array([2.0, 2.0]))

def test_groupby_where():
    da = DataArray([1, 2, 3], dims='x', coords={'x': [0, 1, 0]})
    groupby = DataArrayGroupBy(da, da['x'])
    result = groupby.where(da > 1)
    assert np.array_equal(result.values, np.array([np.nan, 2.0, 3.0]))

def test_groupby_first():
    da = DataArray([1, 2, 3], dims='x', coords={'x': [0, 1, 0]})
    groupby = DataArrayGroupBy(da, da['x'])
    result = groupby.first()
    assert np.array_equal(result.values, np.array([1.0, 2.0]))

def test_groupby_last():
    da = DataArray([1, 2, 3], dims='x', coords={'x': [0, 1, 0]})
    groupby = DataArrayGroupBy(da, da['x'])
    result = groupby.last()
    assert np.array_equal(result.values, np.array([3.0, 2.0]))

def test_groupby_assign_coords():
    da = DataArray([1, 2, 3], dims='x', coords={'x': [0, 1, 0]})
    groupby = DataArrayGroupBy(da, da['x'])
    result = groupby.assign_coords(y=('x', [4, 5, 4]))
    assert 'y' in result.coords
    np.testing.assert_array_equal(result.coords['y'].values, np.array([4, 5, 6]))

def test_groupby_reduce():
    da = DataArray([1, 2, 3], dims='x', coords={'x': [0, 1, 0]})
    groupby = DataArrayGroupBy(da, da['x'])
    result = groupby.reduce(np.sum)
    assert np.array_equal(result.values, np.array([4.0, 2.0]))

def test_groupby_repr():
    da = DataArray([1, 2, 3], dims='x', coords={'x': [0, 1, 0]})
    groupby = DataArrayGroupBy(da, da['x'])
    repr_str = repr(groupby)
    assert "DataArrayGroupBy" in repr_str
    assert "grouped over 'x'" in repr_str

def test_groupby_binary_op():
    da1 = DataArray([1, 2, 3], dims='x', coords={'x': [0, 1, 0]})
    da2 = DataArray([4, 5, 6], dims='x', coords={'x': [0, 1, 0]})
    groupby = DataArrayGroupBy(da1, da1['x'])
    result = groupby + da2
    assert np.array_equal(result.values, np.array([5, 7, 9]))

def test_groupby_empty():
    da = DataArray([], dims='x', coords={'x': []})
    with pytest.raises(ValueError):
        GroupBy(da, da['x'])

def test_unique_value_groups():
    ar = np.array([1, 2, 2, 3, 1, 4, 4, 4])
    values, groups = unique_value_groups(ar)
    assert np.array_equal(values, np.array([1, 2, 3, 4]))
    assert groups == [[0, 4], [1, 2], [3], [5, 6, 7]]

def test_dummy_copy():
    da = DataArray([1, 2, 3], dims='x')
    dummy = _dummy_copy(da)
    assert isinstance(dummy, DataArray)
    assert dummy.shape == ()

def test_is_one_or_none():
    assert _is_one_or_none(1)
    assert _is_one_or_none(None)
    assert not _is_one_or_none(2)

def test_consolidate_slices():
    slices = [slice(0, 2), slice(2, 4), slice(4, 6)]
    consolidated = _consolidate_slices(slices)
    assert consolidated == [slice(0, 6)]

def test_inverse_permutation_indices():
    positions = [np.array([0, 1]), np.array([2, 3])]
    indices = _inverse_permutation_indices(positions)
    assert np.array_equal(indices, np.array([0, 1, 2, 3]))

def test_dummy_group():
    obj = Dataset(coords={'x': ('x', [0, 1, 2])})
    group = _DummyGroup(obj, 'x', obj.coords)
    assert group.name == 'x'
    assert group.size == 3
    assert group.dims == ('x',)
    assert group.ndim == 1
    assert list(group.values) == [0, 1, 2]

def test_groupby_init():
    da = DataArray([1, 2, 3], dims='x', coords={'x': [0, 1, 0]})
    groupby = GroupBy(da, da['x'])
    assert groupby._group.name == 'x'
    assert len(groupby) == 2

def test_groupby_iter():
    da = DataArray([1, 2, 3], dims='x', coords={'x': [0, 1, 0]})
    groupby = GroupBy(da, da['x'])
    groups = list(groupby)
    assert len(groups) == 2
    assert np.array_equal(groups[0][1].values, np.array([1, 3]))
    assert np.array_equal(groups[1][1].values, np.array([2]))

def test_dataarray_groupby_map():
    da = DataArray([1, 2, 3], dims='x', coords={'x': [0, 1, 0]})
    groupby = DataArrayGroupBy(da, da['x'])
    result = groupby.map(lambda x: x.mean())
    assert np.array_equal(result.values, np.array([2.0, 2.0]))

def test_dataset_groupby_map():
    ds = Dataset({'a': ('x', [1, 2, 3])}, coords={'x': [0, 1, 0]})
    groupby = DatasetGroupBy(ds, ds['x'])
    result = groupby.map(lambda x: x.mean())
    assert np.array_equal(result['a'].values, np.array([2.0, 2.0]))
Coverage: 74.03314917127072
Mutation Score: 30.159999999999997
Output: On branch main
Changes not staged for commit:
  (use "git add/rm <file>..." to update what will be committed)
  (use "git restore <file>..." to discard changes in working directory)
        deleted:    .coveragerc

Untracked files:
  (use "git add <file>..." to include in what will be committed)
        xarray/tests/test_groupby.py

no changes added to commit (use "git add" and/or "git commit -a")
commit b73e100e3976dbc4f9705fd333ef866887e6bddb
Author: TestGenEval <>
Date:   Wed Dec 11 12:40:26 2024 +0000

    Testing fixes

diff --git a/xarray/core/groupby.py b/xarray/core/groupby.py
index 8a097251..af5abd9a 100644
--- a/xarray/core/groupby.py
+++ b/xarray/core/groupby.py
@@ -436,7 +436,7 @@ class GroupBy(SupportsArithmetic):
         return zip(self._unique_coord.values, self._iter_grouped())

     def __repr__(self):
-        return "{}, grouped over {!r} \n{!r} groups with labels {}.".format(
+        return "{}, grouped over {!r}\n{!r} groups with labels {}.".format(
             self.__class__.__name__,
             self._unique_coord.name,
             self._unique_coord.size,
diff --git a/xarray/tests/test_groupby.py b/xarray/tests/test_groupby.py
deleted file mode 100644
index 5ef7677c..00000000
--- a/xarray/tests/test_groupby.py
+++ /dev/null
@@ -1,565 +0,0 @@
-import numpy as np
-import pandas as pd
-import pytest
-
-import xarray as xr
-from xarray.core.groupby import _consolidate_slices
-
-from . import assert_allclose, assert_equal, assert_identical, raises_regex
-
-
-@pytest.fixture
-def dataset():
-    ds = xr.Dataset(
-        {"foo": (("x", "y", "z"), np.random.randn(3, 4, 2))},
-        {"x": ["a", "b", "c"], "y": [1, 2, 3, 4], "z": [1, 2]},
-    )
-    ds["boo"] = (("z", "y"), [["f", "g", "h", "j"]] * 2)
-
-    return ds
-
-
-@pytest.fixture
-def array(dataset):
-    return dataset["foo"]
-
-
-def test_consolidate_slices():
-
-    assert _consolidate_slices([slice(3), slice(3, 5)]) == [slice(5)]
-    assert _consolidate_slices([slice(2, 3), slice(3, 6)]) == [slice(2, 6)]
-    assert _consolidate_slices([slice(2, 3, 1), slice(3, 6, 1)]) == [slice(2, 6, 1)]
-
-    slices = [slice(2, 3), slice(5, 6)]
-    assert _consolidate_slices(slices) == slices
-
-    with pytest.raises(ValueError):
-        _consolidate_slices([slice(3), 4])
-
-
-def test_groupby_dims_property(dataset):
-    assert dataset.groupby("x").dims == dataset.isel(x=1).dims
-    assert dataset.groupby("y").dims == dataset.isel(y=1).dims
-
-    stacked = dataset.stack({"xy": ("x", "y")})
-    assert stacked.groupby("xy").dims == stacked.isel(xy=0).dims
-
-
-def test_multi_index_groupby_map(dataset):
-    # regression test for GH873
-    ds = dataset.isel(z=1, drop=True)[["foo"]]
-    expected = 2 * ds
-    actual = (
-        ds.stack(space=["x", "y"])
-        .groupby("space")
-        .map(lambda x: 2 * x)
-        .unstack("space")
-    )
-    assert_equal(expected, actual)
-
-
-def test_multi_index_groupby_sum():
-    # regression test for GH873
-    ds = xr.Dataset(
-        {"foo": (("x", "y", "z"), np.ones((3, 4, 2)))},
-        {"x": ["a", "b", "c"], "y": [1, 2, 3, 4]},
-    )
-    expected = ds.sum("z")
-    actual = ds.stack(space=["x", "y"]).groupby("space").sum("z").unstack("space")
-    assert_equal(expected, actual)
-
-
-def test_groupby_da_datetime():
-    # test groupby with a DataArray of dtype datetime for GH1132
-    # create test data
-    times = pd.date_range("2000-01-01", periods=4)
-    foo = xr.DataArray([1, 2, 3, 4], coords=dict(time=times), dims="time")
-    # create test index
-    dd = times.to_pydatetime()
-    reference_dates = [dd[0], dd[2]]
-    labels = reference_dates[0:1] * 2 + reference_dates[1:2] * 2
-    ind = xr.DataArray(
-        labels, coords=dict(time=times), dims="time", name="reference_date"
-    )
-    g = foo.groupby(ind)
-    actual = g.sum(dim="time")
-    expected = xr.DataArray(
-        [3, 7], coords=dict(reference_date=reference_dates), dims="reference_date"
-    )
-    assert_equal(expected, actual)
-
-
-def test_groupby_duplicate_coordinate_labels():
-    # fix for http://stackoverflow.com/questions/38065129
-    array = xr.DataArray([1, 2, 3], [("x", [1, 1, 2])])
-    expected = xr.DataArray([3, 3], [("x", [1, 2])])
-    actual = array.groupby("x").sum()
-    assert_equal(expected, actual)
-
-
-def test_groupby_input_mutation():
-    # regression test for GH2153
-    array = xr.DataArray([1, 2, 3], [("x", [2, 2, 1])])
-    array_copy = array.copy()
-    expected = xr.DataArray([3, 3], [("x", [1, 2])])
-    actual = array.groupby("x").sum()
-    assert_identical(expected, actual)
-    assert_identical(array, array_copy)  # should not modify inputs
-
-
-@pytest.mark.parametrize(
-    "obj",
-    [
-        xr.DataArray([1, 2, 3, 4, 5, 6], [("x", [1, 1, 1, 2, 2, 2])]),
-        xr.Dataset({"foo": ("x", [1, 2, 3, 4, 5, 6])}, {"x": [1, 1, 1, 2, 2, 2]}),
-    ],
-)
-def test_groupby_map_shrink_groups(obj):
-    expected = obj.isel(x=[0, 1, 3, 4])
-    actual = obj.groupby("x").map(lambda f: f.isel(x=[0, 1]))
-    assert_identical(expected, actual)
-
-
-@pytest.mark.parametrize(
-    "obj",
-    [
-        xr.DataArray([1, 2, 3], [("x", [1, 2, 2])]),
-        xr.Dataset({"foo": ("x", [1, 2, 3])}, {"x": [1, 2, 2]}),
-    ],
-)
-def test_groupby_map_change_group_size(obj):
-    def func(group):
-        if group.sizes["x"] == 1:
-            result = group.isel(x=[0, 0])
-        else:
-            result = group.isel(x=[0])
-        return result
-
-    expected = obj.isel(x=[0, 0, 1])
-    actual = obj.groupby("x").map(func)
-    assert_identical(expected, actual)
-
-
-def test_da_groupby_map_func_args():
-    def func(arg1, arg2, arg3=0):
-        return arg1 + arg2 + arg3
-
-    array = xr.DataArray([1, 1, 1], [("x", [1, 2, 3])])
-    expected = xr.DataArray([3, 3, 3], [("x", [1, 2, 3])])
-    actual = array.groupby("x").map(func, args=(1,), arg3=1)
-    assert_identical(expected, actual)
-
-
-def test_ds_groupby_map_func_args():
-    def func(arg1, arg2, arg3=0):
-        return arg1 + arg2 + arg3
-
-    dataset = xr.Dataset({"foo": ("x", [1, 1, 1])}, {"x": [1, 2, 3]})
-    expected = xr.Dataset({"foo": ("x", [3, 3, 3])}, {"x": [1, 2, 3]})
-    actual = dataset.groupby("x").map(func, args=(1,), arg3=1)
-    assert_identical(expected, actual)
-
-
-def test_da_groupby_empty():
-
-    empty_array = xr.DataArray([], dims="dim")
-
-    with pytest.raises(ValueError):
-        empty_array.groupby("dim")
-
-
-def test_da_groupby_quantile():
-
-    array = xr.DataArray(
-        data=[1, 2, 3, 4, 5, 6], coords={"x": [1, 1, 1, 2, 2, 2]}, dims="x"
-    )
-
-    # Scalar quantile
-    expected = xr.DataArray(
-        data=[2, 5], coords={"x": [1, 2], "quantile": 0.5}, dims="x"
-    )
-    actual = array.groupby("x").quantile(0.5)
-    assert_identical(expected, actual)
-
-    # Vector quantile
-    expected = xr.DataArray(
-        data=[[1, 3], [4, 6]],
-        coords={"x": [1, 2], "quantile": [0, 1]},
-        dims=("x", "quantile"),
-    )
-    actual = array.groupby("x").quantile([0, 1])
-    assert_identical(expected, actual)
-
-    # Multiple dimensions
-    array = xr.DataArray(
-        data=[[1, 11, 26], [2, 12, 22], [3, 13, 23], [4, 16, 24], [5, 15, 25]],
-        coords={"x": [1, 1, 1, 2, 2], "y": [0, 0, 1]},
-        dims=("x", "y"),
-    )
-
-    actual_x = array.groupby("x").quantile(0, dim=...)
-    expected_x = xr.DataArray(
-        data=[1, 4], coords={"x": [1, 2], "quantile": 0}, dims="x"
-    )
-    assert_identical(expected_x, actual_x)
-
-    actual_y = array.groupby("y").quantile(0, dim=...)
-    expected_y = xr.DataArray(
-        data=[1, 22], coords={"y": [0, 1], "quantile": 0}, dims="y"
-    )
-    assert_identical(expected_y, actual_y)
-
-    actual_xx = array.groupby("x").quantile(0)
-    expected_xx = xr.DataArray(
-        data=[[1, 11, 22], [4, 15, 24]],
-        coords={"x": [1, 2], "y": [0, 0, 1], "quantile": 0},
-        dims=("x", "y"),
-    )
-    assert_identical(expected_xx, actual_xx)
-
-    actual_yy = array.groupby("y").quantile(0)
-    expected_yy = xr.DataArray(
-        data=[[1, 26], [2, 22], [3, 23], [4, 24], [5, 25]],
-        coords={"x": [1, 1, 1, 2, 2], "y": [0, 1], "quantile": 0},
-        dims=("x", "y"),
-    )
-    assert_identical(expected_yy, actual_yy)
-
-    times = pd.date_range("2000-01-01", periods=365)
-    x = [0, 1]
-    foo = xr.DataArray(
-        np.reshape(np.arange(365 * 2), (365, 2)),
-        coords={"time": times, "x": x},
-        dims=("time", "x"),
-    )
-    g = foo.groupby(foo.time.dt.month)
-
-    actual = g.quantile(0, dim=...)
-    expected = xr.DataArray(
-        data=[
-            0.0,
-            62.0,
-            120.0,
-            182.0,
-            242.0,
-            304.0,
-            364.0,
-            426.0,
-            488.0,
-            548.0,
-            610.0,
-            670.0,
-        ],
-        coords={"month": np.arange(1, 13), "quantile": 0},
-        dims="month",
-    )
-    assert_identical(expected, actual)
-
-    actual = g.quantile(0, dim="time")[:2]
-    expected = xr.DataArray(
-        data=[[0.0, 1], [62.0, 63]],
-        coords={"month": [1, 2], "x": [0, 1], "quantile": 0},
-        dims=("month", "x"),
-    )
-    assert_identical(expected, actual)
-
-
-def test_ds_groupby_quantile():
-    ds = xr.Dataset(
-        data_vars={"a": ("x", [1, 2, 3, 4, 5, 6])}, coords={"x": [1, 1, 1, 2, 2, 2]}
-    )
-
-    # Scalar quantile
-    expected = xr.Dataset(
-        data_vars={"a": ("x", [2, 5])}, coords={"quantile": 0.5, "x": [1, 2]}
-    )
-    actual = ds.groupby("x").quantile(0.5)
-    assert_identical(expected, actual)
-
-    # Vector quantile
-    expected = xr.Dataset(
-        data_vars={"a": (("x", "quantile"), [[1, 3], [4, 6]])},
-        coords={"x": [1, 2], "quantile": [0, 1]},
-    )
-    actual = ds.groupby("x").quantile([0, 1])
-    assert_identical(expected, actual)
-
-    # Multiple dimensions
-    ds = xr.Dataset(
-        data_vars={
-            "a": (
-                ("x", "y"),
-                [[1, 11, 26], [2, 12, 22], [3, 13, 23], [4, 16, 24], [5, 15, 25]],
-            )
-        },
-        coords={"x": [1, 1, 1, 2, 2], "y": [0, 0, 1]},
-    )
-
-    actual_x = ds.groupby("x").quantile(0, dim=...)
-    expected_x = xr.Dataset({"a": ("x", [1, 4])}, coords={"x": [1, 2], "quantile": 0})
-    assert_identical(expected_x, actual_x)
-
-    actual_y = ds.groupby("y").quantile(0, dim=...)
-    expected_y = xr.Dataset({"a": ("y", [1, 22])}, coords={"y": [0, 1], "quantile": 0})
-    assert_identical(expected_y, actual_y)
-
-    actual_xx = ds.groupby("x").quantile(0)
-    expected_xx = xr.Dataset(
-        {"a": (("x", "y"), [[1, 11, 22], [4, 15, 24]])},
-        coords={"x": [1, 2], "y": [0, 0, 1], "quantile": 0},
-    )
-    assert_identical(expected_xx, actual_xx)
-
-    actual_yy = ds.groupby("y").quantile(0)
-    expected_yy = xr.Dataset(
-        {"a": (("x", "y"), [[1, 26], [2, 22], [3, 23], [4, 24], [5, 25]])},
-        coords={"x": [1, 1, 1, 2, 2], "y": [0, 1], "quantile": 0},
-    ).transpose()
-    assert_identical(expected_yy, actual_yy)
-
-    times = pd.date_range("2000-01-01", periods=365)
-    x = [0, 1]
-    foo = xr.Dataset(
-        {"a": (("time", "x"), np.reshape(np.arange(365 * 2), (365, 2)))},
-        coords=dict(time=times, x=x),
-    )
-    g = foo.groupby(foo.time.dt.month)
-
-    actual = g.quantile(0, dim=...)
-    expected = xr.Dataset(
-        {
-            "a": (
-                "month",
-                [
-                    0.0,
-                    62.0,
-                    120.0,
-                    182.0,
-                    242.0,
-                    304.0,
-                    364.0,
-                    426.0,
-                    488.0,
-                    548.0,
-                    610.0,
-                    670.0,
-                ],
-            )
-        },
-        coords={"month": np.arange(1, 13), "quantile": 0},
-    )
-    assert_identical(expected, actual)
-
-    actual = g.quantile(0, dim="time").isel(month=slice(None, 2))
-    expected = xr.Dataset(
-        data_vars={"a": (("month", "x"), [[0.0, 1], [62.0, 63]])},
-        coords={"month": [1, 2], "x": [0, 1], "quantile": 0},
-    )
-    assert_identical(expected, actual)
-
-
-def test_da_groupby_assign_coords():
-    actual = xr.DataArray(
-        [[3, 4, 5], [6, 7, 8]], dims=["y", "x"], coords={"y": range(2), "x": range(3)}
-    )
-    actual1 = actual.groupby("x").assign_coords({"y": [-1, -2]})
-    actual2 = actual.groupby("x").assign_coords(y=[-1, -2])
-    expected = xr.DataArray(
-        [[3, 4, 5], [6, 7, 8]], dims=["y", "x"], coords={"y": [-1, -2], "x": range(3)}
-    )
-    assert_identical(expected, actual1)
-    assert_identical(expected, actual2)
-
-
-repr_da = xr.DataArray(
-    np.random.randn(10, 20, 6, 24),
-    dims=["x", "y", "z", "t"],
-    coords={
-        "z": ["a", "b", "c", "a", "b", "c"],
-        "x": [1, 1, 1, 2, 2, 3, 4, 5, 3, 4],
-        "t": pd.date_range("2001-01-01", freq="M", periods=24),
-        "month": ("t", list(range(1, 13)) * 2),
-    },
-)
-
-
-@pytest.mark.parametrize("dim", ["x", "y", "z", "month"])
-@pytest.mark.parametrize("obj", [repr_da, repr_da.to_dataset(name="a")])
-def test_groupby_repr(obj, dim):
-    actual = repr(obj.groupby(dim))
-    expected = "%sGroupBy" % obj.__class__.__name__
-    expected += ", grouped over %r " % dim
-    expected += "\n%r groups with labels " % (len(np.unique(obj[dim])))
-    if dim == "x":
-        expected += "1, 2, 3, 4, 5."
-    elif dim == "y":
-        expected += "0, 1, 2, 3, 4, 5, ..., 15, 16, 17, 18, 19."
-    elif dim == "z":
-        expected += "'a', 'b', 'c'."
-    elif dim == "month":
-        expected += "1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12."
-    assert actual == expected
-
-
-@pytest.mark.parametrize("obj", [repr_da, repr_da.to_dataset(name="a")])
-def test_groupby_repr_datetime(obj):
-    actual = repr(obj.groupby("t.month"))
-    expected = "%sGroupBy" % obj.__class__.__name__
-    expected += ", grouped over 'month' "
-    expected += "\n%r groups with labels " % (len(np.unique(obj.t.dt.month)))
-    expected += "1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12."
-    assert actual == expected
-
-
-def test_groupby_drops_nans():
-    # GH2383
-    # nan in 2D data variable (requires stacking)
-    ds = xr.Dataset(
-        {
-            "variable": (("lat", "lon", "time"), np.arange(60.0).reshape((4, 3, 5))),
-            "id": (("lat", "lon"), np.arange(12.0).reshape((4, 3))),
-        },
-        coords={"lat": np.arange(4), "lon": np.arange(3), "time": np.arange(5)},
-    )
-
-    ds["id"].values[0, 0] = np.nan
-    ds["id"].values[3, 0] = np.nan
-    ds["id"].values[-1, -1] = np.nan
-
-    grouped = ds.groupby(ds.id)
-
-    # non reduction operation
-    expected = ds.copy()
-    expected.variable.values[0, 0, :] = np.nan
-    expected.variable.values[-1, -1, :] = np.nan
-    expected.variable.values[3, 0, :] = np.nan
-    actual = grouped.map(lambda x: x).transpose(*ds.variable.dims)
-    assert_identical(actual, expected)
-
-    # reduction along grouped dimension
-    actual = grouped.mean()
-    stacked = ds.stack({"xy": ["lat", "lon"]})
-    expected = (
-        stacked.variable.where(stacked.id.notnull()).rename({"xy": "id"}).to_dataset()
-    )
-    expected["id"] = stacked.id.values
-    assert_identical(actual, expected.dropna("id").transpose(*actual.dims))
-
-    # reduction operation along a different dimension
-    actual = grouped.mean("time")
-    expected = ds.mean("time").where(ds.id.notnull())
-    assert_identical(actual, expected)
-
-    # NaN in non-dimensional coordinate
-    array = xr.DataArray([1, 2, 3], [("x", [1, 2, 3])])
-    array["x1"] = ("x", [1, 1, np.nan])
-    expected = xr.DataArray(3, [("x1", [1])])
-    actual = array.groupby("x1").sum()
-    assert_equal(expected, actual)
-
-    # NaT in non-dimensional coordinate
-    array["t"] = (
-        "x",
-        [
-            np.datetime64("2001-01-01"),
-            np.datetime64("2001-01-01"),
-            np.datetime64("NaT"),
-        ],
-    )
-    expected = xr.DataArray(3, [("t", [np.datetime64("2001-01-01")])])
-    actual = array.groupby("t").sum()
-    assert_equal(expected, actual)
-
-    # test for repeated coordinate labels
-    array = xr.DataArray([0, 1, 2, 4, 3, 4], [("x", [np.nan, 1, 1, np.nan, 2, np.nan])])
-    expected = xr.DataArray([3, 3], [("x", [1, 2])])
-    actual = array.groupby("x").sum()
-    assert_equal(expected, actual)
-
-
-def test_groupby_grouping_errors():
-    dataset = xr.Dataset({"foo": ("x", [1, 1, 1])}, {"x": [1, 2, 3]})
-    with raises_regex(ValueError, "None of the data falls within bins with edges"):
-        dataset.groupby_bins("x", bins=[0.1, 0.2, 0.3])
-
-    with raises_regex(ValueError, "None of the data falls within bins with edges"):
-        dataset.to_array().groupby_bins("x", bins=[0.1, 0.2, 0.3])
-
-    with raises_regex(ValueError, "All bin edges are NaN."):
-        dataset.groupby_bins("x", bins=[np.nan, np.nan, np.nan])
-
-    with raises_regex(ValueError, "All bin edges are NaN."):
-        dataset.to_array().groupby_bins("x", bins=[np.nan, np.nan, np.nan])
-
-    with raises_regex(ValueError, "Failed to group data."):
-        dataset.groupby(dataset.foo * np.nan)
-
-    with raises_regex(ValueError, "Failed to group data."):
-        dataset.to_array().groupby(dataset.foo * np.nan)
-
-
-def test_groupby_reduce_dimension_error(array):
-    grouped = array.groupby("y")
-    with raises_regex(ValueError, "cannot reduce over dimensions"):
-        grouped.mean()
-
-    with raises_regex(ValueError, "cannot reduce over dimensions"):
-        grouped.mean("huh")
-
-    with raises_regex(ValueError, "cannot reduce over dimensions"):
-        grouped.mean(("x", "y", "asd"))
-
-    grouped = array.groupby("y", squeeze=False)
-    assert_identical(array, grouped.mean())
-
-    assert_identical(array.mean("x"), grouped.reduce(np.mean, "x"))
-    assert_allclose(array.mean(["x", "z"]), grouped.reduce(np.mean, ["x", "z"]))
-
-
-def test_groupby_multiple_string_args(array):
-    with pytest.raises(TypeError):
-        array.groupby("x", "y")
-
-
-def test_groupby_bins_timeseries():
-    ds = xr.Dataset()
-    ds["time"] = xr.DataArray(
-        pd.date_range("2010-08-01", "2010-08-15", freq="15min"), dims="time"
-    )
-    ds["val"] = xr.DataArray(np.ones(*ds["time"].shape), dims="time")
-    time_bins = pd.date_range(start="2010-08-01", end="2010-08-15", freq="24H")
-    actual = ds.groupby_bins("time", time_bins).sum()
-    expected = xr.DataArray(
-        96 * np.ones((14,)),
-        dims=["time_bins"],
-        coords={"time_bins": pd.cut(time_bins, time_bins).categories},
-    ).to_dataset(name="val")
-    assert_identical(actual, expected)
-
-
-def test_groupby_none_group_name():
-    # GH158
-    # xarray should not fail if a DataArray's name attribute is None
-
-    data = np.arange(10) + 10
-    da = xr.DataArray(data)  # da.name = None
-    key = xr.DataArray(np.floor_divide(data, 2))
-
-    mean = da.groupby(key).mean()
-    assert "group" in mean.dims
-
-
-def test_groupby_getitem(dataset):
-
-    assert_identical(dataset.sel(x="a"), dataset.groupby("x")["a"])
-    assert_identical(dataset.sel(z=1), dataset.groupby("z")[1])
-
-    assert_identical(dataset.foo.sel(x="a"), dataset.foo.groupby("x")["a"])
-    assert_identical(dataset.foo.sel(z=1), dataset.foo.groupby("z")[1])
-
-    actual = dataset.groupby("boo")["f"].unstack().transpose("x", "y", "z")
-    expected = dataset.sel(y=[1], z=[1, 2]).transpose("x", "y", "z")
-    assert_identical(expected, actual)
-
-
-# TODO: move other groupby tests from test_dataset and test_dataarray over here
Obtaining file:///testbed
  Installing build dependencies: started
  Installing build dependencies: finished with status 'done'
  Checking if build backend supports build_editable: started
  Checking if build backend supports build_editable: finished with status 'done'
  Getting requirements to build editable: started
  Getting requirements to build editable: finished with status 'done'
  Preparing editable metadata (pyproject.toml): started
  Preparing editable metadata (pyproject.toml): finished with status 'done'
Requirement already satisfied: numpy>=1.17 in /opt/miniconda3/envs/testbed/lib/python3.10/site-packages (from xarray==0.17.1.dev53+gb73e100e.d20250207) (1.23.0)
Requirement already satisfied: pandas>=1.0 in /opt/miniconda3/envs/testbed/lib/python3.10/site-packages (from xarray==0.17.1.dev53+gb73e100e.d20250207) (1.5.3)
Requirement already satisfied: setuptools>=40.4 in /opt/miniconda3/envs/testbed/lib/python3.10/site-packages (from xarray==0.17.1.dev53+gb73e100e.d20250207) (68.0.0)
Requirement already satisfied: python-dateutil>=2.8.1 in /opt/miniconda3/envs/testbed/lib/python3.10/site-packages (from pandas>=1.0->xarray==0.17.1.dev53+gb73e100e.d20250207) (2.8.2)
Requirement already satisfied: pytz>=2020.1 in /opt/miniconda3/envs/testbed/lib/python3.10/site-packages (from pandas>=1.0->xarray==0.17.1.dev53+gb73e100e.d20250207) (2023.3)
Requirement already satisfied: six>=1.5 in /opt/miniconda3/envs/testbed/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas>=1.0->xarray==0.17.1.dev53+gb73e100e.d20250207) (1.16.0)
Building wheels for collected packages: xarray
  Building editable for xarray (pyproject.toml): started
  Building editable for xarray (pyproject.toml): finished with status 'done'
  Created wheel for xarray: filename=xarray-0.17.1.dev53+gb73e100e.d20250207-0.editable-py3-none-any.whl size=9074 sha256=615381ae3c039b703e1f1425aae46d163f581176157f00cb8233ce31d7c0a3cd
  Stored in directory: /tmp/pip-ephem-wheel-cache-6a7tcgf_/wheels/0d/a6/cb/465a7b303d624cc531250fa27c75d038ddc29430bdb6ba7c9f
Successfully built xarray
Installing collected packages: xarray
  Attempting uninstall: xarray
    Found existing installation: xarray 0.17.1.dev53+gb73e100e
    Uninstalling xarray-0.17.1.dev53+gb73e100e:
      Successfully uninstalled xarray-0.17.1.dev53+gb73e100e
Successfully installed xarray-0.17.1.dev53+gb73e100e.d20250207
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.
============================= test session starts ==============================
platform linux -- Python 3.10.14, pytest-7.4.0, pluggy-1.5.0
rootdir: /testbed
configfile: setup.cfg
plugins: env-1.1.3, cov-5.0.0, hypothesis-6.110.1, xdist-3.6.1
collected 19 items

xarray/tests/test_groupby.py ...................                         [100%]

=============================== warnings summary ===============================
xarray/__init__.py:1
  /testbed/xarray/__init__.py:1: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html
    import pkg_resources

../opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pkg_resources/__init__.py:2871
  /opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`.
  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
    declare_namespace(pkg)

../opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pkg_resources/__init__.py:2871
  /opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap.responses')`.
  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
    declare_namespace(pkg)

../opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pkg_resources/__init__.py:2350
../opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pkg_resources/__init__.py:2350
../opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pkg_resources/__init__.py:2350
  /opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pkg_resources/__init__.py:2350: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`.
  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
    declare_namespace(parent)

../opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pkg_resources/__init__.py:2871
  /opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap.handlers')`.
  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
    declare_namespace(pkg)

../opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pkg_resources/__init__.py:2871
  /opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap.tests')`.
  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
    declare_namespace(pkg)

xarray/core/dask_array_compat.py:61
xarray/core/dask_array_compat.py:61
  /testbed/xarray/core/dask_array_compat.py:61: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    if LooseVersion(dask_version) > LooseVersion("2.9.0"):

xarray/core/dask_array_compat.py:98
xarray/core/dask_array_compat.py:98
  /testbed/xarray/core/dask_array_compat.py:98: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    if LooseVersion(dask_version) > LooseVersion("2.30.0"):

xarray/core/dask_array_compat.py:155
xarray/core/dask_array_compat.py:155
  /testbed/xarray/core/dask_array_compat.py:155: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    if LooseVersion(dask_version) > LooseVersion("2021.03.0"):

xarray/core/npcompat.py:87
  /testbed/xarray/core/npcompat.py:87: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    if LooseVersion(np.__version__) >= "1.20.0":

../opt/miniconda3/envs/testbed/lib/python3.10/site-packages/setuptools/_distutils/version.py:345
../opt/miniconda3/envs/testbed/lib/python3.10/site-packages/setuptools/_distutils/version.py:345
  /opt/miniconda3/envs/testbed/lib/python3.10/site-packages/setuptools/_distutils/version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    other = LooseVersion(other)

xarray/core/pdcompat.py:45
  /testbed/xarray/core/pdcompat.py:45: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    if LooseVersion(pd.__version__) < "0.25.0":

xarray/coding/cftimeindex.py:62
xarray/coding/cftimeindex.py:62
  /testbed/xarray/coding/cftimeindex.py:62: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    if LooseVersion(pd.__version__) > LooseVersion("1.2.3"):

xarray/tests/__init__.py:59
xarray/tests/__init__.py:59
xarray/tests/__init__.py:59
xarray/tests/__init__.py:59
xarray/tests/__init__.py:59
xarray/tests/__init__.py:59
  /testbed/xarray/tests/__init__.py:59: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    return version.LooseVersion(vstring)

xarray/tests/test_groupby.py::test_groupby_quantile
xarray/tests/test_groupby.py::test_groupby_quantile
  /testbed/xarray/core/variable.py:1933: DeprecationWarning: the `interpolation=` argument to nanquantile was renamed to `method=`, which has additional options.
  Users of the modes 'nearest', 'lower', 'higher', or 'midpoint' are encouraged to review the method they used. (Deprecated NumPy 1.22)
    return np.moveaxis(_quantile_func(npa, **kwargs), 0, -1)

xarray/tests/test_groupby.py::test_groupby_empty
  /testbed/xarray/core/groupby.py:207: FutureWarning: is_monotonic is deprecated and will be removed in a future version. Use is_monotonic_increasing instead.
    return index.is_unique and index.is_monotonic

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
==================================== PASSES ====================================
=========================== short test summary info ============================
PASSED xarray/tests/test_groupby.py::test_check_reduce_dims
PASSED xarray/tests/test_groupby.py::test_groupby_fillna
PASSED xarray/tests/test_groupby.py::test_groupby_quantile
PASSED xarray/tests/test_groupby.py::test_groupby_first
PASSED xarray/tests/test_groupby.py::test_groupby_last
PASSED xarray/tests/test_groupby.py::test_groupby_reduce
PASSED xarray/tests/test_groupby.py::test_groupby_repr
PASSED xarray/tests/test_groupby.py::test_groupby_binary_op
PASSED xarray/tests/test_groupby.py::test_groupby_empty
PASSED xarray/tests/test_groupby.py::test_unique_value_groups
PASSED xarray/tests/test_groupby.py::test_dummy_copy
PASSED xarray/tests/test_groupby.py::test_is_one_or_none
PASSED xarray/tests/test_groupby.py::test_consolidate_slices
PASSED xarray/tests/test_groupby.py::test_inverse_permutation_indices
PASSED xarray/tests/test_groupby.py::test_dummy_group
PASSED xarray/tests/test_groupby.py::test_groupby_init
PASSED xarray/tests/test_groupby.py::test_groupby_iter
PASSED xarray/tests/test_groupby.py::test_dataarray_groupby_map
PASSED xarray/tests/test_groupby.py::test_dataset_groupby_map
======================= 19 passed, 29 warnings in 3.38s ========================

