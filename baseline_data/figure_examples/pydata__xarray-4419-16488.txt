Instance ID: pydata__xarray-4419-16488

Baseline 1 (Pynguin):
Predicted Test Suite: # Test cases automatically generated by Pynguin (https://www.pynguin.eu).
# Please check them before you use them.
import pytest
import xarray.core.concat as module_0
import platform as module_1


@pytest.mark.xfail(strict=True)
def test_case_0():
    none_type_0 = None
    module_0.concat(none_type_0, none_type_0, join=none_type_0)


@pytest.mark.xfail(strict=True)
def test_case_1():
    list_0 = []
    none_type_0 = None
    module_0.concat(list_0, none_type_0, list_0, compat=list_0)


@pytest.mark.xfail(strict=True)
def test_case_2():
    var_0 = module_1.version()
    module_0.concat(var_0, var_0, var_0, join=var_0, combine_attrs=var_0)


@pytest.mark.xfail(strict=True)
def test_case_3():
    var_0 = module_1.version()
    module_0.concat(var_0, var_0, coords=var_0, compat=var_0, combine_attrs=var_0)

Coverage: 14.285714285714286
Mutation Score: 0.25

Baseline 2 (CodaMosa):
Predicted Test Suite: import xarray.core.concat as module_0

def test_case_1():
    try:
        bool_0 = True
        bytes_0 = b'\x1e\xd4\x9f\x99\n\xa0\xb2\xf7;M^'
        var_0 = module_0.concat(bool_0, bytes_0)
    except BaseException:
        pass


def test_case_2():
    try:
        float_0 = 890.037
        dict_0 = {float_0: float_0, float_0: float_0}
        str_0 = '/GMhw~o\ty@'
        bool_0 = False
        tuple_0 = None
        bytes_0 = b'\xd2+I2a'
        var_0 = module_0.concat(dict_0, str_0, bool_0, tuple_0, bytes_0)
    except BaseException:
        pass


def test_case_3():
    try:
        str_0 = 'This gets called if we are applying on an array with a\n        multidimensional group.'
        var_0 = module_0.concat(str_0, str_0)
    except BaseException:
        pass


def test_case_4():
    try:
        bytes_0 = b''
        bool_0 = True
        var_0 = module_0.concat(bytes_0, bool_0)
    except BaseException:
        pass# Automatically generated by Pynguin.

def test_case_5():
    pass

Coverage: 14.285714285714286
Mutation Score: 0.25

Baseline 3 (GPT-4o):
Predicted Test Suite: 
import pytest
import pandas as pd
import numpy as np
from xarray import DataArray, Dataset, concat

def create_test_datasets():
    ds1 = Dataset(
        {
            "var1": ("x", [1, 2, 3]),
            "var2": (("x", "y"), [[1, 2], [3, 4], [5, 6]]),
        },
        coords={"x": [0, 1, 2], "y": [0, 1]},
    )
    ds2 = Dataset(
        {
            "var1": ("x", [4, 5, 6]),
            "var2": (("x", "y"), [[7, 8], [9, 10], [11, 12]]),
        },
        coords={"x": [3, 4, 5], "y": [0, 1]},
    )
    return ds1, ds2

def create_test_dataarrays():
    da1 = DataArray([1, 2, 3], dims="x", name="var1")
    da2 = DataArray([4, 5, 6], dims="x", name="var1")
    return da1, da2

def test_concat_datasets_along_new_dim():
    ds1, ds2 = create_test_datasets()
    result = concat([ds1, ds2], dim="new_dim")
    assert "new_dim" in result.dims
    assert result.dims["new_dim"] == 2
    assert result["var1"].shape == (2, 3)

def test_concat_datasets_along_existing_dim():
    ds1, ds2 = create_test_datasets()
    result = concat([ds1, ds2], dim="x")
    assert "x" in result.dims
    assert result.dims["x"] == 6
    assert np.array_equal(result["var1"], [1, 2, 3, 4, 5, 6])

def test_concat_dataarrays_along_new_dim():
    da1, da2 = create_test_dataarrays()
    result = concat([da1, da2], dim="new_dim")
    assert "new_dim" in result.dims
    assert result.dims["new_dim"] == 2
    assert result.shape == (2, 3)

def test_concat_dataarrays_along_existing_dim():
    da1, da2 = create_test_dataarrays()
    result = concat([da1, da2], dim="x")
    assert "x" in result.dims
    assert result.dims["x"] == 6
    assert np.array_equal(result, [1, 2, 3, 4, 5, 6])

def test_concat_with_different_coords():
    ds1, ds2 = create_test_datasets()
    ds2 = ds2.assign_coords(y=[1, 2])
    with pytest.raises(ValueError):
        concat([ds1, ds2], dim="x", coords="different")

def test_concat_with_fill_value():
    ds1, ds2 = create_test_datasets()
    ds2 = ds2.drop_vars("var2")
    result = concat([ds1, ds2], dim="x", fill_value=-1)
    assert np.array_equal(result["var2"].sel(x=[3, 4, 5]), -1)

def test_concat_with_override_compat():
    ds1, ds2 = create_test_datasets()
    ds2["var1"].attrs["units"] = "meters"
    result = concat([ds1, ds2], dim="x", compat="override")
    assert result["var1"].attrs.get("units") is None

def test_concat_with_identical_compat():
    ds1, ds2 = create_test_datasets()
    ds2["var1"].attrs["units"] = "meters"
    with pytest.raises(ValueError):
        concat([ds1, ds2], dim="x", compat="identical")

def test_concat_with_no_conflicts_compat():
    ds1, ds2 = create_test_datasets()
    ds2["var1"].attrs["units"] = "meters"
    result = concat([ds1, ds2], dim="x", compat="no_conflicts")
    assert result["var1"].attrs.get("units") is None

def test_concat_with_override_combine_attrs():
    ds1, ds2 = create_test_datasets()
    ds1.attrs["description"] = "dataset 1"
    ds2.attrs["description"] = "dataset 2"
    result = concat([ds1, ds2], dim="x", combine_attrs="override")
    assert result.attrs["description"] == "dataset 1"

def test_concat_with_identical_combine_attrs():
    ds1, ds2 = create_test_datasets()
    ds1.attrs["description"] = "same"
    ds2.attrs["description"] = "same"
    result = concat([ds1, ds2], dim="x", combine_attrs="identical")
    assert result.attrs["description"] == "same"

def test_concat_with_drop_combine_attrs():
    ds1, ds2 = create_test_datasets()
    ds1.attrs["description"] = "dataset 1"
    ds2.attrs["description"] = "dataset 2"
    result = concat([ds1, ds2], dim="x", combine_attrs="drop")
    assert "description" not in result.attrs

Coverage: 64.28571428571429
Mutation Score: 33.66

Approach:
Predicted Test Suite: import pytest
import pandas as pd
import numpy as np
from xarray import DataArray, Dataset, concat

def create_test_datasets():
    ds1 = Dataset(
        {
            "var1": ("x", [1, 2, 3]),
            "var2": (("x", "y"), [[1, 2], [3, 4], [5, 6]]),
        },
        coords={"x": [0, 1, 2], "y": [0, 1]},
    )
    ds2 = Dataset(
        {
            "var1": ("x", [4, 5, 6]),
            "var2": (("x", "y"), [[7, 8], [9, 10], [11, 12]]),
        },
        coords={"x": [3, 4, 5], "y": [0, 1]},
    )
    return ds1, ds2

def create_test_dataarrays():
    da1 = DataArray([1, 2, 3], dims="x", name="var1")
    da2 = DataArray([4, 5, 6], dims="x", name="var1")
    return da1, da2

def test_concat_datasets_along_new_dim():
    ds1, ds2 = create_test_datasets()
    result = concat([ds1, ds2], dim="new_dim")
    assert "new_dim" in result.dims
    assert result.dims["new_dim"] == 2
    assert result["var1"].shape == (2, 3, 2)

def test_concat_datasets_along_existing_dim():
    ds1, ds2 = create_test_datasets()
    result = concat([ds1, ds2], dim="x")
    assert "x" in result.dims
    assert result.dims["x"] == 6
    assert np.array_equal(result["var1"], [1, 2, 3, 4, 5, 6])

def test_concat_dataarrays_along_new_dim():
    da1, da2 = create_test_dataarrays()
    result = concat([da1, da2], dim="new_dim")
    assert "new_dim" in result.dims
    assert result.dims["new_dim"] == 2
    assert result.shape == (2, 3, 1)

def test_concat_dataarrays_along_existing_dim():
    da1, da2 = create_test_dataarrays()
    result = concat([da1, da2], dim="x")
    assert "x" in result.dims
    assert result.dims["x"] == 6
    assert np.array_equal(result, [1, 2, 3, 4, 5, 6])
    assert result.shape == (6,)

def test_concat_with_different_coords():
    ds1, ds2 = create_test_datasets()
    ds2 = ds2.assign_coords(y=[1, 2])
    with pytest.raises(ValueError):
        concat([ds1, ds2], dim="x", coords="different")

def test_concat_with_fill_value():
    ds1, ds2 = create_test_datasets()
    ds2 = ds2.drop_vars("var2")
    result = concat([ds1, ds2], dim="x", fill_value=-1)
    assert np.array_equal(result["var2"].sel(x=[3, 4, 5]), -1)

def test_concat_with_override_compat():
    ds1, ds2 = create_test_datasets()
    ds2["var1"].attrs["units"] = "meters"
    result = concat([ds1, ds2], dim="x", compat="override")
    assert result["var1"].attrs.get("units") is None

def test_concat_with_identical_compat():
    ds1, ds2 = create_test_datasets()
    ds2["var1"].attrs["units"] = "meters"
    with pytest.raises(ValueError):
        concat([ds1, ds2], dim="x", compat="identical")

def test_concat_with_no_conflicts_compat():
    ds1, ds2 = create_test_datasets()
    ds2["var1"].attrs["units"] = "meters"
    result = concat([ds1, ds2], dim="x", compat="no_conflicts")
    assert result["var1"].attrs.get("units") is None

def test_concat_with_override_combine_attrs():
    ds1, ds2 = create_test_datasets()
    ds1.attrs["description"] = "dataset 1"
    ds2.attrs["description"] = "dataset 2"
    result = concat([ds1, ds2], dim="x", combine_attrs="override")
    assert result.attrs["description"] == "dataset 1"

def test_concat_with_identical_combine_attrs():
    ds1, ds2 = create_test_datasets()
    ds1.attrs["description"] = "same"
    ds2.attrs["description"] = "same"
    result = concat([ds1, ds2], dim="x", combine_attrs="identical")
    assert result.attrs["description"] == "same"

def test_concat_with_drop_combine_attrs():
    ds1, ds2 = create_test_datasets()
    ds1.attrs["description"] = "dataset 1"
    ds2.attrs["description"] = "dataset 2"
    result = concat([ds1, ds2], dim="x", combine_attrs="drop")
    assert "description" not in result.attrs
Coverage: 64.28571428571429
Mutation Score: 33.66
Output: On branch main
Changes not staged for commit:
  (use "git add/rm <file>..." to update what will be committed)
  (use "git restore <file>..." to discard changes in working directory)
        deleted:    .coveragerc

Untracked files:
  (use "git add <file>..." to include in what will be committed)
        xarray/tests/test_concat.py

no changes added to commit (use "git add" and/or "git commit -a")
commit 757bee64f3b7537be4a1572d97861beea183ed58
Author: TestGenEval <>
Date:   Wed Dec 11 12:21:47 2024 +0000

    Testing fixes

diff --git a/xarray/core/concat.py b/xarray/core/concat.py
index 0955a95f..3a39369e 100644
--- a/xarray/core/concat.py
+++ b/xarray/core/concat.py
@@ -463,6 +463,9 @@ def _dataset_concat(
             combined = concat_vars(vars, dim, positions)
             assert isinstance(combined, Variable)
             result_vars[k] = combined
+        elif k in result_vars:
+            # preserves original variable order
+            result_vars[k] = result_vars.pop(k)

     result = Dataset(result_vars, attrs=result_attrs)
     absent_coord_names = coord_names - set(result.variables)
diff --git a/xarray/tests/test_concat.py b/xarray/tests/test_concat.py
deleted file mode 100644
index 07ae83d3..00000000
--- a/xarray/tests/test_concat.py
+++ /dev/null
@@ -1,560 +0,0 @@
-from copy import deepcopy
-
-import numpy as np
-import pandas as pd
-import pytest
-
-from xarray import DataArray, Dataset, Variable, concat
-from xarray.core import dtypes, merge
-
-from . import (
-    InaccessibleArray,
-    assert_array_equal,
-    assert_equal,
-    assert_identical,
-    raises_regex,
-    requires_dask,
-)
-from .test_dataset import create_test_data
-
-
-def test_concat_compat():
-    ds1 = Dataset(
-        {
-            "has_x_y": (("y", "x"), [[1, 2]]),
-            "has_x": ("x", [1, 2]),
-            "no_x_y": ("z", [1, 2]),
-        },
-        coords={"x": [0, 1], "y": [0], "z": [-1, -2]},
-    )
-    ds2 = Dataset(
-        {
-            "has_x_y": (("y", "x"), [[3, 4]]),
-            "has_x": ("x", [1, 2]),
-            "no_x_y": (("q", "z"), [[1, 2]]),
-        },
-        coords={"x": [0, 1], "y": [1], "z": [-1, -2], "q": [0]},
-    )
-
-    result = concat([ds1, ds2], dim="y", data_vars="minimal", compat="broadcast_equals")
-    assert_equal(ds2.no_x_y, result.no_x_y.transpose())
-
-    for var in ["has_x", "no_x_y"]:
-        assert "y" not in result[var].dims and "y" not in result[var].coords
-    with raises_regex(ValueError, "coordinates in some datasets but not others"):
-        concat([ds1, ds2], dim="q")
-    with raises_regex(ValueError, "'q' is not present in all datasets"):
-        concat([ds2, ds1], dim="q")
-
-
-class TestConcatDataset:
-    @pytest.fixture
-    def data(self):
-        return create_test_data().drop_dims("dim3")
-
-    def rectify_dim_order(self, data, dataset):
-        # return a new dataset with all variable dimensions transposed into
-        # the order in which they are found in `data`
-        return Dataset(
-            {k: v.transpose(*data[k].dims) for k, v in dataset.data_vars.items()},
-            dataset.coords,
-            attrs=dataset.attrs,
-        )
-
-    @pytest.mark.parametrize("coords", ["different", "minimal"])
-    @pytest.mark.parametrize("dim", ["dim1", "dim2"])
-    def test_concat_simple(self, data, dim, coords):
-        datasets = [g for _, g in data.groupby(dim, squeeze=False)]
-        assert_identical(data, concat(datasets, dim, coords=coords))
-
-    def test_concat_merge_variables_present_in_some_datasets(self, data):
-        # coordinates present in some datasets but not others
-        ds1 = Dataset(data_vars={"a": ("y", [0.1])}, coords={"x": 0.1})
-        ds2 = Dataset(data_vars={"a": ("y", [0.2])}, coords={"z": 0.2})
-        actual = concat([ds1, ds2], dim="y", coords="minimal")
-        expected = Dataset({"a": ("y", [0.1, 0.2])}, coords={"x": 0.1, "z": 0.2})
-        assert_identical(expected, actual)
-
-        # data variables present in some datasets but not others
-        split_data = [data.isel(dim1=slice(3)), data.isel(dim1=slice(3, None))]
-        data0, data1 = deepcopy(split_data)
-        data1["foo"] = ("bar", np.random.randn(10))
-        actual = concat([data0, data1], "dim1")
-        expected = data.copy().assign(foo=data1.foo)
-        assert_identical(expected, actual)
-
-    def test_concat_2(self, data):
-        dim = "dim2"
-        datasets = [g for _, g in data.groupby(dim, squeeze=True)]
-        concat_over = [k for k, v in data.coords.items() if dim in v.dims and k != dim]
-        actual = concat(datasets, data[dim], coords=concat_over)
-        assert_identical(data, self.rectify_dim_order(data, actual))
-
-    @pytest.mark.parametrize("coords", ["different", "minimal", "all"])
-    @pytest.mark.parametrize("dim", ["dim1", "dim2"])
-    def test_concat_coords_kwarg(self, data, dim, coords):
-        data = data.copy(deep=True)
-        # make sure the coords argument behaves as expected
-        data.coords["extra"] = ("dim4", np.arange(3))
-        datasets = [g for _, g in data.groupby(dim, squeeze=True)]
-
-        actual = concat(datasets, data[dim], coords=coords)
-        if coords == "all":
-            expected = np.array([data["extra"].values for _ in range(data.dims[dim])])
-            assert_array_equal(actual["extra"].values, expected)
-
-        else:
-            assert_equal(data["extra"], actual["extra"])
-
-    def test_concat(self, data):
-        split_data = [
-            data.isel(dim1=slice(3)),
-            data.isel(dim1=3),
-            data.isel(dim1=slice(4, None)),
-        ]
-        assert_identical(data, concat(split_data, "dim1"))
-
-    def test_concat_dim_precedence(self, data):
-        # verify that the dim argument takes precedence over
-        # concatenating dataset variables of the same name
-        dim = (2 * data["dim1"]).rename("dim1")
-        datasets = [g for _, g in data.groupby("dim1", squeeze=False)]
-        expected = data.copy()
-        expected["dim1"] = dim
-        assert_identical(expected, concat(datasets, dim))
-
-    def test_concat_data_vars(self):
-        data = Dataset({"foo": ("x", np.random.randn(10))})
-        objs = [data.isel(x=slice(5)), data.isel(x=slice(5, None))]
-        for data_vars in ["minimal", "different", "all", [], ["foo"]]:
-            actual = concat(objs, dim="x", data_vars=data_vars)
-            assert_identical(data, actual)
-
-    def test_concat_coords(self):
-        data = Dataset({"foo": ("x", np.random.randn(10))})
-        expected = data.assign_coords(c=("x", [0] * 5 + [1] * 5))
-        objs = [
-            data.isel(x=slice(5)).assign_coords(c=0),
-            data.isel(x=slice(5, None)).assign_coords(c=1),
-        ]
-        for coords in ["different", "all", ["c"]]:
-            actual = concat(objs, dim="x", coords=coords)
-            assert_identical(expected, actual)
-        for coords in ["minimal", []]:
-            with raises_regex(merge.MergeError, "conflicting values"):
-                concat(objs, dim="x", coords=coords)
-
-    def test_concat_constant_index(self):
-        # GH425
-        ds1 = Dataset({"foo": 1.5}, {"y": 1})
-        ds2 = Dataset({"foo": 2.5}, {"y": 1})
-        expected = Dataset({"foo": ("y", [1.5, 2.5]), "y": [1, 1]})
-        for mode in ["different", "all", ["foo"]]:
-            actual = concat([ds1, ds2], "y", data_vars=mode)
-            assert_identical(expected, actual)
-        with raises_regex(merge.MergeError, "conflicting values"):
-            # previously dim="y", and raised error which makes no sense.
-            # "foo" has dimension "y" so minimal should concatenate it?
-            concat([ds1, ds2], "new_dim", data_vars="minimal")
-
-    def test_concat_size0(self):
-        data = create_test_data()
-        split_data = [data.isel(dim1=slice(0, 0)), data]
-        actual = concat(split_data, "dim1")
-        assert_identical(data, actual)
-
-        actual = concat(split_data[::-1], "dim1")
-        assert_identical(data, actual)
-
-    def test_concat_autoalign(self):
-        ds1 = Dataset({"foo": DataArray([1, 2], coords=[("x", [1, 2])])})
-        ds2 = Dataset({"foo": DataArray([1, 2], coords=[("x", [1, 3])])})
-        actual = concat([ds1, ds2], "y")
-        expected = Dataset(
-            {
-                "foo": DataArray(
-                    [[1, 2, np.nan], [1, np.nan, 2]],
-                    dims=["y", "x"],
-                    coords={"x": [1, 2, 3]},
-                )
-            }
-        )
-        assert_identical(expected, actual)
-
-    def test_concat_errors(self):
-        data = create_test_data()
-        split_data = [data.isel(dim1=slice(3)), data.isel(dim1=slice(3, None))]
-
-        with raises_regex(ValueError, "must supply at least one"):
-            concat([], "dim1")
-
-        with raises_regex(ValueError, "Cannot specify both .*='different'"):
-            concat(
-                [data, data], dim="concat_dim", data_vars="different", compat="override"
-            )
-
-        with raises_regex(ValueError, "must supply at least one"):
-            concat([], "dim1")
-
-        with raises_regex(ValueError, "are not coordinates"):
-            concat([data, data], "new_dim", coords=["not_found"])
-
-        with raises_regex(ValueError, "global attributes not"):
-            data0, data1 = deepcopy(split_data)
-            data1.attrs["foo"] = "bar"
-            concat([data0, data1], "dim1", compat="identical")
-        assert_identical(data, concat([data0, data1], "dim1", compat="equals"))
-
-        with raises_regex(ValueError, "compat.* invalid"):
-            concat(split_data, "dim1", compat="foobar")
-
-        with raises_regex(ValueError, "unexpected value for"):
-            concat([data, data], "new_dim", coords="foobar")
-
-        with raises_regex(ValueError, "coordinate in some datasets but not others"):
-            concat([Dataset({"x": 0}), Dataset({"x": [1]})], dim="z")
-
-        with raises_regex(ValueError, "coordinate in some datasets but not others"):
-            concat([Dataset({"x": 0}), Dataset({}, {"x": 1})], dim="z")
-
-    def test_concat_join_kwarg(self):
-        ds1 = Dataset({"a": (("x", "y"), [[0]])}, coords={"x": [0], "y": [0]})
-        ds2 = Dataset({"a": (("x", "y"), [[0]])}, coords={"x": [1], "y": [0.0001]})
-
-        expected = {}
-        expected["outer"] = Dataset(
-            {"a": (("x", "y"), [[0, np.nan], [np.nan, 0]])},
-            {"x": [0, 1], "y": [0, 0.0001]},
-        )
-        expected["inner"] = Dataset(
-            {"a": (("x", "y"), [[], []])}, {"x": [0, 1], "y": []}
-        )
-        expected["left"] = Dataset(
-            {"a": (("x", "y"), np.array([0, np.nan], ndmin=2).T)},
-            coords={"x": [0, 1], "y": [0]},
-        )
-        expected["right"] = Dataset(
-            {"a": (("x", "y"), np.array([np.nan, 0], ndmin=2).T)},
-            coords={"x": [0, 1], "y": [0.0001]},
-        )
-        expected["override"] = Dataset(
-            {"a": (("x", "y"), np.array([0, 0], ndmin=2).T)},
-            coords={"x": [0, 1], "y": [0]},
-        )
-
-        with raises_regex(ValueError, "indexes along dimension 'y'"):
-            actual = concat([ds1, ds2], join="exact", dim="x")
-
-        for join in expected:
-            actual = concat([ds1, ds2], join=join, dim="x")
-            assert_equal(actual, expected[join])
-
-        # regression test for #3681
-        actual = concat(
-            [ds1.drop_vars("x"), ds2.drop_vars("x")], join="override", dim="y"
-        )
-        expected = Dataset(
-            {"a": (("x", "y"), np.array([0, 0], ndmin=2))}, coords={"y": [0, 0.0001]}
-        )
-        assert_identical(actual, expected)
-
-    def test_concat_combine_attrs_kwarg(self):
-        ds1 = Dataset({"a": ("x", [0])}, coords={"x": [0]}, attrs={"b": 42})
-        ds2 = Dataset({"a": ("x", [0])}, coords={"x": [1]}, attrs={"b": 42, "c": 43})
-
-        expected = {}
-        expected["drop"] = Dataset({"a": ("x", [0, 0])}, {"x": [0, 1]})
-        expected["no_conflicts"] = Dataset(
-            {"a": ("x", [0, 0])}, {"x": [0, 1]}, {"b": 42, "c": 43}
-        )
-        expected["override"] = Dataset({"a": ("x", [0, 0])}, {"x": [0, 1]}, {"b": 42})
-
-        with raises_regex(ValueError, "combine_attrs='identical'"):
-            actual = concat([ds1, ds2], dim="x", combine_attrs="identical")
-        with raises_regex(ValueError, "combine_attrs='no_conflicts'"):
-            ds3 = ds2.copy(deep=True)
-            ds3.attrs["b"] = 44
-            actual = concat([ds1, ds3], dim="x", combine_attrs="no_conflicts")
-
-        for combine_attrs in expected:
-            actual = concat([ds1, ds2], dim="x", combine_attrs=combine_attrs)
-            assert_identical(actual, expected[combine_attrs])
-
-    def test_concat_promote_shape(self):
-        # mixed dims within variables
-        objs = [Dataset({}, {"x": 0}), Dataset({"x": [1]})]
-        actual = concat(objs, "x")
-        expected = Dataset({"x": [0, 1]})
-        assert_identical(actual, expected)
-
-        objs = [Dataset({"x": [0]}), Dataset({}, {"x": 1})]
-        actual = concat(objs, "x")
-        assert_identical(actual, expected)
-
-        # mixed dims between variables
-        objs = [Dataset({"x": [2], "y": 3}), Dataset({"x": [4], "y": 5})]
-        actual = concat(objs, "x")
-        expected = Dataset({"x": [2, 4], "y": ("x", [3, 5])})
-        assert_identical(actual, expected)
-
-        # mixed dims in coord variable
-        objs = [Dataset({"x": [0]}, {"y": -1}), Dataset({"x": [1]}, {"y": ("x", [-2])})]
-        actual = concat(objs, "x")
-        expected = Dataset({"x": [0, 1]}, {"y": ("x", [-1, -2])})
-        assert_identical(actual, expected)
-
-        # scalars with mixed lengths along concat dim -- values should repeat
-        objs = [Dataset({"x": [0]}, {"y": -1}), Dataset({"x": [1, 2]}, {"y": -2})]
-        actual = concat(objs, "x")
-        expected = Dataset({"x": [0, 1, 2]}, {"y": ("x", [-1, -2, -2])})
-        assert_identical(actual, expected)
-
-        # broadcast 1d x 1d -> 2d
-        objs = [
-            Dataset({"z": ("x", [-1])}, {"x": [0], "y": [0]}),
-            Dataset({"z": ("y", [1])}, {"x": [1], "y": [0]}),
-        ]
-        actual = concat(objs, "x")
-        expected = Dataset({"z": (("x", "y"), [[-1], [1]])}, {"x": [0, 1], "y": [0]})
-        assert_identical(actual, expected)
-
-    def test_concat_do_not_promote(self):
-        # GH438
-        objs = [
-            Dataset({"y": ("t", [1])}, {"x": 1, "t": [0]}),
-            Dataset({"y": ("t", [2])}, {"x": 1, "t": [0]}),
-        ]
-        expected = Dataset({"y": ("t", [1, 2])}, {"x": 1, "t": [0, 0]})
-        actual = concat(objs, "t")
-        assert_identical(expected, actual)
-
-        objs = [
-            Dataset({"y": ("t", [1])}, {"x": 1, "t": [0]}),
-            Dataset({"y": ("t", [2])}, {"x": 2, "t": [0]}),
-        ]
-        with pytest.raises(ValueError):
-            concat(objs, "t", coords="minimal")
-
-    def test_concat_dim_is_variable(self):
-        objs = [Dataset({"x": 0}), Dataset({"x": 1})]
-        coord = Variable("y", [3, 4])
-        expected = Dataset({"x": ("y", [0, 1]), "y": [3, 4]})
-        actual = concat(objs, coord)
-        assert_identical(actual, expected)
-
-    def test_concat_multiindex(self):
-        x = pd.MultiIndex.from_product([[1, 2, 3], ["a", "b"]])
-        expected = Dataset({"x": x})
-        actual = concat(
-            [expected.isel(x=slice(2)), expected.isel(x=slice(2, None))], "x"
-        )
-        assert expected.equals(actual)
-        assert isinstance(actual.x.to_index(), pd.MultiIndex)
-
-    @pytest.mark.parametrize("fill_value", [dtypes.NA, 2, 2.0, {"a": 2, "b": 1}])
-    def test_concat_fill_value(self, fill_value):
-        datasets = [
-            Dataset({"a": ("x", [2, 3]), "b": ("x", [-2, 1]), "x": [1, 2]}),
-            Dataset({"a": ("x", [1, 2]), "b": ("x", [3, -1]), "x": [0, 1]}),
-        ]
-        if fill_value == dtypes.NA:
-            # if we supply the default, we expect the missing value for a
-            # float array
-            fill_value_a = fill_value_b = np.nan
-        elif isinstance(fill_value, dict):
-            fill_value_a = fill_value["a"]
-            fill_value_b = fill_value["b"]
-        else:
-            fill_value_a = fill_value_b = fill_value
-        expected = Dataset(
-            {
-                "a": (("t", "x"), [[fill_value_a, 2, 3], [1, 2, fill_value_a]]),
-                "b": (("t", "x"), [[fill_value_b, -2, 1], [3, -1, fill_value_b]]),
-            },
-            {"x": [0, 1, 2]},
-        )
-        actual = concat(datasets, dim="t", fill_value=fill_value)
-        assert_identical(actual, expected)
-
-
-class TestConcatDataArray:
-    def test_concat(self):
-        ds = Dataset(
-            {
-                "foo": (["x", "y"], np.random.random((2, 3))),
-                "bar": (["x", "y"], np.random.random((2, 3))),
-            },
-            {"x": [0, 1]},
-        )
-        foo = ds["foo"]
-        bar = ds["bar"]
-
-        # from dataset array:
-        expected = DataArray(
-            np.array([foo.values, bar.values]),
-            dims=["w", "x", "y"],
-            coords={"x": [0, 1]},
-        )
-        actual = concat([foo, bar], "w")
-        assert_equal(expected, actual)
-        # from iteration:
-        grouped = [g for _, g in foo.groupby("x")]
-        stacked = concat(grouped, ds["x"])
-        assert_identical(foo, stacked)
-        # with an index as the 'dim' argument
-        stacked = concat(grouped, ds.indexes["x"])
-        assert_identical(foo, stacked)
-
-        actual = concat([foo[0], foo[1]], pd.Index([0, 1])).reset_coords(drop=True)
-        expected = foo[:2].rename({"x": "concat_dim"})
-        assert_identical(expected, actual)
-
-        actual = concat([foo[0], foo[1]], [0, 1]).reset_coords(drop=True)
-        expected = foo[:2].rename({"x": "concat_dim"})
-        assert_identical(expected, actual)
-
-        with raises_regex(ValueError, "not identical"):
-            concat([foo, bar], dim="w", compat="identical")
-
-        with raises_regex(ValueError, "not a valid argument"):
-            concat([foo, bar], dim="w", data_vars="minimal")
-
-    def test_concat_encoding(self):
-        # Regression test for GH1297
-        ds = Dataset(
-            {
-                "foo": (["x", "y"], np.random.random((2, 3))),
-                "bar": (["x", "y"], np.random.random((2, 3))),
-            },
-            {"x": [0, 1]},
-        )
-        foo = ds["foo"]
-        foo.encoding = {"complevel": 5}
-        ds.encoding = {"unlimited_dims": "x"}
-        assert concat([foo, foo], dim="x").encoding == foo.encoding
-        assert concat([ds, ds], dim="x").encoding == ds.encoding
-
-    @requires_dask
-    def test_concat_lazy(self):
-        import dask.array as da
-
-        arrays = [
-            DataArray(
-                da.from_array(InaccessibleArray(np.zeros((3, 3))), 3), dims=["x", "y"]
-            )
-            for _ in range(2)
-        ]
-        # should not raise
-        combined = concat(arrays, dim="z")
-        assert combined.shape == (2, 3, 3)
-        assert combined.dims == ("z", "x", "y")
-
-    @pytest.mark.parametrize("fill_value", [dtypes.NA, 2, 2.0])
-    def test_concat_fill_value(self, fill_value):
-        foo = DataArray([1, 2], coords=[("x", [1, 2])])
-        bar = DataArray([1, 2], coords=[("x", [1, 3])])
-        if fill_value == dtypes.NA:
-            # if we supply the default, we expect the missing value for a
-            # float array
-            fill_value = np.nan
-        expected = DataArray(
-            [[1, 2, fill_value], [1, fill_value, 2]],
-            dims=["y", "x"],
-            coords={"x": [1, 2, 3]},
-        )
-        actual = concat((foo, bar), dim="y", fill_value=fill_value)
-        assert_identical(actual, expected)
-
-    def test_concat_join_kwarg(self):
-        ds1 = Dataset(
-            {"a": (("x", "y"), [[0]])}, coords={"x": [0], "y": [0]}
-        ).to_array()
-        ds2 = Dataset(
-            {"a": (("x", "y"), [[0]])}, coords={"x": [1], "y": [0.0001]}
-        ).to_array()
-
-        expected = {}
-        expected["outer"] = Dataset(
-            {"a": (("x", "y"), [[0, np.nan], [np.nan, 0]])},
-            {"x": [0, 1], "y": [0, 0.0001]},
-        )
-        expected["inner"] = Dataset(
-            {"a": (("x", "y"), [[], []])}, {"x": [0, 1], "y": []}
-        )
-        expected["left"] = Dataset(
-            {"a": (("x", "y"), np.array([0, np.nan], ndmin=2).T)},
-            coords={"x": [0, 1], "y": [0]},
-        )
-        expected["right"] = Dataset(
-            {"a": (("x", "y"), np.array([np.nan, 0], ndmin=2).T)},
-            coords={"x": [0, 1], "y": [0.0001]},
-        )
-        expected["override"] = Dataset(
-            {"a": (("x", "y"), np.array([0, 0], ndmin=2).T)},
-            coords={"x": [0, 1], "y": [0]},
-        )
-
-        with raises_regex(ValueError, "indexes along dimension 'y'"):
-            actual = concat([ds1, ds2], join="exact", dim="x")
-
-        for join in expected:
-            actual = concat([ds1, ds2], join=join, dim="x")
-            assert_equal(actual, expected[join].to_array())
-
-    def test_concat_combine_attrs_kwarg(self):
-        da1 = DataArray([0], coords=[("x", [0])], attrs={"b": 42})
-        da2 = DataArray([0], coords=[("x", [1])], attrs={"b": 42, "c": 43})
-
-        expected = {}
-        expected["drop"] = DataArray([0, 0], coords=[("x", [0, 1])])
-        expected["no_conflicts"] = DataArray(
-            [0, 0], coords=[("x", [0, 1])], attrs={"b": 42, "c": 43}
-        )
-        expected["override"] = DataArray(
-            [0, 0], coords=[("x", [0, 1])], attrs={"b": 42}
-        )
-
-        with raises_regex(ValueError, "combine_attrs='identical'"):
-            actual = concat([da1, da2], dim="x", combine_attrs="identical")
-        with raises_regex(ValueError, "combine_attrs='no_conflicts'"):
-            da3 = da2.copy(deep=True)
-            da3.attrs["b"] = 44
-            actual = concat([da1, da3], dim="x", combine_attrs="no_conflicts")
-
-        for combine_attrs in expected:
-            actual = concat([da1, da2], dim="x", combine_attrs=combine_attrs)
-            assert_identical(actual, expected[combine_attrs])
-
-
-@pytest.mark.parametrize("attr1", ({"a": {"meta": [10, 20, 30]}}, {"a": [1, 2, 3]}, {}))
-@pytest.mark.parametrize("attr2", ({"a": [1, 2, 3]}, {}))
-def test_concat_attrs_first_variable(attr1, attr2):
-
-    arrs = [
-        DataArray([[1], [2]], dims=["x", "y"], attrs=attr1),
-        DataArray([[3], [4]], dims=["x", "y"], attrs=attr2),
-    ]
-
-    concat_attrs = concat(arrs, "y").attrs
-    assert concat_attrs == attr1
-
-
-def test_concat_merge_single_non_dim_coord():
-    da1 = DataArray([1, 2, 3], dims="x", coords={"x": [1, 2, 3], "y": 1})
-    da2 = DataArray([4, 5, 6], dims="x", coords={"x": [4, 5, 6]})
-
-    expected = DataArray(range(1, 7), dims="x", coords={"x": range(1, 7), "y": 1})
-
-    for coords in ["different", "minimal"]:
-        actual = concat([da1, da2], "x", coords=coords)
-        assert_identical(actual, expected)
-
-    with raises_regex(ValueError, "'y' is not present in all datasets."):
-        concat([da1, da2], dim="x", coords="all")
-
-    da1 = DataArray([1, 2, 3], dims="x", coords={"x": [1, 2, 3], "y": 1})
-    da2 = DataArray([4, 5, 6], dims="x", coords={"x": [4, 5, 6]})
-    da3 = DataArray([7, 8, 9], dims="x", coords={"x": [7, 8, 9], "y": 1})
-    for coords in ["different", "all"]:
-        with raises_regex(ValueError, "'y' not present in all datasets"):
-            concat([da1, da2, da3], dim="x")
Obtaining file:///testbed
  Preparing metadata (setup.py): started
  Preparing metadata (setup.py): finished with status 'done'
Requirement already satisfied: numpy>=1.15 in /opt/miniconda3/envs/testbed/lib/python3.10/site-packages (from xarray==0.16.1.dev92+g757bee64.d20250207) (1.23.0)
Requirement already satisfied: pandas>=0.25 in /opt/miniconda3/envs/testbed/lib/python3.10/site-packages (from xarray==0.16.1.dev92+g757bee64.d20250207) (1.5.3)
Requirement already satisfied: setuptools>=38.4 in /opt/miniconda3/envs/testbed/lib/python3.10/site-packages (from xarray==0.16.1.dev92+g757bee64.d20250207) (68.0.0)
Requirement already satisfied: python-dateutil>=2.8.1 in /opt/miniconda3/envs/testbed/lib/python3.10/site-packages (from pandas>=0.25->xarray==0.16.1.dev92+g757bee64.d20250207) (2.8.2)
Requirement already satisfied: pytz>=2020.1 in /opt/miniconda3/envs/testbed/lib/python3.10/site-packages (from pandas>=0.25->xarray==0.16.1.dev92+g757bee64.d20250207) (2023.3)
Requirement already satisfied: six>=1.5 in /opt/miniconda3/envs/testbed/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas>=0.25->xarray==0.16.1.dev92+g757bee64.d20250207) (1.16.0)
Installing collected packages: xarray
  Attempting uninstall: xarray
    Found existing installation: xarray 0.16.1.dev92+g757bee64
    Uninstalling xarray-0.16.1.dev92+g757bee64:
      Successfully uninstalled xarray-0.16.1.dev92+g757bee64
  DEPRECATION: Legacy editable install of xarray==0.16.1.dev92+g757bee64.d20250207 from file:///testbed (setup.py develop) is deprecated. pip 25.0 will enforce this behaviour change. A possible replacement is to add a pyproject.toml or enable --use-pep517, and use setuptools >= 64. If the resulting installation is not behaving as expected, try using --config-settings editable_mode=compat. Please consult the setuptools documentation for more information. Discussion can be found at https://github.com/pypa/pip/issues/11457
  Running setup.py develop for xarray
Successfully installed xarray
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.
============================= test session starts ==============================
platform linux -- Python 3.10.14, pytest-7.4.0, pluggy-1.5.0
rootdir: /testbed
configfile: setup.cfg
plugins: env-1.1.3, cov-5.0.0, hypothesis-6.108.5, xdist-3.6.1
collected 5 items

xarray/tests/test_concat.py .....                                        [100%]

=============================== warnings summary ===============================
xarray/__init__.py:1
  /testbed/xarray/__init__.py:1: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html
    import pkg_resources

../opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pkg_resources/__init__.py:2871
  /opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`.
  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
    declare_namespace(pkg)

../opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pkg_resources/__init__.py:2871
  /opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap.responses')`.
  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
    declare_namespace(pkg)

../opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pkg_resources/__init__.py:2350
../opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pkg_resources/__init__.py:2350
../opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pkg_resources/__init__.py:2350
  /opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pkg_resources/__init__.py:2350: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`.
  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
    declare_namespace(parent)

../opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pkg_resources/__init__.py:2871
  /opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap.handlers')`.
  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
    declare_namespace(pkg)

../opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pkg_resources/__init__.py:2871
  /opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap.tests')`.
  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
    declare_namespace(pkg)

xarray/core/dask_array_compat.py:61
xarray/core/dask_array_compat.py:61
  /testbed/xarray/core/dask_array_compat.py:61: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    if LooseVersion(dask_version) > LooseVersion("2.9.0"):

xarray/core/pdcompat.py:45
  /testbed/xarray/core/pdcompat.py:45: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    if LooseVersion(pd.__version__) < "0.25.0":

../opt/miniconda3/envs/testbed/lib/python3.10/site-packages/setuptools/_distutils/version.py:345
  /opt/miniconda3/envs/testbed/lib/python3.10/site-packages/setuptools/_distutils/version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    other = LooseVersion(other)

xarray/tests/__init__.py:58
xarray/tests/__init__.py:58
xarray/tests/__init__.py:58
xarray/tests/__init__.py:58
  /testbed/xarray/tests/__init__.py:58: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    return version.LooseVersion(vstring)

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
==================================== PASSES ====================================
=========================== short test summary info ============================
PASSED xarray/tests/test_concat.py::test_concat_datasets_along_existing_dim
PASSED xarray/tests/test_concat.py::test_concat_with_no_conflicts_compat
PASSED xarray/tests/test_concat.py::test_concat_with_override_combine_attrs
PASSED xarray/tests/test_concat.py::test_concat_with_identical_combine_attrs
PASSED xarray/tests/test_concat.py::test_concat_with_drop_combine_attrs
======================== 5 passed, 16 warnings in 3.67s ========================

