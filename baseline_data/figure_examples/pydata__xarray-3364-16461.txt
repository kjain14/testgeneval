Instance ID: pydata__xarray-3364-16461

Baseline 1 (Pynguin):
Predicted Test Suite: # Test cases automatically generated by Pynguin (https://www.pynguin.eu).
# Please check them before you use them.
import pytest
import xarray.core.concat as module_0
import numexpr.utils as module_1
import tokenize as module_2


def test_case_0():
    list_0 = []
    with pytest.raises(ValueError):
        module_0.concat(list_0, list_0, coords=list_0, positions=list_0, join=list_0)


def test_case_1():
    var_0 = module_1.get_vml_version()
    with pytest.raises(TypeError):
        module_0.concat(var_0, var_0, compat=var_0, join=var_0)


def test_case_2():
    var_0 = module_2.any()
    with pytest.raises(ValueError):
        module_0.concat(var_0, var_0, compat=var_0)


def test_case_3():
    int_0 = -1258
    list_0 = [int_0]
    with pytest.raises(TypeError):
        module_0.concat(list_0, int_0, coords=int_0, fill_value=int_0)

Coverage: 13.068181818181818
Mutation Score: 3.489999999999995

Baseline 2 (CodaMosa):
Predicted Test Suite: import xarray.core.concat as module_0

def test_case_1():
    try:
        float_0 = 4666.609361
        bool_0 = None
        var_0 = module_0.concat(float_0, bool_0)
    except BaseException:
        pass


def test_case_2():
    try:
        str_0 = '_group'
        dict_0 = {}
        list_0 = [dict_0, dict_0, str_0, str_0]
        tuple_0 = None
        str_1 = "u4kijQ/iKoy h0U'"
        bytes_0 = b'\x0f\xec\xf9\xc8S\xafI\xed'
        bool_0 = True
        var_0 = module_0.concat(list_0, tuple_0, str_1, bytes_0, str_0, bool_0)
    except BaseException:
        pass


def test_case_3():
    try:
        str_0 = 'X3v?[J@]{t\x0bW:BT~'
        var_0 = module_0.concat(str_0, str_0, str_0)
    except BaseException:
        pass


def test_case_4():
    try:
        tuple_0 = ()
        int_0 = -666
        var_0 = module_0.concat(tuple_0, int_0)
    except BaseException:
        pass# Automatically generated by Pynguin.

def test_case_5():
    pass

Coverage: 13.068181818181818
Mutation Score: 0.0

Baseline 3 (GPT-4o):
Predicted Test Suite: 
import pytest
import pandas as pd
import numpy as np
from xarray import DataArray, Dataset, Variable, concat

def create_test_datasets():
    ds1 = Dataset(
        {
            "temperature": ("x", [15, 20, 25]),
            "humidity": ("x", [30, 45, 60]),
        },
        coords={"x": [0, 1, 2]},
    )
    ds2 = Dataset(
        {
            "temperature": ("x", [18, 22, 28]),
            "humidity": ("x", [35, 50, 65]),
        },
        coords={"x": [3, 4, 5]},
    )
    return ds1, ds2

def create_test_dataarrays():
    da1 = DataArray([1, 2, 3], dims="x", coords={"x": [0, 1, 2]}, name="data")
    da2 = DataArray([4, 5, 6], dims="x", coords={"x": [3, 4, 5]}, name="data")
    return da1, da2

def test_concat_datasets():
    ds1, ds2 = create_test_datasets()
    result = concat([ds1, ds2], dim="x")
    expected_temperature = [15, 20, 25, 18, 22, 28]
    expected_humidity = [30, 45, 60, 35, 50, 65]
    assert list(result["temperature"].values) == expected_temperature
    assert list(result["humidity"].values) == expected_humidity

def test_concat_dataarrays():
    da1, da2 = create_test_dataarrays()
    result = concat([da1, da2], dim="x")
    expected_data = [1, 2, 3, 4, 5, 6]
    assert list(result.values) == expected_data

def test_concat_with_new_dimension():
    ds1, ds2 = create_test_datasets()
    result = concat([ds1, ds2], dim="new_dim")
    assert "new_dim" in result.dims
    assert result.dims["new_dim"] == 2

def test_concat_with_fill_value():
    ds1, ds2 = create_test_datasets()
    ds2 = ds2.drop_vars("humidity")
    result = concat([ds1, ds2], dim="x", fill_value=-999)
    expected_humidity = [30, 45, 60, -999, -999, -999]
    assert list(result["humidity"].values) == expected_humidity

def test_concat_with_different_coords():
    ds1, ds2 = create_test_datasets()
    ds2.coords["x"] = [4, 5, 6]
    with pytest.raises(ValueError):
        concat([ds1, ds2], dim="x", join="exact")

def test_concat_with_override_compat():
    ds1, ds2 = create_test_datasets()
    ds2["temperature"] = ds2["temperature"] + 1
    result = concat([ds1, ds2], dim="x", compat="override")
    expected_temperature = [15, 20, 25, 19, 23, 29]
    assert list(result["temperature"].values) == expected_temperature

def test_concat_dataarray_with_different_names():
    da1 = DataArray([1, 2, 3], dims="x", name="data1")
    da2 = DataArray([4, 5, 6], dims="x", name="data2")
    with pytest.raises(ValueError):
        concat([da1, da2], dim="x", compat="identical")

def test_concat_with_positions():
    ds1, ds2 = create_test_datasets()
    result = concat([ds1, ds2], dim="x", positions=[[0, 2, 4], [1, 3, 5]])
    expected_temperature = [15, 18, 20, 22, 25, 28]
    assert list(result["temperature"].values) == expected_temperature

def test_concat_with_minimal_data_vars():
    ds1, ds2 = create_test_datasets()
    ds2 = ds2.drop_vars("humidity")
    result = concat([ds1, ds2], dim="x", data_vars="minimal")
    assert "humidity" in result

def test_concat_with_all_coords():
    ds1, ds2 = create_test_datasets()
    result = concat([ds1, ds2], dim="x", coords="all")
    assert "x" in result.coords

Coverage: 72.72727272727273
Mutation Score: 42.2

Approach:
Predicted Test Suite: import pytest
import pandas as pd
import numpy as np
from xarray import DataArray, Dataset, Variable, concat

def create_test_datasets():
    ds1 = Dataset(
        {
            "temperature": ("x", [15, 20, 25]),
            "humidity": ("x", [30, 45, 60]),
        },
        coords={"x": [0, 1, 2]},
    )
    ds2 = Dataset(
        {
            "temperature": ("x", [18, 22, 28]),
            "humidity": ("x", [35, 50, 65]),
        },
        coords={"x": [3, 4, 5]},
    )
    return ds1, ds2

def create_test_dataarrays():
    da1 = DataArray([1, 2, 3], dims="x", coords={"x": [0, 1, 2]}, name="data")
    da2 = DataArray([4, 5, 6], dims="x", coords={"x": [3, 4, 5]}, name="data")
    return da1, da2

def test_concat_datasets():
    ds1, ds2 = create_test_datasets()
    result = concat([ds1, ds2], dim="x")
    expected_temperature = [15, 20, 25, 18, 22, 28]
    expected_humidity = [30, 45, 60, 35, 50, 65]
    assert list(result["temperature"].values) == expected_temperature
    assert list(result["humidity"].values) == expected_humidity

def test_concat_dataarrays():
    da1, da2 = create_test_dataarrays()
    result = concat([da1, da2], dim="x")
    expected_data = [1, 2, 3, 4, 5, 6]
    assert list(result.values) == expected_data

def test_concat_with_new_dimension():
    ds1, ds2 = create_test_datasets()
    result = concat([ds1, ds2], dim="new_dim")
    assert "new_dim" in result.dims
    assert result.dims["new_dim"] == 2

def test_concat_with_fill_value():
    ds1, ds2 = create_test_datasets()
    ds2 = ds2.drop_vars("humidity")
    result = concat([ds1, ds2], dim="x", fill_value=-999)
    expected_humidity = [30, 45, 60, -999, -999, -999]
    assert list(result["humidity"].values) == expected_humidity

def test_concat_with_different_coords():
    ds1, ds2 = create_test_datasets()
    ds2.coords["x"] = [4, 5, 6]
    with pytest.raises(ValueError):
        concat([ds1, ds2], dim="x", join="exact")

def test_concat_with_override_compat():
    ds1, ds2 = create_test_datasets()
    ds2["temperature"] = ds2["temperature"] + 1
    result = concat([ds1, ds2], dim="x", compat="override")
    expected_temperature = [15, 20, 25, 19, 23, 29]
    assert list(result["temperature"].values) == expected_temperature

def test_concat_dataarray_with_different_names():
    da1 = DataArray([1, 2, 3], dims="x", name="data1")
    da2 = DataArray([4, 5, 6], dims="x", name="data2")
    with pytest.raises(ValueError):
        concat([da1, da2], dim="x", compat="identical")

def test_concat_with_positions():
    ds1, ds2 = create_test_datasets()
    result = concat([ds1, ds2], dim="x", positions=[[0, 2, 4], [1, 3, 5]])
    expected_temperature = [15, 18, 20, 22, 25, 28]
    assert list(result["temperature"].values) == expected_temperature

def test_concat_with_minimal_data_vars():
    ds1, ds2 = create_test_datasets()
    ds2 = ds2.drop_vars("humidity")
    result = concat([ds1, ds2], dim="x", data_vars="minimal")
    assert "humidity" in result

def test_concat_with_all_coords():
    ds1, ds2 = create_test_datasets()
    result = concat([ds1, ds2], dim="x", coords="all")
    assert "x" in result.coords
Coverage: 72.72727272727273
Mutation Score: 42.2
Output: On branch main
Changes not staged for commit:
  (use "git add/rm <file>..." to update what will be committed)
  (use "git restore <file>..." to discard changes in working directory)
        deleted:    .coveragerc

Untracked files:
  (use "git add <file>..." to include in what will be committed)
        xarray/tests/test_concat.py

no changes added to commit (use "git add" and/or "git commit -a")
commit 7a6284261a59c53aea59ff112a6f672085af3d9b
Author: TestGenEval <>
Date:   Wed Dec 11 12:09:24 2024 +0000

    Testing fixes

diff --git a/xarray/core/concat.py b/xarray/core/concat.py
index ecae2566..e98e8a72 100644
--- a/xarray/core/concat.py
+++ b/xarray/core/concat.py
@@ -312,15 +312,9 @@ def _dataset_concat(
         to_merge = {var: [] for var in variables_to_merge}

         for ds in datasets:
-            absent_merge_vars = variables_to_merge - set(ds.variables)
-            if absent_merge_vars:
-                raise ValueError(
-                    "variables %r are present in some datasets but not others. "
-                    % absent_merge_vars
-                )
-
             for var in variables_to_merge:
-                to_merge[var].append(ds.variables[var])
+                if var in ds:
+                    to_merge[var].append(ds.variables[var])

         for var in variables_to_merge:
             result_vars[var] = unique_variable(
diff --git a/xarray/tests/test_combine.py b/xarray/tests/test_combine.py
index 6bd11276..cd26e7fb 100644
--- a/xarray/tests/test_combine.py
+++ b/xarray/tests/test_combine.py
@@ -782,12 +782,11 @@ class TestAutoCombineOldAPI:
         actual = auto_combine(datasets, concat_dim="t")
         assert_identical(expected, actual)

-    def test_auto_combine_still_fails(self):
-        # concat can't handle new variables (yet):
-        # https://github.com/pydata/xarray/issues/508
+    def test_auto_combine_with_new_variables(self):
         datasets = [Dataset({"x": 0}, {"y": 0}), Dataset({"x": 1}, {"y": 1, "z": 1})]
-        with pytest.raises(ValueError):
-            auto_combine(datasets, "y")
+        actual = auto_combine(datasets, "y")
+        expected = Dataset({"x": ("y", [0, 1])}, {"y": [0, 1], "z": 1})
+        assert_identical(expected, actual)

     def test_auto_combine_no_concat(self):
         objs = [Dataset({"x": 0}), Dataset({"y": 1})]
diff --git a/xarray/tests/test_concat.py b/xarray/tests/test_concat.py
deleted file mode 100644
index c7af0fc9..00000000
--- a/xarray/tests/test_concat.py
+++ /dev/null
@@ -1,453 +0,0 @@
-from copy import deepcopy
-
-import numpy as np
-import pandas as pd
-import pytest
-
-from xarray import DataArray, Dataset, Variable, concat
-from xarray.core import dtypes, merge
-
-from . import (
-    InaccessibleArray,
-    assert_array_equal,
-    assert_equal,
-    assert_identical,
-    raises_regex,
-    requires_dask,
-)
-from .test_dataset import create_test_data
-
-
-def test_concat_compat():
-    ds1 = Dataset(
-        {
-            "has_x_y": (("y", "x"), [[1, 2]]),
-            "has_x": ("x", [1, 2]),
-            "no_x_y": ("z", [1, 2]),
-        },
-        coords={"x": [0, 1], "y": [0], "z": [-1, -2]},
-    )
-    ds2 = Dataset(
-        {
-            "has_x_y": (("y", "x"), [[3, 4]]),
-            "has_x": ("x", [1, 2]),
-            "no_x_y": (("q", "z"), [[1, 2]]),
-        },
-        coords={"x": [0, 1], "y": [1], "z": [-1, -2], "q": [0]},
-    )
-
-    result = concat([ds1, ds2], dim="y", data_vars="minimal", compat="broadcast_equals")
-    assert_equal(ds2.no_x_y, result.no_x_y.transpose())
-
-    for var in ["has_x", "no_x_y"]:
-        assert "y" not in result[var]
-
-    with raises_regex(ValueError, "coordinates in some datasets but not others"):
-        concat([ds1, ds2], dim="q")
-    with raises_regex(ValueError, "'q' is not present in all datasets"):
-        concat([ds2, ds1], dim="q")
-
-
-class TestConcatDataset:
-    @pytest.fixture
-    def data(self):
-        return create_test_data().drop_dims("dim3")
-
-    def rectify_dim_order(self, data, dataset):
-        # return a new dataset with all variable dimensions transposed into
-        # the order in which they are found in `data`
-        return Dataset(
-            {k: v.transpose(*data[k].dims) for k, v in dataset.data_vars.items()},
-            dataset.coords,
-            attrs=dataset.attrs,
-        )
-
-    @pytest.mark.parametrize("coords", ["different", "minimal"])
-    @pytest.mark.parametrize("dim", ["dim1", "dim2"])
-    def test_concat_simple(self, data, dim, coords):
-        datasets = [g for _, g in data.groupby(dim, squeeze=False)]
-        assert_identical(data, concat(datasets, dim, coords=coords))
-
-    def test_concat_2(self, data):
-        dim = "dim2"
-        datasets = [g for _, g in data.groupby(dim, squeeze=True)]
-        concat_over = [k for k, v in data.coords.items() if dim in v.dims and k != dim]
-        actual = concat(datasets, data[dim], coords=concat_over)
-        assert_identical(data, self.rectify_dim_order(data, actual))
-
-    @pytest.mark.parametrize("coords", ["different", "minimal", "all"])
-    @pytest.mark.parametrize("dim", ["dim1", "dim2"])
-    def test_concat_coords_kwarg(self, data, dim, coords):
-        data = data.copy(deep=True)
-        # make sure the coords argument behaves as expected
-        data.coords["extra"] = ("dim4", np.arange(3))
-        datasets = [g for _, g in data.groupby(dim, squeeze=True)]
-
-        actual = concat(datasets, data[dim], coords=coords)
-        if coords == "all":
-            expected = np.array([data["extra"].values for _ in range(data.dims[dim])])
-            assert_array_equal(actual["extra"].values, expected)
-
-        else:
-            assert_equal(data["extra"], actual["extra"])
-
-    def test_concat(self, data):
-        split_data = [
-            data.isel(dim1=slice(3)),
-            data.isel(dim1=3),
-            data.isel(dim1=slice(4, None)),
-        ]
-        assert_identical(data, concat(split_data, "dim1"))
-
-    def test_concat_dim_precedence(self, data):
-        # verify that the dim argument takes precedence over
-        # concatenating dataset variables of the same name
-        dim = (2 * data["dim1"]).rename("dim1")
-        datasets = [g for _, g in data.groupby("dim1", squeeze=False)]
-        expected = data.copy()
-        expected["dim1"] = dim
-        assert_identical(expected, concat(datasets, dim))
-
-    def test_concat_data_vars(self):
-        data = Dataset({"foo": ("x", np.random.randn(10))})
-        objs = [data.isel(x=slice(5)), data.isel(x=slice(5, None))]
-        for data_vars in ["minimal", "different", "all", [], ["foo"]]:
-            actual = concat(objs, dim="x", data_vars=data_vars)
-            assert_identical(data, actual)
-
-    def test_concat_coords(self):
-        data = Dataset({"foo": ("x", np.random.randn(10))})
-        expected = data.assign_coords(c=("x", [0] * 5 + [1] * 5))
-        objs = [
-            data.isel(x=slice(5)).assign_coords(c=0),
-            data.isel(x=slice(5, None)).assign_coords(c=1),
-        ]
-        for coords in ["different", "all", ["c"]]:
-            actual = concat(objs, dim="x", coords=coords)
-            assert_identical(expected, actual)
-        for coords in ["minimal", []]:
-            with raises_regex(merge.MergeError, "conflicting values"):
-                concat(objs, dim="x", coords=coords)
-
-    def test_concat_constant_index(self):
-        # GH425
-        ds1 = Dataset({"foo": 1.5}, {"y": 1})
-        ds2 = Dataset({"foo": 2.5}, {"y": 1})
-        expected = Dataset({"foo": ("y", [1.5, 2.5]), "y": [1, 1]})
-        for mode in ["different", "all", ["foo"]]:
-            actual = concat([ds1, ds2], "y", data_vars=mode)
-            assert_identical(expected, actual)
-        with raises_regex(merge.MergeError, "conflicting values"):
-            # previously dim="y", and raised error which makes no sense.
-            # "foo" has dimension "y" so minimal should concatenate it?
-            concat([ds1, ds2], "new_dim", data_vars="minimal")
-
-    def test_concat_size0(self):
-        data = create_test_data()
-        split_data = [data.isel(dim1=slice(0, 0)), data]
-        actual = concat(split_data, "dim1")
-        assert_identical(data, actual)
-
-        actual = concat(split_data[::-1], "dim1")
-        assert_identical(data, actual)
-
-    def test_concat_autoalign(self):
-        ds1 = Dataset({"foo": DataArray([1, 2], coords=[("x", [1, 2])])})
-        ds2 = Dataset({"foo": DataArray([1, 2], coords=[("x", [1, 3])])})
-        actual = concat([ds1, ds2], "y")
-        expected = Dataset(
-            {
-                "foo": DataArray(
-                    [[1, 2, np.nan], [1, np.nan, 2]],
-                    dims=["y", "x"],
-                    coords={"x": [1, 2, 3]},
-                )
-            }
-        )
-        assert_identical(expected, actual)
-
-    def test_concat_errors(self):
-        data = create_test_data()
-        split_data = [data.isel(dim1=slice(3)), data.isel(dim1=slice(3, None))]
-
-        with raises_regex(ValueError, "must supply at least one"):
-            concat([], "dim1")
-
-        with raises_regex(ValueError, "Cannot specify both .*='different'"):
-            concat(
-                [data, data], dim="concat_dim", data_vars="different", compat="override"
-            )
-
-        with raises_regex(ValueError, "must supply at least one"):
-            concat([], "dim1")
-
-        with raises_regex(ValueError, "are not coordinates"):
-            concat([data, data], "new_dim", coords=["not_found"])
-
-        with raises_regex(ValueError, "global attributes not"):
-            data0, data1 = deepcopy(split_data)
-            data1.attrs["foo"] = "bar"
-            concat([data0, data1], "dim1", compat="identical")
-        assert_identical(data, concat([data0, data1], "dim1", compat="equals"))
-
-        with raises_regex(ValueError, "present in some datasets"):
-            data0, data1 = deepcopy(split_data)
-            data1["foo"] = ("bar", np.random.randn(10))
-            concat([data0, data1], "dim1")
-
-        with raises_regex(ValueError, "compat.* invalid"):
-            concat(split_data, "dim1", compat="foobar")
-
-        with raises_regex(ValueError, "unexpected value for"):
-            concat([data, data], "new_dim", coords="foobar")
-
-        with raises_regex(ValueError, "coordinate in some datasets but not others"):
-            concat([Dataset({"x": 0}), Dataset({"x": [1]})], dim="z")
-
-        with raises_regex(ValueError, "coordinate in some datasets but not others"):
-            concat([Dataset({"x": 0}), Dataset({}, {"x": 1})], dim="z")
-
-    def test_concat_join_kwarg(self):
-        ds1 = Dataset({"a": (("x", "y"), [[0]])}, coords={"x": [0], "y": [0]})
-        ds2 = Dataset({"a": (("x", "y"), [[0]])}, coords={"x": [1], "y": [0.0001]})
-
-        expected = {}
-        expected["outer"] = Dataset(
-            {"a": (("x", "y"), [[0, np.nan], [np.nan, 0]])},
-            {"x": [0, 1], "y": [0, 0.0001]},
-        )
-        expected["inner"] = Dataset(
-            {"a": (("x", "y"), [[], []])}, {"x": [0, 1], "y": []}
-        )
-        expected["left"] = Dataset(
-            {"a": (("x", "y"), np.array([0, np.nan], ndmin=2).T)},
-            coords={"x": [0, 1], "y": [0]},
-        )
-        expected["right"] = Dataset(
-            {"a": (("x", "y"), np.array([np.nan, 0], ndmin=2).T)},
-            coords={"x": [0, 1], "y": [0.0001]},
-        )
-        expected["override"] = Dataset(
-            {"a": (("x", "y"), np.array([0, 0], ndmin=2).T)},
-            coords={"x": [0, 1], "y": [0]},
-        )
-
-        with raises_regex(ValueError, "indexes along dimension 'y'"):
-            actual = concat([ds1, ds2], join="exact", dim="x")
-
-        for join in expected:
-            actual = concat([ds1, ds2], join=join, dim="x")
-            assert_equal(actual, expected[join])
-
-    def test_concat_promote_shape(self):
-        # mixed dims within variables
-        objs = [Dataset({}, {"x": 0}), Dataset({"x": [1]})]
-        actual = concat(objs, "x")
-        expected = Dataset({"x": [0, 1]})
-        assert_identical(actual, expected)
-
-        objs = [Dataset({"x": [0]}), Dataset({}, {"x": 1})]
-        actual = concat(objs, "x")
-        assert_identical(actual, expected)
-
-        # mixed dims between variables
-        objs = [Dataset({"x": [2], "y": 3}), Dataset({"x": [4], "y": 5})]
-        actual = concat(objs, "x")
-        expected = Dataset({"x": [2, 4], "y": ("x", [3, 5])})
-        assert_identical(actual, expected)
-
-        # mixed dims in coord variable
-        objs = [Dataset({"x": [0]}, {"y": -1}), Dataset({"x": [1]}, {"y": ("x", [-2])})]
-        actual = concat(objs, "x")
-        expected = Dataset({"x": [0, 1]}, {"y": ("x", [-1, -2])})
-        assert_identical(actual, expected)
-
-        # scalars with mixed lengths along concat dim -- values should repeat
-        objs = [Dataset({"x": [0]}, {"y": -1}), Dataset({"x": [1, 2]}, {"y": -2})]
-        actual = concat(objs, "x")
-        expected = Dataset({"x": [0, 1, 2]}, {"y": ("x", [-1, -2, -2])})
-        assert_identical(actual, expected)
-
-        # broadcast 1d x 1d -> 2d
-        objs = [
-            Dataset({"z": ("x", [-1])}, {"x": [0], "y": [0]}),
-            Dataset({"z": ("y", [1])}, {"x": [1], "y": [0]}),
-        ]
-        actual = concat(objs, "x")
-        expected = Dataset({"z": (("x", "y"), [[-1], [1]])}, {"x": [0, 1], "y": [0]})
-        assert_identical(actual, expected)
-
-    def test_concat_do_not_promote(self):
-        # GH438
-        objs = [
-            Dataset({"y": ("t", [1])}, {"x": 1, "t": [0]}),
-            Dataset({"y": ("t", [2])}, {"x": 1, "t": [0]}),
-        ]
-        expected = Dataset({"y": ("t", [1, 2])}, {"x": 1, "t": [0, 0]})
-        actual = concat(objs, "t")
-        assert_identical(expected, actual)
-
-        objs = [
-            Dataset({"y": ("t", [1])}, {"x": 1, "t": [0]}),
-            Dataset({"y": ("t", [2])}, {"x": 2, "t": [0]}),
-        ]
-        with pytest.raises(ValueError):
-            concat(objs, "t", coords="minimal")
-
-    def test_concat_dim_is_variable(self):
-        objs = [Dataset({"x": 0}), Dataset({"x": 1})]
-        coord = Variable("y", [3, 4])
-        expected = Dataset({"x": ("y", [0, 1]), "y": [3, 4]})
-        actual = concat(objs, coord)
-        assert_identical(actual, expected)
-
-    def test_concat_multiindex(self):
-        x = pd.MultiIndex.from_product([[1, 2, 3], ["a", "b"]])
-        expected = Dataset({"x": x})
-        actual = concat(
-            [expected.isel(x=slice(2)), expected.isel(x=slice(2, None))], "x"
-        )
-        assert expected.equals(actual)
-        assert isinstance(actual.x.to_index(), pd.MultiIndex)
-
-    @pytest.mark.parametrize("fill_value", [dtypes.NA, 2, 2.0])
-    def test_concat_fill_value(self, fill_value):
-        datasets = [
-            Dataset({"a": ("x", [2, 3]), "x": [1, 2]}),
-            Dataset({"a": ("x", [1, 2]), "x": [0, 1]}),
-        ]
-        if fill_value == dtypes.NA:
-            # if we supply the default, we expect the missing value for a
-            # float array
-            fill_value = np.nan
-        expected = Dataset(
-            {"a": (("t", "x"), [[fill_value, 2, 3], [1, 2, fill_value]])},
-            {"x": [0, 1, 2]},
-        )
-        actual = concat(datasets, dim="t", fill_value=fill_value)
-        assert_identical(actual, expected)
-
-
-class TestConcatDataArray:
-    def test_concat(self):
-        ds = Dataset(
-            {
-                "foo": (["x", "y"], np.random.random((2, 3))),
-                "bar": (["x", "y"], np.random.random((2, 3))),
-            },
-            {"x": [0, 1]},
-        )
-        foo = ds["foo"]
-        bar = ds["bar"]
-
-        # from dataset array:
-        expected = DataArray(
-            np.array([foo.values, bar.values]),
-            dims=["w", "x", "y"],
-            coords={"x": [0, 1]},
-        )
-        actual = concat([foo, bar], "w")
-        assert_equal(expected, actual)
-        # from iteration:
-        grouped = [g for _, g in foo.groupby("x")]
-        stacked = concat(grouped, ds["x"])
-        assert_identical(foo, stacked)
-        # with an index as the 'dim' argument
-        stacked = concat(grouped, ds.indexes["x"])
-        assert_identical(foo, stacked)
-
-        actual = concat([foo[0], foo[1]], pd.Index([0, 1])).reset_coords(drop=True)
-        expected = foo[:2].rename({"x": "concat_dim"})
-        assert_identical(expected, actual)
-
-        actual = concat([foo[0], foo[1]], [0, 1]).reset_coords(drop=True)
-        expected = foo[:2].rename({"x": "concat_dim"})
-        assert_identical(expected, actual)
-
-        with raises_regex(ValueError, "not identical"):
-            concat([foo, bar], dim="w", compat="identical")
-
-        with raises_regex(ValueError, "not a valid argument"):
-            concat([foo, bar], dim="w", data_vars="minimal")
-
-    def test_concat_encoding(self):
-        # Regression test for GH1297
-        ds = Dataset(
-            {
-                "foo": (["x", "y"], np.random.random((2, 3))),
-                "bar": (["x", "y"], np.random.random((2, 3))),
-            },
-            {"x": [0, 1]},
-        )
-        foo = ds["foo"]
-        foo.encoding = {"complevel": 5}
-        ds.encoding = {"unlimited_dims": "x"}
-        assert concat([foo, foo], dim="x").encoding == foo.encoding
-        assert concat([ds, ds], dim="x").encoding == ds.encoding
-
-    @requires_dask
-    def test_concat_lazy(self):
-        import dask.array as da
-
-        arrays = [
-            DataArray(
-                da.from_array(InaccessibleArray(np.zeros((3, 3))), 3), dims=["x", "y"]
-            )
-            for _ in range(2)
-        ]
-        # should not raise
-        combined = concat(arrays, dim="z")
-        assert combined.shape == (2, 3, 3)
-        assert combined.dims == ("z", "x", "y")
-
-    @pytest.mark.parametrize("fill_value", [dtypes.NA, 2, 2.0])
-    def test_concat_fill_value(self, fill_value):
-        foo = DataArray([1, 2], coords=[("x", [1, 2])])
-        bar = DataArray([1, 2], coords=[("x", [1, 3])])
-        if fill_value == dtypes.NA:
-            # if we supply the default, we expect the missing value for a
-            # float array
-            fill_value = np.nan
-        expected = DataArray(
-            [[1, 2, fill_value], [1, fill_value, 2]],
-            dims=["y", "x"],
-            coords={"x": [1, 2, 3]},
-        )
-        actual = concat((foo, bar), dim="y", fill_value=fill_value)
-        assert_identical(actual, expected)
-
-    def test_concat_join_kwarg(self):
-        ds1 = Dataset(
-            {"a": (("x", "y"), [[0]])}, coords={"x": [0], "y": [0]}
-        ).to_array()
-        ds2 = Dataset(
-            {"a": (("x", "y"), [[0]])}, coords={"x": [1], "y": [0.0001]}
-        ).to_array()
-
-        expected = {}
-        expected["outer"] = Dataset(
-            {"a": (("x", "y"), [[0, np.nan], [np.nan, 0]])},
-            {"x": [0, 1], "y": [0, 0.0001]},
-        )
-        expected["inner"] = Dataset(
-            {"a": (("x", "y"), [[], []])}, {"x": [0, 1], "y": []}
-        )
-        expected["left"] = Dataset(
-            {"a": (("x", "y"), np.array([0, np.nan], ndmin=2).T)},
-            coords={"x": [0, 1], "y": [0]},
-        )
-        expected["right"] = Dataset(
-            {"a": (("x", "y"), np.array([np.nan, 0], ndmin=2).T)},
-            coords={"x": [0, 1], "y": [0.0001]},
-        )
-        expected["override"] = Dataset(
-            {"a": (("x", "y"), np.array([0, 0], ndmin=2).T)},
-            coords={"x": [0, 1], "y": [0]},
-        )
-
-        with raises_regex(ValueError, "indexes along dimension 'y'"):
-            actual = concat([ds1, ds2], join="exact", dim="x")
-
-        for join in expected:
-            actual = concat([ds1, ds2], join=join, dim="x")
-            assert_equal(actual, expected[join].to_array())
Obtaining file:///testbed
  Preparing metadata (setup.py): started
  Preparing metadata (setup.py): finished with status 'done'
Requirement already satisfied: numpy>=1.14 in /opt/miniconda3/envs/testbed/lib/python3.10/site-packages (from xarray==0.13.0+34.g7a628426.dirty) (1.23.0)
Requirement already satisfied: pandas>=0.24 in /opt/miniconda3/envs/testbed/lib/python3.10/site-packages (from xarray==0.13.0+34.g7a628426.dirty) (1.5.3)
Requirement already satisfied: python-dateutil>=2.8.1 in /opt/miniconda3/envs/testbed/lib/python3.10/site-packages (from pandas>=0.24->xarray==0.13.0+34.g7a628426.dirty) (2.8.2)
Requirement already satisfied: pytz>=2020.1 in /opt/miniconda3/envs/testbed/lib/python3.10/site-packages (from pandas>=0.24->xarray==0.13.0+34.g7a628426.dirty) (2023.3)
Requirement already satisfied: six>=1.5 in /opt/miniconda3/envs/testbed/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas>=0.24->xarray==0.13.0+34.g7a628426.dirty) (1.16.0)
Installing collected packages: xarray
  Attempting uninstall: xarray
    Found existing installation: xarray 0.13.0+34.g7a628426
    Uninstalling xarray-0.13.0+34.g7a628426:
      Successfully uninstalled xarray-0.13.0+34.g7a628426
  DEPRECATION: Legacy editable install of xarray==0.13.0+34.g7a628426.dirty from file:///testbed (setup.py develop) is deprecated. pip 25.0 will enforce this behaviour change. A possible replacement is to add a pyproject.toml or enable --use-pep517, and use setuptools >= 64. If the resulting installation is not behaving as expected, try using --config-settings editable_mode=compat. Please consult the setuptools documentation for more information. Discussion can be found at https://github.com/pypa/pip/issues/11457
  Running setup.py develop for xarray
Successfully installed xarray
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.
============================= test session starts ==============================
platform linux -- Python 3.10.14, pytest-7.4.0, pluggy-1.5.0
rootdir: /testbed
configfile: setup.cfg
plugins: env-1.1.3, cov-5.0.0, hypothesis-6.110.1, xdist-3.6.1
collected 6 items

xarray/tests/test_concat.py ......                                       [100%]

=============================== warnings summary ===============================
xarray/core/pdcompat.py:45
  /testbed/xarray/core/pdcompat.py:45: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    if LooseVersion(pd.__version__) < "0.25.0":

../opt/miniconda3/envs/testbed/lib/python3.10/site-packages/setuptools/_distutils/version.py:345
  /opt/miniconda3/envs/testbed/lib/python3.10/site-packages/setuptools/_distutils/version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    other = LooseVersion(other)

xarray/core/dask_array_compat.py:7
xarray/core/dask_array_compat.py:7
  /testbed/xarray/core/dask_array_compat.py:7: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    if LooseVersion(dask_version) >= LooseVersion("2.0.0"):

../opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pydap/lib.py:5
  /opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pydap/lib.py:5: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html
    from pkg_resources import get_distribution

../opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pkg_resources/__init__.py:2871
  /opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`.
  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
    declare_namespace(pkg)

../opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pkg_resources/__init__.py:2871
  /opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap.responses')`.
  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
    declare_namespace(pkg)

../opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pkg_resources/__init__.py:2350
../opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pkg_resources/__init__.py:2350
../opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pkg_resources/__init__.py:2350
  /opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pkg_resources/__init__.py:2350: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`.
  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
    declare_namespace(parent)

../opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pkg_resources/__init__.py:2871
  /opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap.handlers')`.
  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
    declare_namespace(pkg)

../opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pkg_resources/__init__.py:2871
  /opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap.tests')`.
  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
    declare_namespace(pkg)

xarray/tests/test_concat.py::test_concat_with_new_dimension
  /testbed/xarray/core/alignment.py:300: FutureWarning: Index.__or__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__or__.  Use index.union(other) instead.
    index = joiner(matching_indexes)

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
==================================== PASSES ====================================
=========================== short test summary info ============================
PASSED xarray/tests/test_concat.py::test_concat_datasets
PASSED xarray/tests/test_concat.py::test_concat_dataarrays
PASSED xarray/tests/test_concat.py::test_concat_with_new_dimension
PASSED xarray/tests/test_concat.py::test_concat_dataarray_with_different_names
PASSED xarray/tests/test_concat.py::test_concat_with_positions
PASSED xarray/tests/test_concat.py::test_concat_with_all_coords
======================== 6 passed, 13 warnings in 3.18s ========================

