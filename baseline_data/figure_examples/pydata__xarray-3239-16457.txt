Instance ID: pydata__xarray-3239-16457

Baseline 1 (Pynguin):
Predicted Test Suite: # Test cases automatically generated by Pynguin (https://www.pynguin.eu).
# Please check them before you use them.
import pytest
import xarray.core.merge as module_0
import numexpr.expressions as module_1
import bottleneck._version as module_2
import sysconfig as module_3
import urllib.request as module_4
import email.utils as module_5


def test_case_0():
    merge_error_0 = module_0.MergeError()


def test_case_1():
    list_0 = []
    ordered_dict_0 = module_0.broadcast_dimension_size(list_0)
    var_0 = module_0.coerce_pandas_values(list_0)


@pytest.mark.xfail(strict=True)
def test_case_2():
    none_type_0 = None
    module_0.merge_coords(none_type_0, none_type_0, none_type_0, fill_value=none_type_0)


def test_case_3():
    list_0 = []
    var_0 = module_0.merge(list_0)
    assert (
        f"{type(var_0).__module__}.{type(var_0).__qualname__}"
        == "xarray.core.dataset.Dataset"
    )
    assert len(var_0) == 0


def test_case_4():
    list_0 = []
    var_0 = module_0.merge(list_0)
    assert (
        f"{type(var_0).__module__}.{type(var_0).__qualname__}"
        == "xarray.core.dataset.Dataset"
    )
    assert len(var_0) == 0
    var_1 = module_0.dataset_update_method(var_0, list_0)


@pytest.mark.xfail(strict=True)
def test_case_5():
    str_0 = "k <$s"
    module_0.determine_coords(str_0)


@pytest.mark.xfail(strict=True)
def test_case_6():
    var_0 = module_1.get_optimization()
    module_0.coerce_pandas_values(var_0)


@pytest.mark.xfail(strict=True)
def test_case_7():
    var_0 = module_2.get_versions()
    var_1 = module_0.merge_data_and_coords(var_0, var_0)
    module_0.dataset_update_method(var_0, var_0)


@pytest.mark.xfail(strict=True)
def test_case_8():
    none_type_0 = None
    module_0.dataset_merge_method(
        none_type_0, none_type_0, none_type_0, none_type_0, none_type_0, none_type_0
    )


@pytest.mark.xfail(strict=True)
def test_case_9():
    list_0 = []
    module_0.dataset_update_method(list_0, list_0)


def test_case_10():
    none_type_0 = None
    ordered_default_dict_0 = module_0.OrderedDefaultDict(none_type_0)
    assert (
        f"{type(ordered_default_dict_0).__module__}.{type(ordered_default_dict_0).__qualname__}"
        == "xarray.core.merge.OrderedDefaultDict"
    )
    assert len(ordered_default_dict_0) == 0


@pytest.mark.xfail(strict=True)
def test_case_11():
    none_type_0 = None
    module_0.merge_coords_for_inplace_math(none_type_0)


@pytest.mark.xfail(strict=True)
def test_case_12():
    none_type_0 = None
    module_0.expand_and_merge_variables(none_type_0)


@pytest.mark.xfail(strict=True)
def test_case_13():
    none_type_0 = None
    module_0.merge_data_and_coords(none_type_0, none_type_0, join=none_type_0)


@pytest.mark.xfail(strict=True)
def test_case_14():
    list_0 = []
    var_0 = module_0.merge(list_0)
    assert (
        f"{type(var_0).__module__}.{type(var_0).__qualname__}"
        == "xarray.core.dataset.Dataset"
    )
    assert len(var_0) == 0
    var_1 = module_0.assert_valid_explicit_coords(var_0, list_0, list_0)
    module_0.dataset_merge_method(var_0, list_0, var_0, list_0, var_0, list_0)


@pytest.mark.xfail(strict=True)
def test_case_15():
    list_0 = []
    var_0 = module_0.merge(list_0)
    assert (
        f"{type(var_0).__module__}.{type(var_0).__qualname__}"
        == "xarray.core.dataset.Dataset"
    )
    assert len(var_0) == 0
    module_0.dataset_merge_method(var_0, list_0, var_0, list_0, var_0, list_0)


def test_case_16():
    list_0 = []
    var_0 = module_0.merge(list_0)
    assert (
        f"{type(var_0).__module__}.{type(var_0).__qualname__}"
        == "xarray.core.dataset.Dataset"
    )
    assert len(var_0) == 0
    var_1 = module_0.assert_valid_explicit_coords(var_0, var_0, var_0)
    var_2 = module_0.dataset_update_method(var_0, list_0)
    with pytest.raises(TypeError):
        module_0.merge(var_2)


@pytest.mark.xfail(strict=True)
def test_case_17():
    list_0 = module_3.get_makefile_filename()
    ordered_dict_0 = module_4.pathname2url(list_0)
    module_0.assert_valid_explicit_coords(
        ordered_dict_0, ordered_dict_0, ordered_dict_0
    )


@pytest.mark.xfail(strict=True)
def test_case_18():
    list_0 = []
    var_0 = module_0.merge(list_0)
    assert (
        f"{type(var_0).__module__}.{type(var_0).__qualname__}"
        == "xarray.core.dataset.Dataset"
    )
    assert len(var_0) == 0
    var_1 = var_0.__eq__(var_0)
    var_2 = module_0.dataset_update_method(var_0, var_1)
    module_0.dataset_merge_method(var_0, list_0, var_0, list_0, var_0, var_1)


@pytest.mark.xfail(strict=True)
def test_case_19():
    list_0 = []
    var_0 = module_0.merge(list_0)
    assert (
        f"{type(var_0).__module__}.{type(var_0).__qualname__}"
        == "xarray.core.dataset.Dataset"
    )
    assert len(var_0) == 0
    var_1 = var_0.__eq__(var_0)
    var_2 = module_0.dataset_update_method(var_0, var_1)
    none_type_0 = None
    module_0.dataset_merge_method(var_2, var_1, none_type_0, var_2, var_0, var_0)


@pytest.mark.xfail(strict=True)
def test_case_20():
    list_0 = []
    var_0 = module_0.merge(list_0)
    assert (
        f"{type(var_0).__module__}.{type(var_0).__qualname__}"
        == "xarray.core.dataset.Dataset"
    )
    assert len(var_0) == 0
    var_1 = module_0.extract_indexes(var_0)
    str_0 = "T"
    dict_0 = {str_0: var_0, str_0: list_0, str_0: str_0}
    module_0.dataset_merge_method(dict_0, str_0, dict_0, str_0, list_0, str_0)


@pytest.mark.xfail(strict=True)
def test_case_21():
    var_0 = module_2.get_versions()
    module_0.dataset_update_method(var_0, var_0)


@pytest.mark.xfail(strict=True)
def test_case_22():
    list_0 = []
    var_0 = module_5.make_msgid()
    module_0.dataset_merge_method(list_0, list_0, var_0, var_0, var_0, var_0)


def test_case_23():
    list_0 = []
    var_0 = module_0.merge(list_0)
    assert (
        f"{type(var_0).__module__}.{type(var_0).__qualname__}"
        == "xarray.core.dataset.Dataset"
    )
    assert len(var_0) == 0
    var_1 = module_0.merge_data_and_coords(var_0, var_0)
    var_2 = module_0.dataset_update_method(var_0, var_0)
    with pytest.raises(module_0.MergeError):
        module_0.unique_variable(list_0, var_2, var_1, var_0)


@pytest.mark.xfail(strict=True)
def test_case_24():
    list_0 = []
    var_0 = module_0.merge(list_0)
    assert (
        f"{type(var_0).__module__}.{type(var_0).__qualname__}"
        == "xarray.core.dataset.Dataset"
    )
    assert len(var_0) == 0
    var_1 = module_2.get_versions()
    var_2 = module_0.dataset_update_method(var_0, var_1)
    module_0.unique_variable(var_0, list_0)

Coverage: 79.74137931034483
Mutation Score: 28.569999999999993

Baseline 2 (CodaMosa):
Predicted Test Suite: import collections as module_1
import numpy as module_4
import pytest
import xarray.core.dataset as module_2
import xarray.core.merge as module_0
import xarray.core.variable as module_3

def test_case_1():
    pass


def test_case_2():
    list_0 = []
    ordered_dict_0 = module_0.merge_variables(list_0)
    assert ordered_dict_0 == {}
    assert module_0.TYPE_CHECKING is False
    assert len(module_0.PANDAS_TYPES) == 3


def test_case_3():
    ordered_dict_0 = module_1.OrderedDict()
    list_0 = [ordered_dict_0, ordered_dict_0, ordered_dict_0]
    var_0 = module_0.merge(list_0)
    assert ordered_dict_0 == {}
    assert len(var_0) == 0
    assert module_0.TYPE_CHECKING is False
    assert len(module_0.PANDAS_TYPES) == 3
    assert module_2.Dataset.load_store is not None
    assert module_2.Dataset.from_dataframe is not None
    assert module_2.Dataset.from_dict is not None


def test_case_4():
    list_0 = []
    var_0 = module_0.merge_coords(list_0)
    assert var_0 == {}
    assert module_0.TYPE_CHECKING is False
    assert len(module_0.PANDAS_TYPES) == 3


def test_case_5():
    list_0 = []
    var_0 = module_0.expand_and_merge_variables(list_0)
    assert var_0 == {}
    assert module_0.TYPE_CHECKING is False
    assert len(module_0.PANDAS_TYPES) == 3


def test_case_6():
    list_0 = []
    dict_0 = {}
    str_0 = 'oH87F3$Ol\\$&i-{_u'
    ordered_dict_0 = module_0.merge_variables(list_0, str_0)
    assert ordered_dict_0 == {}
    assert module_0.TYPE_CHECKING is False
    assert len(module_0.PANDAS_TYPES) == 3
    var_0 = module_0.merge_data_and_coords(ordered_dict_0, dict_0)
    assert var_0 == ({}, {*()}, {})


def test_case_7():
    str_0 = 'KhVS6^^UOE'
    list_0 = [str_0]
    variable_0 = module_3.Variable(list_0, list_0)
    list_1 = [variable_0, variable_0, variable_0, variable_0]
    ordered_dict_0 = module_0.broadcast_dimension_size(list_1)
    assert len(variable_0) == 1
    assert ordered_dict_0 == {'KhVS6^^UOE': 1}
    assert module_3.IS_NEP18_ACTIVE is True
    assert module_3.dask_array_type == ()
    assert len(module_3.integer_types) == 2
    assert len(module_3.NON_NUMPY_SUPPORTED_ARRAY_TYPES) == 2
    assert len(module_3.BASIC_INDEXING_TYPES) == 3
    assert module_0.TYPE_CHECKING is False
    assert len(module_0.PANDAS_TYPES) == 3
    assert module_3.Variable.concat is not None
    ndarray_0 = variable_0.__array__()
    assert len(ndarray_0) == 1
    dict_0 = {str_0: ndarray_0, str_0: str_0, str_0: variable_0}
    var_0 = module_0.merge_data_and_coords(ordered_dict_0, dict_0)
    assert len(var_0) == 3
    assert module_4.newaxis is None
    assert module_4.little_endian is True
    assert module_4.Inf == pytest.approx(1e309, abs=0.01, rel=0.01)
    assert module_4.inf == pytest.approx(1e309, abs=0.01, rel=0.01)
    assert module_4.infty == pytest.approx(1e309, abs=0.01, rel=0.01)
    assert module_4.Infinity == pytest.approx(1e309, abs=0.01, rel=0.01)
    assert module_4.CLIP == 0
    assert module_4.RAISE == 2
    assert module_4.WRAP == 1
    assert module_4.MAXDIMS == 32
    assert module_4.BUFSIZE == 8192
    assert module_4.ALLOW_THREADS == 1
    assert module_4.MAY_SHARE_BOUNDS == 0
    assert module_4.MAY_SHARE_EXACT == -1
    assert module_4.ERR_CALL == 3
    assert module_4.ERR_DEFAULT == 521
    assert module_4.ERR_IGNORE == 0
    assert module_4.ERR_LOG == 5
    assert module_4.ERR_PRINT == 4
    assert module_4.ERR_RAISE == 2
    assert module_4.ERR_WARN == 1
    assert module_4.FLOATING_POINT_SUPPORT == 1
    assert module_4.FPE_DIVIDEBYZERO == 1
    assert module_4.FPE_INVALID == 8
    assert module_4.FPE_OVERFLOW == 2
    assert module_4.FPE_UNDERFLOW == 4
    assert module_4.NINF == pytest.approx(-1e309, abs=0.01, rel=0.01)
    assert module_4.NZERO == pytest.approx(-0.0, abs=0.01, rel=0.01)
    assert module_4.PINF == pytest.approx(1e309, abs=0.01, rel=0.01)
    assert module_4.PZERO == pytest.approx(0.0, abs=0.01, rel=0.01)
    assert module_4.SHIFT_DIVIDEBYZERO == 0
    assert module_4.SHIFT_INVALID == 9
    assert module_4.SHIFT_OVERFLOW == 3
    assert module_4.SHIFT_UNDERFLOW == 6
    assert module_4.UFUNC_BUFSIZE_DEFAULT == 8192
    assert module_4.UFUNC_PYVALS_NAME == 'UFUNC_PYVALS'
    assert module_4.e == pytest.approx(2.718281828459045, abs=0.01, rel=0.01)
    assert module_4.euler_gamma == pytest.approx(0.5772156649015329, abs=0.01, rel=0.01)
    assert module_4.pi == pytest.approx(3.141592653589793, abs=0.01, rel=0.01)
    assert len(module_4.sctypeDict) == 136
    assert len(module_4.sctypes) == 5
    assert len(module_4.ScalarType) == 31
    assert len(module_4.cast) == 24
    assert len(module_4.nbytes) == 24
    assert module_4.typecodes == {'Character': 'c', 'Integer': 'bhilqp', 'UnsignedInteger': 'BHILQP', 'Float': 'efdg', 'Complex': 'FDG', 'AllInteger': 'bBhHiIlLqQpP', 'AllFloat': 'efdgFDG', 'Datetime': 'Mm', 'All': '?bhilqpBHILQPefdgFDGSUVOMm'}
    assert module_4.tracemalloc_domain == 389047
    assert module_4.mgrid.sparse is False
    assert module_4.ogrid.sparse is True
    assert len(module_4.r_) == 0
    assert len(module_4.c_) == 0
    assert module_4.s_.maketuple is False
    assert module_4.index_exp.maketuple is True
    assert module_4.oldnumeric == 'removed'
    assert module_4.numarray == 'removed'
    assert module_4.use_hugepage == 1
    assert module_4.kernel_version == (6, 8)


def test_case_8():
    list_0 = []
    tuple_0 = module_0.merge_core(list_0)
    assert tuple_0 == ({}, {*()}, {})
    assert module_0.TYPE_CHECKING is False
    assert len(module_0.PANDAS_TYPES) == 3


def test_case_9():
    str_0 = '8&'
    dict_0 = {str_0: str_0, str_0: str_0, str_0: str_0, str_0: str_0}
    tuple_0 = (dict_0,)
    var_0 = module_0.coerce_pandas_values(tuple_0)
    assert var_0 == [{'8&': '8&'}]
    assert module_0.TYPE_CHECKING is False
    assert len(module_0.PANDAS_TYPES) == 3


def test_case_10():
    bytes_0 = b'\xcb/F\xb4\xfd\xb8DV\xaa=\xcc'
    list_0 = [bytes_0, bytes_0, bytes_0]
    str_0 = ' k-]\ncjIQFCHP~k7'
    float_0 = 0.001
    list_1 = [list_0, bytes_0, list_0, float_0]
    var_0 = module_0.unique_variable(list_0, str_0, list_1, list_1)
    assert var_0 == ' '
    assert module_0.TYPE_CHECKING is False
    assert len(module_0.PANDAS_TYPES) == 3


def test_case_11():
    str_0 = '7'
    str_1 = (str_0,)
    int_0 = 5
    int_1 = [int_0, int_0, int_0]
    variable_0 = module_3.Variable(str_1, int_1)
    str_2 = 'a'
    variable_1 = {str_2: variable_0}
    variable_2 = {str_2: variable_0}
    variable_3 = [variable_1, variable_2]
    str_3 = 'override'
    ordered_dict_0 = module_0.merge_variables(variable_3, str_3)
    assert len(variable_0) == 3
    assert len(ordered_dict_0) == 1
    assert module_3.IS_NEP18_ACTIVE is True
    assert module_3.dask_array_type == ()
    assert len(module_3.integer_types) == 2
    assert len(module_3.NON_NUMPY_SUPPORTED_ARRAY_TYPES) == 2
    assert len(module_3.BASIC_INDEXING_TYPES) == 3
    assert module_0.TYPE_CHECKING is False
    assert len(module_0.PANDAS_TYPES) == 3
    assert module_3.Variable.concat is not None
    var_0 = ordered_dict_0[str_2]


def test_case_12():
    str_0 = 'x'
    str_1 = (str_0,)
    int_0 = 1
    int_1 = 2
    int_2 = 3
    int_3 = [int_0, int_1, int_2]
    variable_0 = module_3.Variable(str_1, int_3)
    str_2 = (str_0,)
    int_4 = [int_0, int_1, int_2]
    variable_1 = module_3.Variable(str_2, int_4)
    str_3 = (str_0,)
    int_5 = 4
    int_6 = [int_0, int_1, int_5]
    variable_2 = module_3.Variable(str_3, int_6)
    str_4 = 'test'
    variable_3 = [variable_0, variable_1]
    str_5 = 'identical'
    var_0 = module_0.unique_variable(str_4, variable_3, str_5)
    assert len(variable_0) == 3
    assert len(variable_1) == 3
    assert len(variable_2) == 3
    assert len(var_0) == 3
    assert module_3.IS_NEP18_ACTIVE is True
    assert module_3.dask_array_type == ()
    assert len(module_3.integer_types) == 2
    assert len(module_3.NON_NUMPY_SUPPORTED_ARRAY_TYPES) == 2
    assert len(module_3.BASIC_INDEXING_TYPES) == 3
    assert module_0.TYPE_CHECKING is False
    assert len(module_0.PANDAS_TYPES) == 3
    assert module_3.Variable.concat is not None
    variable_4 = [variable_0, variable_1]
    str_6 = 'broadcast_equals'
    var_1 = module_0.unique_variable(str_4, variable_4, str_6)
    assert len(var_1) == 3
    str_7 = (str_0,)
    var_2 = None
    var_3 = [int_0, int_1, var_2]
    variable_5 = module_3.Variable(str_7, var_3)
    assert len(variable_5) == 3
    variable_6 = [variable_0, variable_5]
    str_8 = 'no_conflicts'
    var_4 = module_0.unique_variable(str_4, variable_6, str_8)
    assert len(var_4) == 3
    variable_7 = [variable_0, variable_2]
    str_9 = 'override'
    var_5 = module_0.unique_variable(str_4, variable_7, str_9)
    assert len(var_5) == 3


def test_case_13():
    str_0 = None
    dict_0 = {str_0: str_0, str_0: str_0}
    list_0 = [dict_0, dict_0, dict_0]
    var_0 = module_0.determine_coords(list_0)
    assert var_0 == ({*()}, {*()})
    assert module_0.TYPE_CHECKING is False
    assert len(module_0.PANDAS_TYPES) == 3# Automatically generated by Pynguin.


def test_case_14():
    try:
        dict_0 = {}
        list_0 = [dict_0]
        ordered_dict_0 = module_0.broadcast_dimension_size(list_0)
    except BaseException:
        pass


def test_case_15():
    try:
        ordered_dict_0 = module_1.OrderedDict()
        list_0 = [ordered_dict_0, ordered_dict_0]
        var_0 = module_0.merge(list_0)
        assert ordered_dict_0 == {}
        assert len(var_0) == 0
        assert module_0.TYPE_CHECKING is False
        assert len(module_0.PANDAS_TYPES) == 3
        assert module_2.Dataset.load_store is not None
        assert module_2.Dataset.from_dataframe is not None
        assert module_2.Dataset.from_dict is not None
        str_0 = 'x>'
        dict_0 = {str_0: ordered_dict_0, str_0: str_0, str_0: str_0}
        list_1 = []
        ordered_dict_1 = module_0.broadcast_dimension_size(list_1)
        assert ordered_dict_1 == {}
        assert module_2.TYPE_CHECKING is False
        assert module_2.OPTIONS == {'display_width': 80, 'arithmetic_join': 'inner', 'enable_cftimeindex': True, 'file_cache_maxsize': 128, 'warn_for_unclosed_files': False, 'cmap_sequential': 'viridis', 'cmap_divergent': 'RdBu_r', 'keep_attrs': 'default'}
        assert module_2.dask_array_type == ()
        list_2 = [dict_0, list_0]
        var_1 = module_0.coerce_pandas_values(list_2)
    except BaseException:
        pass


def test_case_16():
    try:
        float_0 = 3247.84423
        tuple_0 = module_0.merge_core(float_0)
    except BaseException:
        pass


def test_case_17():
    try:
        merge_error_0 = module_0.MergeError()
        assert module_0.TYPE_CHECKING is False
        bytes_0 = b'\xb6\xa6\xe1\xc2'
        bool_0 = True
        dict_0 = {merge_error_0: bytes_0}
        var_0 = module_0.merge_data_and_coords(bool_0, dict_0, merge_error_0)
    except BaseException:
        pass


def test_case_18():
    try:
        merge_error_0 = module_0.MergeError()
        assert module_0.TYPE_CHECKING is False
        var_0 = module_0.merge_coords_for_inplace_math(merge_error_0)
    except BaseException:
        pass


def test_case_19():
    try:
        str_0 = '27'
        var_0 = module_0.merge_coords_for_inplace_math(str_0)
    except BaseException:
        pass


def test_case_20():
    try:
        bytes_0 = b'-\x9b:;\x01(>c\xcf'
        var_0 = module_0.determine_coords(bytes_0)
    except BaseException:
        pass


def test_case_21():
    try:
        str_0 = 'jz<t'
        var_0 = module_0.coerce_pandas_values(str_0)
    except BaseException:
        pass


def test_case_22():
    try:
        str_0 = 'GfoK[?@A\\'
        var_0 = module_0.expand_and_merge_variables(str_0)
    except BaseException:
        pass


def test_case_23():
    try:
        bytes_0 = b'\xf2\xeb\xfaS.\x7f\xcf\xe2'
        list_0 = [bytes_0, bytes_0, bytes_0, bytes_0]
        ordered_default_dict_0 = None
        list_1 = [list_0]
        tuple_0 = (ordered_default_dict_0, list_1, list_1, list_0)
        var_0 = module_0.assert_valid_explicit_coords(list_0, list_1, tuple_0)
    except BaseException:
        pass


def test_case_24():
    try:
        merge_error_0 = module_0.MergeError()
        assert module_0.TYPE_CHECKING is False
        bytes_0 = b'\xb6\xa6\xe1\xc2'
        list_0 = [bytes_0, bytes_0, merge_error_0]
        str_0 = '1xG&yRSST'
        var_0 = module_0.merge(list_0, str_0)
    except BaseException:
        pass


def test_case_25():
    try:
        str_0 = '($=3]rzD44'
        set_0 = {str_0, str_0}
        list_0 = [str_0]
        float_0 = -995.72
        int_0 = 3
        var_0 = module_0.dataset_merge_method(str_0, set_0, list_0, float_0, float_0, int_0)
    except BaseException:
        pass


def test_case_26():
    try:
        tuple_0 = ()
        str_0 = ';7'
        str_1 = 'Y$^R\t]h%OX^P#`'
        dict_0 = {str_0: tuple_0, str_1: tuple_0}
        int_0 = 162
        str_2 = '3.-!D}'
        bytes_0 = b'\x0e\xe0%hG\xd7\x84M\x08\xec\xe3\xe8\x0e\x02mGM'
        var_0 = module_0.dataset_merge_method(tuple_0, str_1, dict_0, int_0, str_2, bytes_0)
    except BaseException:
        pass


def test_case_27():
    try:
        int_0 = -50
        str_0 = None
        str_1 = 'JR\\cI8Nv5s(h?O'
        dict_0 = {str_0: int_0, str_1: str_0}
        dict_1 = None
        set_0 = None
        float_0 = -2608.9
        str_2 = 'uRY-'
        var_0 = module_0.dataset_merge_method(int_0, dict_0, dict_1, set_0, float_0, str_2)
    except BaseException:
        pass


def test_case_28():
    try:
        int_0 = -2895
        var_0 = module_0.dataset_update_method(int_0, int_0)
    except BaseException:
        pass


def test_case_29():
    try:
        ordered_dict_0 = None
        set_0 = set()
        dict_0 = {}
        tuple_0 = (ordered_dict_0, set_0, dict_0)
        tuple_1 = (ordered_dict_0, set_0, dict_0)
        str_0 = 'Mrm&!nU'
        bytes_0 = b'S#}'
        list_0 = [ordered_dict_0]
        float_0 = 4249.9452
        var_0 = module_0.dataset_merge_method(tuple_0, tuple_1, str_0, bytes_0, list_0, float_0)
    except BaseException:
        pass


def test_case_30():
    try:
        str_0 = "\x0b/}agz3? +]\\C'"
        dict_0 = {str_0: str_0, str_0: str_0, str_0: str_0}
        list_0 = [dict_0]
        list_1 = [dict_0, list_0]
        ordered_dict_0 = module_0.merge_variables(list_0, list_1)
    except BaseException:
        pass


def test_case_31():
    try:
        str_0 = ''
        var_0 = module_0.merge_coords_for_inplace_math(str_0)
        assert var_0 == {}
        assert module_0.TYPE_CHECKING is False
        assert len(module_0.PANDAS_TYPES) == 3
        bool_0 = None
        dict_0 = {str_0: bool_0}
        var_1 = module_0.dataset_update_method(bool_0, dict_0)
    except BaseException:
        pass


def test_case_32():
    try:
        list_0 = []
        merge_error_0 = module_0.MergeError(*list_0)
        assert module_0.TYPE_CHECKING is False
        bytes_0 = b'\xa8\xb1\xdc\xbf\xa6\x17\xd5\xc8\xba\x80LV* t'
        list_1 = []
        str_0 = "'I1CE@\\E9WHmsrDX-81"
        ordered_dict_0 = module_0.merge_variables(list_1, str_0)
        assert merge_error_0 is not None
        assert ordered_dict_0 == {}
        assert len(module_0.PANDAS_TYPES) == 3
        int_0 = 2980
        ordered_dict_1 = None
        set_0 = set()
        hashable_0 = None
        int_1 = 3778
        dict_0 = {hashable_0: int_0, hashable_0: int_0, hashable_0: int_0, hashable_0: int_1}
        tuple_0 = (ordered_dict_1, set_0, dict_0)
        float_0 = None
        var_0 = module_0.dataset_merge_method(merge_error_0, bytes_0, ordered_dict_0, int_0, tuple_0, float_0)
    except BaseException:
        pass


def test_case_33():
    try:
        bytes_0 = b'\xb6\xa6\x7f\xf3'
        str_0 = 'g|'
        var_0 = module_0.unique_variable(bytes_0, bytes_0, str_0)
    except BaseException:
        pass


def test_case_34():
    try:
        set_0 = None
        str_0 = 'AjR9-3B?l 5%'
        dict_0 = {str_0: set_0, str_0: set_0}
        list_0 = [dict_0, str_0, str_0, str_0]
        var_0 = module_0.expand_variable_dicts(list_0)
    except BaseException:
        pass


def test_case_35():
    try:
        complex_0 = None
        set_0 = {complex_0}
        str_0 = 'Zvl0qzd,0BX89\x0c1'
        ordered_default_dict_0 = module_0.OrderedDefaultDict(str_0)
        ordered_dict_0 = None
        bool_0 = False
        list_0 = [set_0, complex_0, str_0, complex_0]
        tuple_0 = (ordered_dict_0, bool_0, list_0, str_0)
        var_0 = module_0.unique_variable(ordered_default_dict_0, tuple_0)
    except BaseException:
        pass


def test_case_36():
    try:
        ordered_dict_0 = module_1.OrderedDict()
        list_0 = [ordered_dict_0, ordered_dict_0]
        var_0 = module_0.merge(list_0)
        assert ordered_dict_0 == {}
        assert len(var_0) == 0
        assert module_0.TYPE_CHECKING is False
        assert len(module_0.PANDAS_TYPES) == 3
        assert module_2.Dataset.load_store is not None
        assert module_2.Dataset.from_dataframe is not None
        assert module_2.Dataset.from_dict is not None
        str_0 = '\rx>'
        str_1 = None
        dict_0 = {str_0: ordered_dict_0, str_0: str_0, str_1: str_0}
        list_1 = []
        ordered_dict_1 = module_0.broadcast_dimension_size(list_1)
        assert ordered_dict_1 == {}
        assert module_2.TYPE_CHECKING is False
        assert module_2.OPTIONS == {'display_width': 80, 'arithmetic_join': 'inner', 'enable_cftimeindex': True, 'file_cache_maxsize': 128, 'warn_for_unclosed_files': False, 'cmap_sequential': 'viridis', 'cmap_divergent': 'RdBu_r', 'keep_attrs': 'default'}
        assert module_2.dask_array_type == ()
        int_0 = 255
        variable_0 = module_3.Variable(ordered_dict_1, int_0, dict_0)
        variable_1 = variable_0.transpose()
        var_1 = module_0.assert_valid_explicit_coords(variable_1, int_0, ordered_dict_1)
        int_1 = 797
        var_2 = module_0.dataset_update_method(var_0, var_0)
        ordered_default_dict_0 = module_0.OrderedDefaultDict(ordered_dict_1)
        merge_error_0 = module_0.MergeError()
        ordered_default_dict_1 = module_0.OrderedDefaultDict(int_1)
        tuple_0 = (merge_error_0, ordered_default_dict_1)
        var_3 = module_0.extract_indexes(tuple_0)
        ordered_default_dict_2 = module_0.OrderedDefaultDict(int_1)
        var_4 = module_0.dataset_update_method(list_0, int_1)
    except BaseException:
        pass


def test_case_37():
    try:
        str_0 = '7'
        int_0 = 2
        int_1 = [int_0, int_0, int_0]
        variable_0 = module_3.Variable(str_0, int_1)
        str_1 = (str_0,)
        int_2 = 5
        int_3 = [int_2, int_2, int_2]
        variable_1 = module_3.Variable(str_1, int_3)
        str_2 = 'a'
        variable_2 = {str_2: variable_0}
        variable_3 = {str_2: variable_1}
        variable_4 = [variable_2, variable_3]
        str_3 = 'override'
        ordered_dict_0 = module_0.merge_variables(variable_4, str_3)
        assert len(variable_0) == 3
        assert len(variable_1) == 3
        assert ordered_dict_0 == {}
        assert module_3.IS_NEP18_ACTIVE is True
        assert module_3.dask_array_type == ()
        assert len(module_3.integer_types) == 2
        assert len(module_3.NON_NUMPY_SUPPORTED_ARRAY_TYPES) == 2
        assert len(module_3.BASIC_INDEXING_TYPES) == 3
        assert module_0.TYPE_CHECKING is False
        assert len(module_0.PANDAS_TYPES) == 3
        assert module_3.Variable.concat is not None
        var_0 = ordered_dict_0[str_2]
    except BaseException:
        pass


def test_case_38():
    try:
        ordered_dict_0 = None
        dict_0 = {ordered_dict_0: ordered_dict_0, ordered_dict_0: ordered_dict_0}
        list_0 = [dict_0]
        var_0 = module_0.merge_coords(list_0)
        assert len(var_0) == 1
        assert module_0.TYPE_CHECKING is False
        assert len(module_0.PANDAS_TYPES) == 3
    except BaseException:
        pass


def test_case_39():
    try:
        str_0 = 'x'
        str_1 = (str_0,)
        int_0 = 1
        int_1 = 2
        int_2 = 3
        int_3 = [int_0, int_1, int_2]
        variable_0 = module_3.Variable(str_1, int_3)
        str_2 = (str_0,)
        int_4 = [int_0, int_1, int_2]
        variable_1 = module_3.Variable(str_2, int_4)
        str_3 = (str_0,)
        int_5 = 4
        int_6 = 5
        int_7 = 6
        int_8 = [int_5, int_6, int_7]
        variable_2 = module_3.Variable(str_3, int_8)
        str_4 = 'y'
        str_5 = (str_4,)
        int_9 = 7
        int_10 = 8
        int_11 = 9
        int_12 = [int_9, int_10, int_11]
        variable_3 = module_3.Variable(str_5, int_12)
        str_6 = 'a'
        variable_4 = {str_6: variable_0}
        variable_5 = {str_6: variable_1}
        variable_6 = [variable_4, variable_5]
        ordered_dict_0 = module_0.merge_variables(variable_6)
        assert len(variable_0) == 3
        assert len(variable_1) == 3
        assert len(variable_2) == 3
        assert len(variable_3) == 3
        assert len(ordered_dict_0) == 1
        assert module_3.IS_NEP18_ACTIVE is True
        assert module_3.dask_array_type == ()
        assert len(module_3.integer_types) == 2
        assert len(module_3.NON_NUMPY_SUPPORTED_ARRAY_TYPES) == 2
        assert len(module_3.BASIC_INDEXING_TYPES) == 3
        assert module_0.TYPE_CHECKING is False
        assert len(module_0.PANDAS_TYPES) == 3
        assert module_3.Variable.concat is not None
        var_0 = ordered_dict_0[str_6]
        str_7 = 'Test case 1 failed'
        variable_7 = {str_6: variable_2}
        variable_8 = [variable_4, variable_7]
        ordered_dict_1 = module_0.merge_variables(variable_8, str_7)
    except BaseException:
        pass


def test_case_40():
    try:
        str_0 = 'x'
        str_1 = (str_0,)
        int_0 = 1
        int_1 = 2
        int_2 = 3
        int_3 = [int_0, int_1, int_2]
        variable_0 = module_3.Variable(str_1, int_3)
        str_2 = (str_0,)
        int_4 = [int_0, int_1, str_0, int_1, int_2]
        variable_1 = module_3.Variable(str_2, int_4)
        str_3 = (str_0,)
        int_5 = 4
        int_6 = 5
        int_7 = 6
        int_8 = [int_5, int_6, int_7]
        variable_2 = module_3.Variable(str_3, int_8)
        str_4 = 'y'
        str_5 = (str_4,)
        int_9 = 7
        int_10 = 8
        int_11 = 9
        int_12 = [int_9, int_10, int_11]
        variable_3 = module_3.Variable(str_5, int_12)
        str_6 = 'a'
        variable_4 = {str_6: variable_0}
        variable_5 = {str_6: variable_1}
        variable_6 = [variable_4, variable_5]
        ordered_dict_0 = module_0.merge_variables(variable_6)
    except BaseException:
        pass


def test_case_41():
    try:
        str_0 = 'K{hVS6^^UOE'
        list_0 = [str_0]
        variable_0 = module_3.Variable(list_0, list_0)
        variable_1 = variable_0.transpose()
        ndarray_0 = variable_1.__array__()
        variable_2 = module_3.Variable(str_0, ndarray_0)
        list_1 = [variable_2, variable_0, variable_2, variable_2]
        ordered_dict_0 = module_0.broadcast_dimension_size(list_1)
        assert len(variable_0) == 1
        assert len(variable_1) == 1
        assert len(ndarray_0) == 1
        assert len(variable_2) == 1
        assert ordered_dict_0 == {'K{hVS6^^UOE': 1}
        assert module_3.IS_NEP18_ACTIVE is True
        assert module_3.dask_array_type == ()
        assert len(module_3.integer_types) == 2
        assert len(module_3.NON_NUMPY_SUPPORTED_ARRAY_TYPES) == 2
        assert len(module_3.BASIC_INDEXING_TYPES) == 3
        assert module_4.newaxis is None
        assert module_4.little_endian is True
        assert module_4.Inf == pytest.approx(1e309, abs=0.01, rel=0.01)
        assert module_4.inf == pytest.approx(1e309, abs=0.01, rel=0.01)
        assert module_4.infty == pytest.approx(1e309, abs=0.01, rel=0.01)
        assert module_4.Infinity == pytest.approx(1e309, abs=0.01, rel=0.01)
        assert module_4.CLIP == 0
        assert module_4.RAISE == 2
        assert module_4.WRAP == 1
        assert module_4.MAXDIMS == 32
        assert module_4.BUFSIZE == 8192
        assert module_4.ALLOW_THREADS == 1
        assert module_4.MAY_SHARE_BOUNDS == 0
        assert module_4.MAY_SHARE_EXACT == -1
        assert module_4.ERR_CALL == 3
        assert module_4.ERR_DEFAULT == 521
        assert module_4.ERR_IGNORE == 0
        assert module_4.ERR_LOG == 5
        assert module_4.ERR_PRINT == 4
        assert module_4.ERR_RAISE == 2
        assert module_4.ERR_WARN == 1
        assert module_4.FLOATING_POINT_SUPPORT == 1
        assert module_4.FPE_DIVIDEBYZERO == 1
        assert module_4.FPE_INVALID == 8
        assert module_4.FPE_OVERFLOW == 2
        assert module_4.FPE_UNDERFLOW == 4
        assert module_4.NINF == pytest.approx(-1e309, abs=0.01, rel=0.01)
        assert module_4.NZERO == pytest.approx(-0.0, abs=0.01, rel=0.01)
        assert module_4.PINF == pytest.approx(1e309, abs=0.01, rel=0.01)
        assert module_4.PZERO == pytest.approx(0.0, abs=0.01, rel=0.01)
        assert module_4.SHIFT_DIVIDEBYZERO == 0
        assert module_4.SHIFT_INVALID == 9
        assert module_4.SHIFT_OVERFLOW == 3
        assert module_4.SHIFT_UNDERFLOW == 6
        assert module_4.UFUNC_BUFSIZE_DEFAULT == 8192
        assert module_4.UFUNC_PYVALS_NAME == 'UFUNC_PYVALS'
        assert module_4.e == pytest.approx(2.718281828459045, abs=0.01, rel=0.01)
        assert module_4.euler_gamma == pytest.approx(0.5772156649015329, abs=0.01, rel=0.01)
        assert module_4.pi == pytest.approx(3.141592653589793, abs=0.01, rel=0.01)
        assert len(module_4.sctypeDict) == 136
        assert len(module_4.sctypes) == 5
        assert len(module_4.ScalarType) == 31
        assert len(module_4.cast) == 24
        assert len(module_4.nbytes) == 24
        assert module_4.typecodes == {'Character': 'c', 'Integer': 'bhilqp', 'UnsignedInteger': 'BHILQP', 'Float': 'efdg', 'Complex': 'FDG', 'AllInteger': 'bBhHiIlLqQpP', 'AllFloat': 'efdgFDG', 'Datetime': 'Mm', 'All': '?bhilqpBHILQPefdgFDGSUVOMm'}
        assert module_4.tracemalloc_domain == 389047
        assert module_4.mgrid.sparse is False
        assert module_4.ogrid.sparse is True
        assert len(module_4.r_) == 0
        assert len(module_4.c_) == 0
        assert module_4.s_.maketuple is False
        assert module_4.index_exp.maketuple is True
        assert module_4.oldnumeric == 'removed'
        assert module_4.numarray == 'removed'
        assert module_4.use_hugepage == 1
        assert module_4.kernel_version == (6, 8)
        assert module_0.TYPE_CHECKING is False
        assert len(module_0.PANDAS_TYPES) == 3
        assert module_3.Variable.concat is not None
        ndarray_1 = variable_1.__array__()
        assert len(ndarray_1) == 1
        str_1 = None
        dict_0 = {str_0: ndarray_1, str_0: str_0, str_1: variable_0}
        var_0 = module_0.merge_data_and_coords(ordered_dict_0, dict_0)
    except BaseException:
        pass


def test_case_42():
    try:
        ordered_dict_0 = module_1.OrderedDict()
        list_0 = [ordered_dict_0, ordered_dict_0]
        var_0 = module_0.merge(list_0)
        assert ordered_dict_0 == {}
        assert len(var_0) == 0
        assert module_0.TYPE_CHECKING is False
        assert len(module_0.PANDAS_TYPES) == 3
        assert module_2.Dataset.load_store is not None
        assert module_2.Dataset.from_dataframe is not None
        assert module_2.Dataset.from_dict is not None
        list_1 = [var_0, var_0, list_0]
        var_1 = module_0.expand_and_merge_variables(list_1)
    except BaseException:
        pass

Coverage: 85.77586206896552
Mutation Score: 20.78

Baseline 3 (GPT-4o):
Predicted Test Suite: 
import pytest
from collections import OrderedDict
from xarray.core import merge
from xarray.core.variable import Variable
from xarray.core.merge import MergeError, OrderedDefaultDict
from xarray.core.utils import Frozen
import pandas as pd
import numpy as np

# Mock classes for testing
class MockVariable:
    def __init__(self, dims, shape):
        self.dims = dims
        self.shape = shape

    def set_dims(self, dim_lengths):
        return self

    def compute(self):
        return self

    def identical(self, other):
        return self.dims == other.dims and self.shape == other.shape

    def equals(self, other):
        return self.identical(other)

    def broadcast_equals(self, other):
        return self.identical(other)

    def fillna(self, other):
        return self

    def to_index(self):
        return pd.Index(np.arange(self.shape[0]))

# Test cases

def test_broadcast_dimension_size():
    variables = [MockVariable(('x',), (3,)), MockVariable(('y',), (4,))]
    result = merge.broadcast_dimension_size(variables)
    assert result == OrderedDict([('x', 3), ('y', 4)])

    with pytest.raises(ValueError):
        variables = [MockVariable(('x',), (3,)), MockVariable(('x',), (4,))]
        merge.broadcast_dimension_size(variables)

def test_unique_variable():
    var1 = MockVariable(('x',), (3,))
    var2 = MockVariable(('x',), (3,))
    variables = [var1, var2]
    result = merge.unique_variable('test', variables)
    assert result == var1

    with pytest.raises(MergeError):
        var3 = MockVariable(('x',), (4,))
        variables = [var1, var3]
        merge.unique_variable('test', variables)

def test_assert_compat_valid():
    merge._assert_compat_valid('identical')
    with pytest.raises(ValueError):
        merge._assert_compat_valid('invalid_compat')

def test_ordered_default_dict():
    def default_factory():
        return []

    odd = OrderedDefaultDict(default_factory)
    odd['key'].append(1)
    assert odd['key'] == [1]
    assert odd['missing'] == []

def test_merge_variables():
    var1 = MockVariable(('x',), (3,))
    var2 = MockVariable(('y',), (4,))
    list_of_variables_dicts = [{'a': var1}, {'b': var2}]
    result = merge.merge_variables(list_of_variables_dicts)
    assert result == OrderedDict([('a', var1), ('b', var2)])

def test_expand_variable_dicts():
    var1 = MockVariable(('x',), (3,))
    var2 = MockVariable(('y',), (4,))
    list_of_variable_dicts = [{'a': var1, 'b': var2}]
    result = merge.expand_variable_dicts(list_of_variable_dicts)
    assert result == [{'a': var1, 'b': var2}]

def test_determine_coords():
    var1 = MockVariable(('x',), (3,))
    var2 = MockVariable(('y',), (4,))
    list_of_variable_dicts = [{'a': var1, 'b': var2}]
    coord_names, noncoord_names = merge.determine_coords(list_of_variable_dicts)
    assert coord_names == set()
    assert noncoord_names == set()

def test_coerce_pandas_values():
    df = pd.DataFrame({'a': [1, 2, 3]})
    result = merge.coerce_pandas_values([df])
    assert isinstance(result[0]['a'], MockVariable)

def test_merge_coords_for_inplace_math():
    var1 = MockVariable(('x',), (3,))
    var2 = MockVariable(('y',), (4,))
    objs = [{'a': var1}, {'b': var2}]
    result = merge.merge_coords_for_inplace_math(objs)
    assert result == OrderedDict([('a', var1), ('b', var2)])

def test_merge_core():
    var1 = MockVariable(('x',), (3,))
    var2 = MockVariable(('y',), (4,))
    objs = [{'a': var1}, {'b': var2}]
    variables, coord_names, dims = merge.merge_core(objs)
    assert variables == OrderedDict([('a', var1), ('b', var2)])
    assert coord_names == set()
    assert dims == {'x': 3, 'y': 4}

def test_merge():
    var1 = MockVariable(('x',), (3,))
    var2 = MockVariable(('y',), (4,))
    objs = [{'a': var1}, {'b': var2}]
    result = merge.merge(objs)
    assert result.variables == OrderedDict([('a', var1), ('b', var2)])

Coverage: 55.60344827586207
Mutation Score: 28.14

Approach:
Predicted Test Suite: import pytest
from collections import OrderedDict
from xarray.core import merge

from xarray.core.merge import MergeError, OrderedDefaultDict
from xarray.core.utils import Frozen
import pandas as pd
import numpy as np

# Mock classes for testing
class MockVariable:
    def __init__(self, dims, shape):
        self.dims = dims
        self.shape = shape

    def set_dims(self, dim_lengths):
        return self

    def compute(self):
        return self

    def identical(self, other):
        return self.dims == other.dims and self.shape == other.shape

    def equals(self, other):
        return self.identical(other)

    def broadcast_equals(self, other):
        return self.identical(other)

    def fillna(self, other):
        return self

    def to_index(self):
        return pd.Index(np.arange(self.shape[0]))

# Test cases

def test_broadcast_dimension_size():
    variables = [MockVariable(('x',), (3,)), MockVariable(('y',), (4,))]
    result = merge.broadcast_dimension_size(variables)
    assert result == OrderedDict([('x', 3), ('y', 4)])

    with pytest.raises(ValueError):
        variables = [MockVariable(('x',), (3,)), MockVariable(('x',), (4,))]
        merge.broadcast_dimension_size(variables)

def test_unique_variable():
    var1 = MockVariable(('x',), (3,))
    var2 = MockVariable(('x',), (3,))
    variables = [var1, var2]
    result = merge.unique_variable('test', variables)
    assert result is var1

    with pytest.raises(MergeError):
        var3 = MockVariable(('x',), (4,))
        variables = [var1, var3]
        merge.unique_variable('test', variables)

def test_assert_compat_valid():
    merge._assert_compat_valid('identical')
    with pytest.raises(ValueError):
        merge._assert_compat_valid('invalid_compat')

def test_ordered_default_dict():
    def default_factory():
        return []

    odd = OrderedDefaultDict(default_factory)
    odd['key'].append(1)
    assert odd['key'] == [1]
    assert odd['missing'] == []

def test_merge_variables():
    var1 = MockVariable(('x',), (3,))
    var2 = MockVariable(('y',), (4,))
    list_of_variables_dicts = [{'a': var1}, {'b': var2}]
    result = merge.merge_variables(list_of_variables_dicts)
    assert result == OrderedDict([('a', var1), ('b', var2)])

def test_expand_variable_dicts():
    var1 = MockVariable(('x',), (3,))
    var2 = MockVariable(('y',), (4,))
    list_of_variable_dicts = [{'a': var1, 'b': var2}]
    result = merge.expand_variable_dicts(list_of_variable_dicts)
    assert result == [{'a': var1, 'b': var2}]

def test_determine_coords():
    var1 = MockVariable(('x',), (3,))
    var2 = MockVariable(('y',), (4,))
    list_of_variable_dicts = [{'a': var1, 'b': var2}]
    coord_names, noncoord_names = merge.determine_coords(list_of_variable_dicts)
    assert coord_names == set()
    assert noncoord_names == set()

def test_coerce_pandas_values():
    df = pd.DataFrame({'a': [1, 2, 3]})
    result = merge.coerce_pandas_values([df])
    assert isinstance(result[0]['a'], merge.Variable)

def test_merge_coords_for_inplace_math():
    var1 = MockVariable(('x',), (3,))
    var2 = MockVariable(('y',), (4,))
    objs = [{'a': var1}, {'b': var2}]
    result = merge.merge_coords_for_inplace_math(objs)
    assert result == OrderedDict([('a', var1), ('b', var2)])

def test_merge_core():
    var1 = MockVariable(('x',), (3,))
    var2 = MockVariable(('y',), (4,))
    objs = [{'a': var1}, {'b': var2}]
    variables, coord_names, dims = merge.merge_core(objs)
    assert variables == OrderedDict([('a', var1), ('b', var2)])
    assert coord_names == set()
    assert dims == {}

def test_merge():
    var1 = MockVariable(('x',), (3,))
    var2 = MockVariable(('y',), (4,))
    objs = [{'a': var1}, {'b': var2}]
    result = merge.merge(objs)
    assert result.variables == OrderedDict([('a', var1), ('b', var2)])
Coverage: 51.293103448275865
Mutation Score: 26.409999999999997
Output: On branch main
Changes not staged for commit:
  (use "git add/rm <file>..." to update what will be committed)
  (use "git restore <file>..." to discard changes in working directory)
        deleted:    .coveragerc
        modified:   xarray/tests/test_merge.py

no changes added to commit (use "git add" and/or "git commit -a")
commit 2c8ba46b3dc47f2c4e488d40d4d5a855ddefc99d
Author: TestGenEval <>
Date:   Wed Dec 11 12:05:16 2024 +0000

    Testing fixes

diff --git a/xarray/backends/api.py b/xarray/backends/api.py
index a20d3c2a..1f0869cf 100644
--- a/xarray/backends/api.py
+++ b/xarray/backends/api.py
@@ -761,7 +761,7 @@ def open_mfdataset(
         `xarray.auto_combine` is used, but in the future this behavior will
         switch to use `xarray.combine_by_coords` by default.
     compat : {'identical', 'equals', 'broadcast_equals',
-              'no_conflicts'}, optional
+              'no_conflicts', 'override'}, optional
         String indicating how to compare variables of the same name for
         potential conflicts when merging:
          * 'broadcast_equals': all values must be equal when variables are
@@ -772,6 +772,7 @@ def open_mfdataset(
          * 'no_conflicts': only values which are not null in both datasets
            must be equal. The returned dataset then contains the combination
            of all non-null values.
+         * 'override': skip comparing and pick variable from first dataset
     preprocess : callable, optional
         If provided, call this function on each dataset prior to concatenation.
         You can find the file-name from which each dataset was loaded in
diff --git a/xarray/core/combine.py b/xarray/core/combine.py
index c24be88b..e35bb51e 100644
--- a/xarray/core/combine.py
+++ b/xarray/core/combine.py
@@ -243,6 +243,7 @@ def _combine_1d(
                 dim=concat_dim,
                 data_vars=data_vars,
                 coords=coords,
+                compat=compat,
                 fill_value=fill_value,
                 join=join,
             )
@@ -351,7 +352,7 @@ def combine_nested(
         Must be the same length as the depth of the list passed to
         ``datasets``.
     compat : {'identical', 'equals', 'broadcast_equals',
-              'no_conflicts'}, optional
+              'no_conflicts', 'override'}, optional
         String indicating how to compare variables of the same name for
         potential merge conflicts:

@@ -363,6 +364,7 @@ def combine_nested(
         - 'no_conflicts': only values which are not null in both datasets
           must be equal. The returned dataset then contains the combination
           of all non-null values.
+        - 'override': skip comparing and pick variable from first dataset
     data_vars : {'minimal', 'different', 'all' or list of str}, optional
         Details are in the documentation of concat
     coords : {'minimal', 'different', 'all' or list of str}, optional
@@ -504,7 +506,7 @@ def combine_by_coords(
     datasets : sequence of xarray.Dataset
         Dataset objects to combine.
     compat : {'identical', 'equals', 'broadcast_equals',
-              'no_conflicts'}, optional
+              'no_conflicts', 'override'}, optional
         String indicating how to compare variables of the same name for
         potential conflicts:

@@ -516,6 +518,7 @@ def combine_by_coords(
         - 'no_conflicts': only values which are not null in both datasets
           must be equal. The returned dataset then contains the combination
           of all non-null values.
+        - 'override': skip comparing and pick variable from first dataset
     data_vars : {'minimal', 'different', 'all' or list of str}, optional
         Details are in the documentation of concat
     coords : {'minimal', 'different', 'all' or list of str}, optional
@@ -598,6 +601,7 @@ def combine_by_coords(
             concat_dims=concat_dims,
             data_vars=data_vars,
             coords=coords,
+            compat=compat,
             fill_value=fill_value,
             join=join,
         )
@@ -667,7 +671,7 @@ def auto_combine(
         component files. Set ``concat_dim=None`` explicitly to disable
         concatenation.
     compat : {'identical', 'equals', 'broadcast_equals',
-             'no_conflicts'}, optional
+             'no_conflicts', 'override'}, optional
         String indicating how to compare variables of the same name for
         potential conflicts:
         - 'broadcast_equals': all values must be equal when variables are
@@ -678,6 +682,7 @@ def auto_combine(
         - 'no_conflicts': only values which are not null in both datasets
           must be equal. The returned dataset then contains the combination
           of all non-null values.
+        - 'override': skip comparing and pick variable from first dataset
     data_vars : {'minimal', 'different', 'all' or list of str}, optional
         Details are in the documentation of concat
     coords : {'minimal', 'different', 'all' o list of str}, optional
@@ -832,6 +837,7 @@ def _old_auto_combine(
                 dim=dim,
                 data_vars=data_vars,
                 coords=coords,
+                compat=compat,
                 fill_value=fill_value,
                 join=join,
             )
@@ -850,6 +856,7 @@ def _auto_concat(
     coords="different",
     fill_value=dtypes.NA,
     join="outer",
+    compat="no_conflicts",
 ):
     if len(datasets) == 1 and dim is None:
         # There is nothing more to combine, so kick out early.
@@ -876,5 +883,10 @@ def _auto_concat(
                 )
             dim, = concat_dims
         return concat(
-            datasets, dim=dim, data_vars=data_vars, coords=coords, fill_value=fill_value
+            datasets,
+            dim=dim,
+            data_vars=data_vars,
+            coords=coords,
+            fill_value=fill_value,
+            compat=compat,
         )
diff --git a/xarray/core/concat.py b/xarray/core/concat.py
index d5dfa49a..e68c247d 100644
--- a/xarray/core/concat.py
+++ b/xarray/core/concat.py
@@ -4,6 +4,7 @@ import pandas as pd

 from . import dtypes, utils
 from .alignment import align
+from .merge import unique_variable, _VALID_COMPAT
 from .variable import IndexVariable, Variable, as_variable
 from .variable import concat as concat_vars

@@ -59,12 +60,19 @@ def concat(
             those corresponding to other dimensions.
           * list of str: The listed coordinate variables will be concatenated,
             in addition to the 'minimal' coordinates.
-    compat : {'equals', 'identical'}, optional
-        String indicating how to compare non-concatenated variables and
-        dataset global attributes for potential conflicts. 'equals' means
-        that all variable values and dimensions must be the same;
-        'identical' means that variable attributes and global attributes
-        must also be equal.
+    compat : {'identical', 'equals', 'broadcast_equals', 'no_conflicts', 'override'}, optional
+        String indicating how to compare non-concatenated variables of the same name for
+        potential conflicts. This is passed down to merge.
+
+        - 'broadcast_equals': all values must be equal when variables are
+          broadcast against each other to ensure common dimensions.
+        - 'equals': all values and dimensions must be the same.
+        - 'identical': all values, dimensions and attributes must be the
+          same.
+        - 'no_conflicts': only values which are not null in both datasets
+          must be equal. The returned dataset then contains the combination
+          of all non-null values.
+        - 'override': skip comparing and pick variable from first dataset
     positions : None or list of integer arrays, optional
         List of integer arrays which specifies the integer positions to which
         to assign each dataset along the concatenated dimension. If not
@@ -107,6 +115,12 @@ def concat(
     except StopIteration:
         raise ValueError("must supply at least one object to concatenate")

+    if compat not in _VALID_COMPAT:
+        raise ValueError(
+            "compat=%r invalid: must be 'broadcast_equals', 'equals', 'identical', 'no_conflicts' or 'override'"
+            % compat
+        )
+
     if isinstance(first_obj, DataArray):
         f = _dataarray_concat
     elif isinstance(first_obj, Dataset):
@@ -143,23 +157,39 @@ def _calc_concat_dim_coord(dim):
     return dim, coord


-def _calc_concat_over(datasets, dim, data_vars, coords):
+def _calc_concat_over(datasets, dim, dim_names, data_vars, coords, compat):
     """
     Determine which dataset variables need to be concatenated in the result,
-    and which can simply be taken from the first dataset.
     """
     # Return values
     concat_over = set()
     equals = {}

-    if dim in datasets[0]:
+    if dim in dim_names:
+        concat_over_existing_dim = True
         concat_over.add(dim)
+    else:
+        concat_over_existing_dim = False
+
+    concat_dim_lengths = []
     for ds in datasets:
+        if concat_over_existing_dim:
+            if dim not in ds.dims:
+                if dim in ds:
+                    ds = ds.set_coords(dim)
+                else:
+                    raise ValueError("%r is not present in all datasets" % dim)
         concat_over.update(k for k, v in ds.variables.items() if dim in v.dims)
+        concat_dim_lengths.append(ds.dims.get(dim, 1))

     def process_subset_opt(opt, subset):
         if isinstance(opt, str):
             if opt == "different":
+                if compat == "override":
+                    raise ValueError(
+                        "Cannot specify both %s='different' and compat='override'."
+                        % subset
+                    )
                 # all nonindexes that are not the same in each dataset
                 for k in getattr(datasets[0], subset):
                     if k not in concat_over:
@@ -173,7 +203,7 @@ def _calc_concat_over(datasets, dim, data_vars, coords):
                         for ds_rhs in datasets[1:]:
                             v_rhs = ds_rhs.variables[k].compute()
                             computed.append(v_rhs)
-                            if not v_lhs.equals(v_rhs):
+                            if not getattr(v_lhs, compat)(v_rhs):
                                 concat_over.add(k)
                                 equals[k] = False
                                 # computed variables are not to be re-computed
@@ -209,7 +239,29 @@ def _calc_concat_over(datasets, dim, data_vars, coords):

     process_subset_opt(data_vars, "data_vars")
     process_subset_opt(coords, "coords")
-    return concat_over, equals
+    return concat_over, equals, concat_dim_lengths
+
+
+# determine dimensional coordinate names and a dict mapping name to DataArray
+def _parse_datasets(datasets):
+
+    dims = set()
+    all_coord_names = set()
+    data_vars = set()  # list of data_vars
+    dim_coords = dict()  # maps dim name to variable
+    dims_sizes = {}  # shared dimension sizes to expand variables
+
+    for ds in datasets:
+        dims_sizes.update(ds.dims)
+        all_coord_names.update(ds.coords)
+        data_vars.update(ds.data_vars)
+
+        for dim in set(ds.dims) - dims:
+            if dim not in dim_coords:
+                dim_coords[dim] = ds.coords[dim].variable
+        dims = dims | set(ds.dims)
+
+    return dim_coords, dims_sizes, all_coord_names, data_vars


 def _dataset_concat(
@@ -227,11 +279,6 @@ def _dataset_concat(
     """
     from .dataset import Dataset

-    if compat not in ["equals", "identical"]:
-        raise ValueError(
-            "compat=%r invalid: must be 'equals' " "or 'identical'" % compat
-        )
-
     dim, coord = _calc_concat_dim_coord(dim)
     # Make sure we're working on a copy (we'll be loading variables)
     datasets = [ds.copy() for ds in datasets]
@@ -239,62 +286,65 @@ def _dataset_concat(
         *datasets, join=join, copy=False, exclude=[dim], fill_value=fill_value
     )

-    concat_over, equals = _calc_concat_over(datasets, dim, data_vars, coords)
+    dim_coords, dims_sizes, coord_names, data_names = _parse_datasets(datasets)
+    dim_names = set(dim_coords)
+    unlabeled_dims = dim_names - coord_names
+
+    both_data_and_coords = coord_names & data_names
+    if both_data_and_coords:
+        raise ValueError(
+            "%r is a coordinate in some datasets but not others." % both_data_and_coords
+        )
+    # we don't want the concat dimension in the result dataset yet
+    dim_coords.pop(dim, None)
+    dims_sizes.pop(dim, None)
+
+    # case where concat dimension is a coordinate or data_var but not a dimension
+    if (dim in coord_names or dim in data_names) and dim not in dim_names:
+        datasets = [ds.expand_dims(dim) for ds in datasets]
+
+    # determine which variables to concatentate
+    concat_over, equals, concat_dim_lengths = _calc_concat_over(
+        datasets, dim, dim_names, data_vars, coords, compat
+    )
+
+    # determine which variables to merge, and then merge them according to compat
+    variables_to_merge = (coord_names | data_names) - concat_over - dim_names
+
+    result_vars = {}
+    if variables_to_merge:
+        to_merge = {var: [] for var in variables_to_merge}
+
+        for ds in datasets:
+            absent_merge_vars = variables_to_merge - set(ds.variables)
+            if absent_merge_vars:
+                raise ValueError(
+                    "variables %r are present in some datasets but not others. "
+                    % absent_merge_vars
+                )

-    def insert_result_variable(k, v):
-        assert isinstance(v, Variable)
-        if k in datasets[0].coords:
-            result_coord_names.add(k)
-        result_vars[k] = v
+            for var in variables_to_merge:
+                to_merge[var].append(ds.variables[var])

-    # create the new dataset and add constant variables
-    result_vars = OrderedDict()
-    result_coord_names = set(datasets[0].coords)
+        for var in variables_to_merge:
+            result_vars[var] = unique_variable(
+                var, to_merge[var], compat=compat, equals=equals.get(var, None)
+            )
+    else:
+        result_vars = OrderedDict()
+    result_vars.update(dim_coords)
+
+    # assign attrs and encoding from first dataset
     result_attrs = datasets[0].attrs
     result_encoding = datasets[0].encoding

-    for k, v in datasets[0].variables.items():
-        if k not in concat_over:
-            insert_result_variable(k, v)
-
-    # check that global attributes and non-concatenated variables are fixed
-    # across all datasets
+    # check that global attributes are fixed across all datasets if necessary
     for ds in datasets[1:]:
         if compat == "identical" and not utils.dict_equiv(ds.attrs, result_attrs):
-            raise ValueError("dataset global attributes not equal")
-        for k, v in ds.variables.items():
-            if k not in result_vars and k not in concat_over:
-                raise ValueError("encountered unexpected variable %r" % k)
-            elif (k in result_coord_names) != (k in ds.coords):
-                raise ValueError(
-                    "%r is a coordinate in some datasets but not " "others" % k
-                )
-            elif k in result_vars and k != dim:
-                # Don't use Variable.identical as it internally invokes
-                # Variable.equals, and we may already know the answer
-                if compat == "identical" and not utils.dict_equiv(
-                    v.attrs, result_vars[k].attrs
-                ):
-                    raise ValueError("variable %s not identical across datasets" % k)
-
-                # Proceed with equals()
-                try:
-                    # May be populated when using the "different" method
-                    is_equal = equals[k]
-                except KeyError:
-                    result_vars[k].load()
-                    is_equal = v.equals(result_vars[k])
-                if not is_equal:
-                    raise ValueError("variable %s not equal across datasets" % k)
+            raise ValueError("Dataset global attributes not equal.")

     # we've already verified everything is consistent; now, calculate
     # shared dimension sizes so we can expand the necessary variables
-    dim_lengths = [ds.dims.get(dim, 1) for ds in datasets]
-    non_concat_dims = {}
-    for ds in datasets:
-        non_concat_dims.update(ds.dims)
-    non_concat_dims.pop(dim, None)
-
     def ensure_common_dims(vars):
         # ensure each variable with the given name shares the same
         # dimensions and the same shape for all of them except along the
@@ -302,25 +352,27 @@ def _dataset_concat(
         common_dims = tuple(pd.unique([d for v in vars for d in v.dims]))
         if dim not in common_dims:
             common_dims = (dim,) + common_dims
-        for var, dim_len in zip(vars, dim_lengths):
+        for var, dim_len in zip(vars, concat_dim_lengths):
             if var.dims != common_dims:
-                common_shape = tuple(
-                    non_concat_dims.get(d, dim_len) for d in common_dims
-                )
+                common_shape = tuple(dims_sizes.get(d, dim_len) for d in common_dims)
                 var = var.set_dims(common_dims, common_shape)
             yield var

     # stack up each variable to fill-out the dataset (in order)
+    # n.b. this loop preserves variable order, needed for groupby.
     for k in datasets[0].variables:
         if k in concat_over:
             vars = ensure_common_dims([ds.variables[k] for ds in datasets])
             combined = concat_vars(vars, dim, positions)
-            insert_result_variable(k, combined)
+            assert isinstance(combined, Variable)
+            result_vars[k] = combined

     result = Dataset(result_vars, attrs=result_attrs)
-    result = result.set_coords(result_coord_names)
+    result = result.set_coords(coord_names)
     result.encoding = result_encoding

+    result = result.drop(unlabeled_dims, errors="ignore")
+
     if coord is not None:
         # add concat dimension last to ensure that its in the final Dataset
         result[coord.name] = coord
@@ -342,7 +394,7 @@ def _dataarray_concat(

     if data_vars != "all":
         raise ValueError(
-            "data_vars is not a valid argument when " "concatenating DataArray objects"
+            "data_vars is not a valid argument when concatenating DataArray objects"
         )

     datasets = []
diff --git a/xarray/core/dataarray.py b/xarray/core/dataarray.py
index 807badde..e35b067c 100644
--- a/xarray/core/dataarray.py
+++ b/xarray/core/dataarray.py
@@ -1549,8 +1549,8 @@ class DataArray(AbstractArray, DataWithCoords):
         obj : DataArray
             Another DataArray, with this data but replaced coordinates.

-        Example
-        -------
+        Examples
+        --------
         >>> arr = xr.DataArray(data=np.ones((2, 3)),
         ...                    dims=['x', 'y'],
         ...                    coords={'x':
diff --git a/xarray/core/merge.py b/xarray/core/merge.py
index 225507b9..6dba659f 100644
--- a/xarray/core/merge.py
+++ b/xarray/core/merge.py
@@ -44,6 +44,7 @@ _VALID_COMPAT = Frozen(
         "broadcast_equals": 2,
         "minimal": 3,
         "no_conflicts": 4,
+        "override": 5,
     }
 )

@@ -70,8 +71,8 @@ class MergeError(ValueError):
     # TODO: move this to an xarray.exceptions module?


-def unique_variable(name, variables, compat="broadcast_equals"):
-    # type: (Any, List[Variable], str) -> Variable
+def unique_variable(name, variables, compat="broadcast_equals", equals=None):
+    # type: (Any, List[Variable], str, bool) -> Variable
     """Return the unique variable from a list of variables or raise MergeError.

     Parameters
@@ -81,8 +82,10 @@ def unique_variable(name, variables, compat="broadcast_equals"):
     variables : list of xarray.Variable
         List of Variable objects, all of which go by the same name in different
         inputs.
-    compat : {'identical', 'equals', 'broadcast_equals', 'no_conflicts'}, optional
+    compat : {'identical', 'equals', 'broadcast_equals', 'no_conflicts', 'override'}, optional
         Type of equality check to use.
+    equals: None or bool,
+        corresponding to result of compat test

     Returns
     -------
@@ -93,30 +96,38 @@ def unique_variable(name, variables, compat="broadcast_equals"):
     MergeError: if any of the variables are not equal.
     """  # noqa
     out = variables[0]
-    if len(variables) > 1:
-        combine_method = None

-        if compat == "minimal":
-            compat = "broadcast_equals"
+    if len(variables) == 1 or compat == "override":
+        return out
+
+    combine_method = None
+
+    if compat == "minimal":
+        compat = "broadcast_equals"
+
+    if compat == "broadcast_equals":
+        dim_lengths = broadcast_dimension_size(variables)
+        out = out.set_dims(dim_lengths)
+
+    if compat == "no_conflicts":
+        combine_method = "fillna"

-        if compat == "broadcast_equals":
-            dim_lengths = broadcast_dimension_size(variables)
-            out = out.set_dims(dim_lengths)
+    if equals is None:
+        out = out.compute()
+        for var in variables[1:]:
+            equals = getattr(out, compat)(var)
+            if not equals:
+                break

-        if compat == "no_conflicts":
-            combine_method = "fillna"
+    if not equals:
+        raise MergeError(
+            "conflicting values for variable %r on objects to be combined. You can skip this check by specifying compat='override'."
+            % (name)
+        )

+    if combine_method:
         for var in variables[1:]:
-            if not getattr(out, compat)(var):
-                raise MergeError(
-                    "conflicting values for variable %r on "
-                    "objects to be combined:\n"
-                    "first value: %r\nsecond value: %r" % (name, out, var)
-                )
-            if combine_method:
-                # TODO: add preservation of attrs into fillna
-                out = getattr(out, combine_method)(var)
-                out.attrs = var.attrs
+            out = getattr(out, combine_method)(var)

     return out

@@ -152,7 +163,7 @@ def merge_variables(
     priority_vars : mapping with Variable or None values, optional
         If provided, variables are always taken from this dict in preference to
         the input variable dictionaries, without checking for conflicts.
-    compat : {'identical', 'equals', 'broadcast_equals', 'minimal', 'no_conflicts'}, optional
+    compat : {'identical', 'equals', 'broadcast_equals', 'minimal', 'no_conflicts', 'override'}, optional
         Type of equality check to use when checking for conflicts.

     Returns
@@ -449,7 +460,7 @@ def merge_core(
     ----------
     objs : list of mappings
         All values must be convertable to labeled arrays.
-    compat : {'identical', 'equals', 'broadcast_equals', 'no_conflicts'}, optional
+    compat : {'identical', 'equals', 'broadcast_equals', 'no_conflicts', 'override'}, optional
         Compatibility checks to use when merging variables.
     join : {'outer', 'inner', 'left', 'right'}, optional
         How to combine objects with different indexes.
@@ -519,7 +530,7 @@ def merge(objects, compat="no_conflicts", join="outer", fill_value=dtypes.NA):
     objects : Iterable[Union[xarray.Dataset, xarray.DataArray, dict]]
         Merge together all variables from these objects. If any of them are
         DataArray objects, they must have a name.
-    compat : {'identical', 'equals', 'broadcast_equals', 'no_conflicts'}, optional
+    compat : {'identical', 'equals', 'broadcast_equals', 'no_conflicts', 'override'}, optional
         String indicating how to compare variables of the same name for
         potential conflicts:

@@ -531,6 +542,7 @@ def merge(objects, compat="no_conflicts", join="outer", fill_value=dtypes.NA):
         - 'no_conflicts': only values which are not null in both datasets
           must be equal. The returned dataset then contains the combination
           of all non-null values.
+        - 'override': skip comparing and pick variable from first dataset
     join : {'outer', 'inner', 'left', 'right', 'exact'}, optional
         String indicating how to combine differing indexes in objects.

diff --git a/xarray/tests/test_combine.py b/xarray/tests/test_combine.py
deleted file mode 100644
index f786a851..00000000
--- a/xarray/tests/test_combine.py
+++ /dev/null
@@ -1,886 +0,0 @@
-from collections import OrderedDict
-from datetime import datetime
-from itertools import product
-
-import numpy as np
-import pytest
-
-from xarray import (
-    DataArray,
-    Dataset,
-    auto_combine,
-    combine_by_coords,
-    combine_nested,
-    concat,
-)
-from xarray.core import dtypes
-from xarray.core.combine import (
-    _check_shape_tile_ids,
-    _combine_all_along_first_dim,
-    _combine_nd,
-    _infer_concat_order_from_coords,
-    _infer_concat_order_from_positions,
-    _new_tile_id,
-)
-
-from . import assert_equal, assert_identical, raises_regex
-from .test_dataset import create_test_data
-
-
-def assert_combined_tile_ids_equal(dict1, dict2):
-    assert len(dict1) == len(dict2)
-    for k, v in dict1.items():
-        assert k in dict2.keys()
-        assert_equal(dict1[k], dict2[k])
-
-
-class TestTileIDsFromNestedList:
-    def test_1d(self):
-        ds = create_test_data
-        input = [ds(0), ds(1)]
-
-        expected = {(0,): ds(0), (1,): ds(1)}
-        actual = _infer_concat_order_from_positions(input)
-        assert_combined_tile_ids_equal(expected, actual)
-
-    def test_2d(self):
-        ds = create_test_data
-        input = [[ds(0), ds(1)], [ds(2), ds(3)], [ds(4), ds(5)]]
-
-        expected = {
-            (0, 0): ds(0),
-            (0, 1): ds(1),
-            (1, 0): ds(2),
-            (1, 1): ds(3),
-            (2, 0): ds(4),
-            (2, 1): ds(5),
-        }
-        actual = _infer_concat_order_from_positions(input)
-        assert_combined_tile_ids_equal(expected, actual)
-
-    def test_3d(self):
-        ds = create_test_data
-        input = [
-            [[ds(0), ds(1)], [ds(2), ds(3)], [ds(4), ds(5)]],
-            [[ds(6), ds(7)], [ds(8), ds(9)], [ds(10), ds(11)]],
-        ]
-
-        expected = {
-            (0, 0, 0): ds(0),
-            (0, 0, 1): ds(1),
-            (0, 1, 0): ds(2),
-            (0, 1, 1): ds(3),
-            (0, 2, 0): ds(4),
-            (0, 2, 1): ds(5),
-            (1, 0, 0): ds(6),
-            (1, 0, 1): ds(7),
-            (1, 1, 0): ds(8),
-            (1, 1, 1): ds(9),
-            (1, 2, 0): ds(10),
-            (1, 2, 1): ds(11),
-        }
-        actual = _infer_concat_order_from_positions(input)
-        assert_combined_tile_ids_equal(expected, actual)
-
-    def test_single_dataset(self):
-        ds = create_test_data(0)
-        input = [ds]
-
-        expected = {(0,): ds}
-        actual = _infer_concat_order_from_positions(input)
-        assert_combined_tile_ids_equal(expected, actual)
-
-    def test_redundant_nesting(self):
-        ds = create_test_data
-        input = [[ds(0)], [ds(1)]]
-
-        expected = {(0, 0): ds(0), (1, 0): ds(1)}
-        actual = _infer_concat_order_from_positions(input)
-        assert_combined_tile_ids_equal(expected, actual)
-
-    def test_ignore_empty_list(self):
-        ds = create_test_data(0)
-        input = [ds, []]
-        expected = {(0,): ds}
-        actual = _infer_concat_order_from_positions(input)
-        assert_combined_tile_ids_equal(expected, actual)
-
-    def test_uneven_depth_input(self):
-        # Auto_combine won't work on ragged input
-        # but this is just to increase test coverage
-        ds = create_test_data
-        input = [ds(0), [ds(1), ds(2)]]
-
-        expected = {(0,): ds(0), (1, 0): ds(1), (1, 1): ds(2)}
-        actual = _infer_concat_order_from_positions(input)
-        assert_combined_tile_ids_equal(expected, actual)
-
-    def test_uneven_length_input(self):
-        # Auto_combine won't work on ragged input
-        # but this is just to increase test coverage
-        ds = create_test_data
-        input = [[ds(0)], [ds(1), ds(2)]]
-
-        expected = {(0, 0): ds(0), (1, 0): ds(1), (1, 1): ds(2)}
-        actual = _infer_concat_order_from_positions(input)
-        assert_combined_tile_ids_equal(expected, actual)
-
-    def test_infer_from_datasets(self):
-        ds = create_test_data
-        input = [ds(0), ds(1)]
-
-        expected = {(0,): ds(0), (1,): ds(1)}
-        actual = _infer_concat_order_from_positions(input)
-        assert_combined_tile_ids_equal(expected, actual)
-
-
-class TestTileIDsFromCoords:
-    def test_1d(self):
-        ds0 = Dataset({"x": [0, 1]})
-        ds1 = Dataset({"x": [2, 3]})
-
-        expected = {(0,): ds0, (1,): ds1}
-        actual, concat_dims = _infer_concat_order_from_coords([ds1, ds0])
-        assert_combined_tile_ids_equal(expected, actual)
-        assert concat_dims == ["x"]
-
-    def test_2d(self):
-        ds0 = Dataset({"x": [0, 1], "y": [10, 20, 30]})
-        ds1 = Dataset({"x": [2, 3], "y": [10, 20, 30]})
-        ds2 = Dataset({"x": [0, 1], "y": [40, 50, 60]})
-        ds3 = Dataset({"x": [2, 3], "y": [40, 50, 60]})
-        ds4 = Dataset({"x": [0, 1], "y": [70, 80, 90]})
-        ds5 = Dataset({"x": [2, 3], "y": [70, 80, 90]})
-
-        expected = {
-            (0, 0): ds0,
-            (1, 0): ds1,
-            (0, 1): ds2,
-            (1, 1): ds3,
-            (0, 2): ds4,
-            (1, 2): ds5,
-        }
-        actual, concat_dims = _infer_concat_order_from_coords(
-            [ds1, ds0, ds3, ds5, ds2, ds4]
-        )
-        assert_combined_tile_ids_equal(expected, actual)
-        assert concat_dims == ["x", "y"]
-
-    def test_no_dimension_coords(self):
-        ds0 = Dataset({"foo": ("x", [0, 1])})
-        ds1 = Dataset({"foo": ("x", [2, 3])})
-        with raises_regex(ValueError, "Could not find any dimension"):
-            _infer_concat_order_from_coords([ds1, ds0])
-
-    def test_coord_not_monotonic(self):
-        ds0 = Dataset({"x": [0, 1]})
-        ds1 = Dataset({"x": [3, 2]})
-        with raises_regex(
-            ValueError,
-            "Coordinate variable x is neither " "monotonically increasing nor",
-        ):
-            _infer_concat_order_from_coords([ds1, ds0])
-
-    def test_coord_monotonically_decreasing(self):
-        ds0 = Dataset({"x": [3, 2]})
-        ds1 = Dataset({"x": [1, 0]})
-
-        expected = {(0,): ds0, (1,): ds1}
-        actual, concat_dims = _infer_concat_order_from_coords([ds1, ds0])
-        assert_combined_tile_ids_equal(expected, actual)
-        assert concat_dims == ["x"]
-
-    def test_no_concatenation_needed(self):
-        ds = Dataset({"foo": ("x", [0, 1])})
-        expected = {(): ds}
-        actual, concat_dims = _infer_concat_order_from_coords([ds])
-        assert_combined_tile_ids_equal(expected, actual)
-        assert concat_dims == []
-
-    def test_2d_plus_bystander_dim(self):
-        ds0 = Dataset({"x": [0, 1], "y": [10, 20, 30], "t": [0.1, 0.2]})
-        ds1 = Dataset({"x": [2, 3], "y": [10, 20, 30], "t": [0.1, 0.2]})
-        ds2 = Dataset({"x": [0, 1], "y": [40, 50, 60], "t": [0.1, 0.2]})
-        ds3 = Dataset({"x": [2, 3], "y": [40, 50, 60], "t": [0.1, 0.2]})
-
-        expected = {(0, 0): ds0, (1, 0): ds1, (0, 1): ds2, (1, 1): ds3}
-        actual, concat_dims = _infer_concat_order_from_coords([ds1, ds0, ds3, ds2])
-        assert_combined_tile_ids_equal(expected, actual)
-        assert concat_dims == ["x", "y"]
-
-    def test_string_coords(self):
-        ds0 = Dataset({"person": ["Alice", "Bob"]})
-        ds1 = Dataset({"person": ["Caroline", "Daniel"]})
-
-        expected = {(0,): ds0, (1,): ds1}
-        actual, concat_dims = _infer_concat_order_from_coords([ds1, ds0])
-        assert_combined_tile_ids_equal(expected, actual)
-        assert concat_dims == ["person"]
-
-    # Decided against natural sorting of string coords GH #2616
-    def test_lexicographic_sort_string_coords(self):
-        ds0 = Dataset({"simulation": ["run8", "run9"]})
-        ds1 = Dataset({"simulation": ["run10", "run11"]})
-
-        expected = {(0,): ds1, (1,): ds0}
-        actual, concat_dims = _infer_concat_order_from_coords([ds1, ds0])
-        assert_combined_tile_ids_equal(expected, actual)
-        assert concat_dims == ["simulation"]
-
-    def test_datetime_coords(self):
-        ds0 = Dataset({"time": [datetime(2000, 3, 6), datetime(2001, 3, 7)]})
-        ds1 = Dataset({"time": [datetime(1999, 1, 1), datetime(1999, 2, 4)]})
-
-        expected = {(0,): ds1, (1,): ds0}
-        actual, concat_dims = _infer_concat_order_from_coords([ds0, ds1])
-        assert_combined_tile_ids_equal(expected, actual)
-        assert concat_dims == ["time"]
-
-
-@pytest.fixture(scope="module")
-def create_combined_ids():
-    return _create_combined_ids
-
-
-def _create_combined_ids(shape):
-    tile_ids = _create_tile_ids(shape)
-    nums = range(len(tile_ids))
-    return {tile_id: create_test_data(num) for tile_id, num in zip(tile_ids, nums)}
-
-
-def _create_tile_ids(shape):
-    tile_ids = product(*(range(i) for i in shape))
-    return list(tile_ids)
-
-
-class TestNewTileIDs:
-    @pytest.mark.parametrize(
-        "old_id, new_id",
-        [((3, 0, 1), (0, 1)), ((0, 0), (0,)), ((1,), ()), ((0,), ()), ((1, 0), (0,))],
-    )
-    def test_new_tile_id(self, old_id, new_id):
-        ds = create_test_data
-        assert _new_tile_id((old_id, ds)) == new_id
-
-    def test_get_new_tile_ids(self, create_combined_ids):
-        shape = (1, 2, 3)
-        combined_ids = create_combined_ids(shape)
-
-        expected_tile_ids = sorted(combined_ids.keys())
-        actual_tile_ids = _create_tile_ids(shape)
-        assert expected_tile_ids == actual_tile_ids
-
-
-class TestCombineND:
-    @pytest.mark.parametrize("concat_dim", ["dim1", "new_dim"])
-    def test_concat_once(self, create_combined_ids, concat_dim):
-        shape = (2,)
-        combined_ids = create_combined_ids(shape)
-        ds = create_test_data
-        result = _combine_all_along_first_dim(
-            combined_ids,
-            dim=concat_dim,
-            data_vars="all",
-            coords="different",
-            compat="no_conflicts",
-        )
-
-        expected_ds = concat([ds(0), ds(1)], dim=concat_dim)
-        assert_combined_tile_ids_equal(result, {(): expected_ds})
-
-    def test_concat_only_first_dim(self, create_combined_ids):
-        shape = (2, 3)
-        combined_ids = create_combined_ids(shape)
-        result = _combine_all_along_first_dim(
-            combined_ids,
-            dim="dim1",
-            data_vars="all",
-            coords="different",
-            compat="no_conflicts",
-        )
-
-        ds = create_test_data
-        partway1 = concat([ds(0), ds(3)], dim="dim1")
-        partway2 = concat([ds(1), ds(4)], dim="dim1")
-        partway3 = concat([ds(2), ds(5)], dim="dim1")
-        expected_datasets = [partway1, partway2, partway3]
-        expected = {(i,): ds for i, ds in enumerate(expected_datasets)}
-
-        assert_combined_tile_ids_equal(result, expected)
-
-    @pytest.mark.parametrize("concat_dim", ["dim1", "new_dim"])
-    def test_concat_twice(self, create_combined_ids, concat_dim):
-        shape = (2, 3)
-        combined_ids = create_combined_ids(shape)
-        result = _combine_nd(combined_ids, concat_dims=["dim1", concat_dim])
-
-        ds = create_test_data
-        partway1 = concat([ds(0), ds(3)], dim="dim1")
-        partway2 = concat([ds(1), ds(4)], dim="dim1")
-        partway3 = concat([ds(2), ds(5)], dim="dim1")
-        expected = concat([partway1, partway2, partway3], dim=concat_dim)
-
-        assert_equal(result, expected)
-
-
-class TestCheckShapeTileIDs:
-    def test_check_depths(self):
-        ds = create_test_data(0)
-        combined_tile_ids = {(0,): ds, (0, 1): ds}
-        with raises_regex(ValueError, "sub-lists do not have " "consistent depths"):
-            _check_shape_tile_ids(combined_tile_ids)
-
-    def test_check_lengths(self):
-        ds = create_test_data(0)
-        combined_tile_ids = {(0, 0): ds, (0, 1): ds, (0, 2): ds, (1, 0): ds, (1, 1): ds}
-        with raises_regex(ValueError, "sub-lists do not have " "consistent lengths"):
-            _check_shape_tile_ids(combined_tile_ids)
-
-
-class TestNestedCombine:
-    def test_nested_concat(self):
-        objs = [Dataset({"x": [0]}), Dataset({"x": [1]})]
-        expected = Dataset({"x": [0, 1]})
-        actual = combine_nested(objs, concat_dim="x")
-        assert_identical(expected, actual)
-        actual = combine_nested(objs, concat_dim=["x"])
-        assert_identical(expected, actual)
-
-        actual = combine_nested([actual], concat_dim=None)
-        assert_identical(expected, actual)
-
-        actual = combine_nested([actual], concat_dim="x")
-        assert_identical(expected, actual)
-
-        objs = [Dataset({"x": [0, 1]}), Dataset({"x": [2]})]
-        actual = combine_nested(objs, concat_dim="x")
-        expected = Dataset({"x": [0, 1, 2]})
-        assert_identical(expected, actual)
-
-        # ensure combine_nested handles non-sorted variables
-        objs = [
-            Dataset(OrderedDict([("x", ("a", [0])), ("y", ("a", [0]))])),
-            Dataset(OrderedDict([("y", ("a", [1])), ("x", ("a", [1]))])),
-        ]
-        actual = combine_nested(objs, concat_dim="a")
-        expected = Dataset({"x": ("a", [0, 1]), "y": ("a", [0, 1])})
-        assert_identical(expected, actual)
-
-        objs = [Dataset({"x": [0], "y": [0]}), Dataset({"x": [0]})]
-        with pytest.raises(KeyError):
-            combine_nested(objs, concat_dim="x")
-
-    @pytest.mark.parametrize(
-        "join, expected",
-        [
-            ("outer", Dataset({"x": [0, 1], "y": [0, 1]})),
-            ("inner", Dataset({"x": [0, 1], "y": []})),
-            ("left", Dataset({"x": [0, 1], "y": [0]})),
-            ("right", Dataset({"x": [0, 1], "y": [1]})),
-        ],
-    )
-    def test_combine_nested_join(self, join, expected):
-        objs = [Dataset({"x": [0], "y": [0]}), Dataset({"x": [1], "y": [1]})]
-        actual = combine_nested(objs, concat_dim="x", join=join)
-        assert_identical(expected, actual)
-
-    def test_combine_nested_join_exact(self):
-        objs = [Dataset({"x": [0], "y": [0]}), Dataset({"x": [1], "y": [1]})]
-        with raises_regex(ValueError, "indexes along dimension"):
-            combine_nested(objs, concat_dim="x", join="exact")
-
-    def test_empty_input(self):
-        assert_identical(Dataset(), combine_nested([], concat_dim="x"))
-
-    # Fails because of concat's weird treatment of dimension coords, see #2975
-    @pytest.mark.xfail
-    def test_nested_concat_too_many_dims_at_once(self):
-        objs = [Dataset({"x": [0], "y": [1]}), Dataset({"y": [0], "x": [1]})]
-        with pytest.raises(ValueError, match="not equal across datasets"):
-            combine_nested(objs, concat_dim="x", coords="minimal")
-
-    def test_nested_concat_along_new_dim(self):
-        objs = [
-            Dataset({"a": ("x", [10]), "x": [0]}),
-            Dataset({"a": ("x", [20]), "x": [0]}),
-        ]
-        expected = Dataset({"a": (("t", "x"), [[10], [20]]), "x": [0]})
-        actual = combine_nested(objs, concat_dim="t")
-        assert_identical(expected, actual)
-
-        # Same but with a DataArray as new dim, see GH #1988 and #2647
-        dim = DataArray([100, 150], name="baz", dims="baz")
-        expected = Dataset(
-            {"a": (("baz", "x"), [[10], [20]]), "x": [0], "baz": [100, 150]}
-        )
-        actual = combine_nested(objs, concat_dim=dim)
-        assert_identical(expected, actual)
-
-    def test_nested_merge(self):
-        data = Dataset({"x": 0})
-        actual = combine_nested([data, data, data], concat_dim=None)
-        assert_identical(data, actual)
-
-        ds1 = Dataset({"a": ("x", [1, 2]), "x": [0, 1]})
-        ds2 = Dataset({"a": ("x", [2, 3]), "x": [1, 2]})
-        expected = Dataset({"a": ("x", [1, 2, 3]), "x": [0, 1, 2]})
-        actual = combine_nested([ds1, ds2], concat_dim=None)
-        assert_identical(expected, actual)
-        actual = combine_nested([ds1, ds2], concat_dim=[None])
-        assert_identical(expected, actual)
-
-        tmp1 = Dataset({"x": 0})
-        tmp2 = Dataset({"x": np.nan})
-        actual = combine_nested([tmp1, tmp2], concat_dim=None)
-        assert_identical(tmp1, actual)
-        actual = combine_nested([tmp1, tmp2], concat_dim=[None])
-        assert_identical(tmp1, actual)
-
-        # Single object, with a concat_dim explicitly provided
-        # Test the issue reported in GH #1988
-        objs = [Dataset({"x": 0, "y": 1})]
-        dim = DataArray([100], name="baz", dims="baz")
-        actual = combine_nested(objs, concat_dim=[dim])
-        expected = Dataset({"x": ("baz", [0]), "y": ("baz", [1])}, {"baz": [100]})
-        assert_identical(expected, actual)
-
-        # Just making sure that auto_combine is doing what is
-        # expected for non-scalar values, too.
-        objs = [Dataset({"x": ("z", [0, 1]), "y": ("z", [1, 2])})]
-        dim = DataArray([100], name="baz", dims="baz")
-        actual = combine_nested(objs, concat_dim=[dim])
-        expected = Dataset(
-            {"x": (("baz", "z"), [[0, 1]]), "y": (("baz", "z"), [[1, 2]])},
-            {"baz": [100]},
-        )
-        assert_identical(expected, actual)
-
-    def test_concat_multiple_dims(self):
-        objs = [
-            [Dataset({"a": (("x", "y"), [[0]])}), Dataset({"a": (("x", "y"), [[1]])})],
-            [Dataset({"a": (("x", "y"), [[2]])}), Dataset({"a": (("x", "y"), [[3]])})],
-        ]
-        actual = combine_nested(objs, concat_dim=["x", "y"])
-        expected = Dataset({"a": (("x", "y"), [[0, 1], [2, 3]])})
-        assert_identical(expected, actual)
-
-    def test_concat_name_symmetry(self):
-        """Inspired by the discussion on GH issue #2777"""
-
-        da1 = DataArray(name="a", data=[[0]], dims=["x", "y"])
-        da2 = DataArray(name="b", data=[[1]], dims=["x", "y"])
-        da3 = DataArray(name="a", data=[[2]], dims=["x", "y"])
-        da4 = DataArray(name="b", data=[[3]], dims=["x", "y"])
-
-        x_first = combine_nested([[da1, da2], [da3, da4]], concat_dim=["x", "y"])
-        y_first = combine_nested([[da1, da3], [da2, da4]], concat_dim=["y", "x"])
-
-        assert_identical(x_first, y_first)
-
-    def test_concat_one_dim_merge_another(self):
-        data = create_test_data()
-        data1 = data.copy(deep=True)
-        data2 = data.copy(deep=True)
-
-        objs = [
-            [data1.var1.isel(dim2=slice(4)), data2.var1.isel(dim2=slice(4, 9))],
-            [data1.var2.isel(dim2=slice(4)), data2.var2.isel(dim2=slice(4, 9))],
-        ]
-
-        expected = data[["var1", "var2"]]
-        actual = combine_nested(objs, concat_dim=[None, "dim2"])
-        assert expected.identical(actual)
-
-    def test_auto_combine_2d(self):
-        ds = create_test_data
-
-        partway1 = concat([ds(0), ds(3)], dim="dim1")
-        partway2 = concat([ds(1), ds(4)], dim="dim1")
-        partway3 = concat([ds(2), ds(5)], dim="dim1")
-        expected = concat([partway1, partway2, partway3], dim="dim2")
-
-        datasets = [[ds(0), ds(1), ds(2)], [ds(3), ds(4), ds(5)]]
-        result = combine_nested(datasets, concat_dim=["dim1", "dim2"])
-        assert_equal(result, expected)
-
-    def test_combine_nested_missing_data_new_dim(self):
-        # Your data includes "time" and "station" dimensions, and each year's
-        # data has a different set of stations.
-        datasets = [
-            Dataset({"a": ("x", [2, 3]), "x": [1, 2]}),
-            Dataset({"a": ("x", [1, 2]), "x": [0, 1]}),
-        ]
-        expected = Dataset(
-            {"a": (("t", "x"), [[np.nan, 2, 3], [1, 2, np.nan]])}, {"x": [0, 1, 2]}
-        )
-        actual = combine_nested(datasets, concat_dim="t")
-        assert_identical(expected, actual)
-
-    def test_invalid_hypercube_input(self):
-        ds = create_test_data
-
-        datasets = [[ds(0), ds(1), ds(2)], [ds(3), ds(4)]]
-        with raises_regex(ValueError, "sub-lists do not have " "consistent lengths"):
-            combine_nested(datasets, concat_dim=["dim1", "dim2"])
-
-        datasets = [[ds(0), ds(1)], [[ds(3), ds(4)]]]
-        with raises_regex(ValueError, "sub-lists do not have " "consistent depths"):
-            combine_nested(datasets, concat_dim=["dim1", "dim2"])
-
-        datasets = [[ds(0), ds(1)], [ds(3), ds(4)]]
-        with raises_regex(ValueError, "concat_dims has length"):
-            combine_nested(datasets, concat_dim=["dim1"])
-
-    def test_merge_one_dim_concat_another(self):
-        objs = [
-            [Dataset({"foo": ("x", [0, 1])}), Dataset({"bar": ("x", [10, 20])})],
-            [Dataset({"foo": ("x", [2, 3])}), Dataset({"bar": ("x", [30, 40])})],
-        ]
-        expected = Dataset({"foo": ("x", [0, 1, 2, 3]), "bar": ("x", [10, 20, 30, 40])})
-
-        actual = combine_nested(objs, concat_dim=["x", None], compat="equals")
-        assert_identical(expected, actual)
-
-        # Proving it works symmetrically
-        objs = [
-            [Dataset({"foo": ("x", [0, 1])}), Dataset({"foo": ("x", [2, 3])})],
-            [Dataset({"bar": ("x", [10, 20])}), Dataset({"bar": ("x", [30, 40])})],
-        ]
-        actual = combine_nested(objs, concat_dim=[None, "x"], compat="equals")
-        assert_identical(expected, actual)
-
-    def test_combine_concat_over_redundant_nesting(self):
-        objs = [[Dataset({"x": [0]}), Dataset({"x": [1]})]]
-        actual = combine_nested(objs, concat_dim=[None, "x"])
-        expected = Dataset({"x": [0, 1]})
-        assert_identical(expected, actual)
-
-        objs = [[Dataset({"x": [0]})], [Dataset({"x": [1]})]]
-        actual = combine_nested(objs, concat_dim=["x", None])
-        expected = Dataset({"x": [0, 1]})
-        assert_identical(expected, actual)
-
-        objs = [[Dataset({"x": [0]})]]
-        actual = combine_nested(objs, concat_dim=[None, None])
-        expected = Dataset({"x": [0]})
-        assert_identical(expected, actual)
-
-    def test_combine_nested_but_need_auto_combine(self):
-        objs = [Dataset({"x": [0, 1]}), Dataset({"x": [2], "wall": [0]})]
-        with raises_regex(ValueError, "cannot be combined"):
-            combine_nested(objs, concat_dim="x")
-
-    @pytest.mark.parametrize("fill_value", [dtypes.NA, 2, 2.0])
-    def test_combine_nested_fill_value(self, fill_value):
-        datasets = [
-            Dataset({"a": ("x", [2, 3]), "x": [1, 2]}),
-            Dataset({"a": ("x", [1, 2]), "x": [0, 1]}),
-        ]
-        if fill_value == dtypes.NA:
-            # if we supply the default, we expect the missing value for a
-            # float array
-            fill_value = np.nan
-        expected = Dataset(
-            {"a": (("t", "x"), [[fill_value, 2, 3], [1, 2, fill_value]])},
-            {"x": [0, 1, 2]},
-        )
-        actual = combine_nested(datasets, concat_dim="t", fill_value=fill_value)
-        assert_identical(expected, actual)
-
-
-class TestCombineAuto:
-    def test_combine_by_coords(self):
-        objs = [Dataset({"x": [0]}), Dataset({"x": [1]})]
-        actual = combine_by_coords(objs)
-        expected = Dataset({"x": [0, 1]})
-        assert_identical(expected, actual)
-
-        actual = combine_by_coords([actual])
-        assert_identical(expected, actual)
-
-        objs = [Dataset({"x": [0, 1]}), Dataset({"x": [2]})]
-        actual = combine_by_coords(objs)
-        expected = Dataset({"x": [0, 1, 2]})
-        assert_identical(expected, actual)
-
-        # ensure auto_combine handles non-sorted variables
-        objs = [
-            Dataset({"x": ("a", [0]), "y": ("a", [0]), "a": [0]}),
-            Dataset({"x": ("a", [1]), "y": ("a", [1]), "a": [1]}),
-        ]
-        actual = combine_by_coords(objs)
-        expected = Dataset({"x": ("a", [0, 1]), "y": ("a", [0, 1]), "a": [0, 1]})
-        assert_identical(expected, actual)
-
-        objs = [Dataset({"x": [0], "y": [0]}), Dataset({"y": [1], "x": [1]})]
-        actual = combine_by_coords(objs)
-        expected = Dataset({"x": [0, 1], "y": [0, 1]})
-        assert_equal(actual, expected)
-
-        objs = [Dataset({"x": 0}), Dataset({"x": 1})]
-        with raises_regex(ValueError, "Could not find any dimension " "coordinates"):
-            combine_by_coords(objs)
-
-        objs = [Dataset({"x": [0], "y": [0]}), Dataset({"x": [0]})]
-        with raises_regex(ValueError, "Every dimension needs a coordinate"):
-            combine_by_coords(objs)
-
-        def test_empty_input(self):
-            assert_identical(Dataset(), combine_by_coords([]))
-
-    @pytest.mark.parametrize(
-        "join, expected",
-        [
-            ("outer", Dataset({"x": [0, 1], "y": [0, 1]})),
-            ("inner", Dataset({"x": [0, 1], "y": []})),
-            ("left", Dataset({"x": [0, 1], "y": [0]})),
-            ("right", Dataset({"x": [0, 1], "y": [1]})),
-        ],
-    )
-    def test_combine_coords_join(self, join, expected):
-        objs = [Dataset({"x": [0], "y": [0]}), Dataset({"x": [1], "y": [1]})]
-        actual = combine_nested(objs, concat_dim="x", join=join)
-        assert_identical(expected, actual)
-
-    def test_combine_coords_join_exact(self):
-        objs = [Dataset({"x": [0], "y": [0]}), Dataset({"x": [1], "y": [1]})]
-        with raises_regex(ValueError, "indexes along dimension"):
-            combine_nested(objs, concat_dim="x", join="exact")
-
-    def test_infer_order_from_coords(self):
-        data = create_test_data()
-        objs = [data.isel(dim2=slice(4, 9)), data.isel(dim2=slice(4))]
-        actual = combine_by_coords(objs)
-        expected = data
-        assert expected.broadcast_equals(actual)
-
-    def test_combine_leaving_bystander_dimensions(self):
-        # Check non-monotonic bystander dimension coord doesn't raise
-        # ValueError on combine (https://github.com/pydata/xarray/issues/3150)
-        ycoord = ["a", "c", "b"]
-
-        data = np.random.rand(7, 3)
-
-        ds1 = Dataset(
-            data_vars=dict(data=(["x", "y"], data[:3, :])),
-            coords=dict(x=[1, 2, 3], y=ycoord),
-        )
-
-        ds2 = Dataset(
-            data_vars=dict(data=(["x", "y"], data[3:, :])),
-            coords=dict(x=[4, 5, 6, 7], y=ycoord),
-        )
-
-        expected = Dataset(
-            data_vars=dict(data=(["x", "y"], data)),
-            coords=dict(x=[1, 2, 3, 4, 5, 6, 7], y=ycoord),
-        )
-
-        actual = combine_by_coords((ds1, ds2))
-        assert_identical(expected, actual)
-
-    def test_combine_by_coords_previously_failed(self):
-        # In the above scenario, one file is missing, containing the data for
-        # one year's data for one variable.
-        datasets = [
-            Dataset({"a": ("x", [0]), "x": [0]}),
-            Dataset({"b": ("x", [0]), "x": [0]}),
-            Dataset({"a": ("x", [1]), "x": [1]}),
-        ]
-        expected = Dataset({"a": ("x", [0, 1]), "b": ("x", [0, np.nan])}, {"x": [0, 1]})
-        actual = combine_by_coords(datasets)
-        assert_identical(expected, actual)
-
-    def test_combine_by_coords_still_fails(self):
-        # concat can't handle new variables (yet):
-        # https://github.com/pydata/xarray/issues/508
-        datasets = [Dataset({"x": 0}, {"y": 0}), Dataset({"x": 1}, {"y": 1, "z": 1})]
-        with pytest.raises(ValueError):
-            combine_by_coords(datasets, "y")
-
-    def test_combine_by_coords_no_concat(self):
-        objs = [Dataset({"x": 0}), Dataset({"y": 1})]
-        actual = combine_by_coords(objs)
-        expected = Dataset({"x": 0, "y": 1})
-        assert_identical(expected, actual)
-
-        objs = [Dataset({"x": 0, "y": 1}), Dataset({"y": np.nan, "z": 2})]
-        actual = combine_by_coords(objs)
-        expected = Dataset({"x": 0, "y": 1, "z": 2})
-        assert_identical(expected, actual)
-
-    def test_check_for_impossible_ordering(self):
-        ds0 = Dataset({"x": [0, 1, 5]})
-        ds1 = Dataset({"x": [2, 3]})
-        with raises_regex(
-            ValueError, "does not have monotonic global indexes" " along dimension x"
-        ):
-            combine_by_coords([ds1, ds0])
-
-
-@pytest.mark.filterwarnings(
-    "ignore:In xarray version 0.13 `auto_combine` " "will be deprecated"
-)
-@pytest.mark.filterwarnings("ignore:Also `open_mfdataset` will no longer")
-@pytest.mark.filterwarnings("ignore:The datasets supplied")
-class TestAutoCombineOldAPI:
-    """
-    Set of tests which check that old 1-dimensional auto_combine behaviour is
-    still satisfied. #2616
-    """
-
-    def test_auto_combine(self):
-        objs = [Dataset({"x": [0]}), Dataset({"x": [1]})]
-        actual = auto_combine(objs)
-        expected = Dataset({"x": [0, 1]})
-        assert_identical(expected, actual)
-
-        actual = auto_combine([actual])
-        assert_identical(expected, actual)
-
-        objs = [Dataset({"x": [0, 1]}), Dataset({"x": [2]})]
-        actual = auto_combine(objs)
-        expected = Dataset({"x": [0, 1, 2]})
-        assert_identical(expected, actual)
-
-        # ensure auto_combine handles non-sorted variables
-        objs = [
-            Dataset(OrderedDict([("x", ("a", [0])), ("y", ("a", [0]))])),
-            Dataset(OrderedDict([("y", ("a", [1])), ("x", ("a", [1]))])),
-        ]
-        actual = auto_combine(objs)
-        expected = Dataset({"x": ("a", [0, 1]), "y": ("a", [0, 1])})
-        assert_identical(expected, actual)
-
-        objs = [Dataset({"x": [0], "y": [0]}), Dataset({"y": [1], "x": [1]})]
-        with raises_regex(ValueError, "too many .* dimensions"):
-            auto_combine(objs)
-
-        objs = [Dataset({"x": 0}), Dataset({"x": 1})]
-        with raises_regex(ValueError, "cannot infer dimension"):
-            auto_combine(objs)
-
-        objs = [Dataset({"x": [0], "y": [0]}), Dataset({"x": [0]})]
-        with pytest.raises(KeyError):
-            auto_combine(objs)
-
-    def test_auto_combine_previously_failed(self):
-        # In the above scenario, one file is missing, containing the data for
-        # one year's data for one variable.
-        datasets = [
-            Dataset({"a": ("x", [0]), "x": [0]}),
-            Dataset({"b": ("x", [0]), "x": [0]}),
-            Dataset({"a": ("x", [1]), "x": [1]}),
-        ]
-        expected = Dataset({"a": ("x", [0, 1]), "b": ("x", [0, np.nan])}, {"x": [0, 1]})
-        actual = auto_combine(datasets)
-        assert_identical(expected, actual)
-
-        # Your data includes "time" and "station" dimensions, and each year's
-        # data has a different set of stations.
-        datasets = [
-            Dataset({"a": ("x", [2, 3]), "x": [1, 2]}),
-            Dataset({"a": ("x", [1, 2]), "x": [0, 1]}),
-        ]
-        expected = Dataset(
-            {"a": (("t", "x"), [[np.nan, 2, 3], [1, 2, np.nan]])}, {"x": [0, 1, 2]}
-        )
-        actual = auto_combine(datasets, concat_dim="t")
-        assert_identical(expected, actual)
-
-    def test_auto_combine_still_fails(self):
-        # concat can't handle new variables (yet):
-        # https://github.com/pydata/xarray/issues/508
-        datasets = [Dataset({"x": 0}, {"y": 0}), Dataset({"x": 1}, {"y": 1, "z": 1})]
-        with pytest.raises(ValueError):
-            auto_combine(datasets, "y")
-
-    def test_auto_combine_no_concat(self):
-        objs = [Dataset({"x": 0}), Dataset({"y": 1})]
-        actual = auto_combine(objs)
-        expected = Dataset({"x": 0, "y": 1})
-        assert_identical(expected, actual)
-
-        objs = [Dataset({"x": 0, "y": 1}), Dataset({"y": np.nan, "z": 2})]
-        actual = auto_combine(objs)
-        expected = Dataset({"x": 0, "y": 1, "z": 2})
-        assert_identical(expected, actual)
-
-        data = Dataset({"x": 0})
-        actual = auto_combine([data, data, data], concat_dim=None)
-        assert_identical(data, actual)
-
-        # Single object, with a concat_dim explicitly provided
-        # Test the issue reported in GH #1988
-        objs = [Dataset({"x": 0, "y": 1})]
-        dim = DataArray([100], name="baz", dims="baz")
-        actual = auto_combine(objs, concat_dim=dim)
-        expected = Dataset({"x": ("baz", [0]), "y": ("baz", [1])}, {"baz": [100]})
-        assert_identical(expected, actual)
-
-        # Just making sure that auto_combine is doing what is
-        # expected for non-scalar values, too.
-        objs = [Dataset({"x": ("z", [0, 1]), "y": ("z", [1, 2])})]
-        dim = DataArray([100], name="baz", dims="baz")
-        actual = auto_combine(objs, concat_dim=dim)
-        expected = Dataset(
-            {"x": (("baz", "z"), [[0, 1]]), "y": (("baz", "z"), [[1, 2]])},
-            {"baz": [100]},
-        )
-        assert_identical(expected, actual)
-
-    def test_auto_combine_order_by_appearance_not_coords(self):
-        objs = [
-            Dataset({"foo": ("x", [0])}, coords={"x": ("x", [1])}),
-            Dataset({"foo": ("x", [1])}, coords={"x": ("x", [0])}),
-        ]
-        actual = auto_combine(objs)
-        expected = Dataset({"foo": ("x", [0, 1])}, coords={"x": ("x", [1, 0])})
-        assert_identical(expected, actual)
-
-    @pytest.mark.parametrize("fill_value", [dtypes.NA, 2, 2.0])
-    def test_auto_combine_fill_value(self, fill_value):
-        datasets = [
-            Dataset({"a": ("x", [2, 3]), "x": [1, 2]}),
-            Dataset({"a": ("x", [1, 2]), "x": [0, 1]}),
-        ]
-        if fill_value == dtypes.NA:
-            # if we supply the default, we expect the missing value for a
-            # float array
-            fill_value = np.nan
-        expected = Dataset(
-            {"a": (("t", "x"), [[fill_value, 2, 3], [1, 2, fill_value]])},
-            {"x": [0, 1, 2]},
-        )
-        actual = auto_combine(datasets, concat_dim="t", fill_value=fill_value)
-        assert_identical(expected, actual)
-
-
-class TestAutoCombineDeprecation:
-    """
-    Set of tests to check that FutureWarnings are correctly raised until the
-    deprecation cycle is complete. #2616
-    """
-
-    def test_auto_combine_with_concat_dim(self):
-        objs = [Dataset({"x": [0]}), Dataset({"x": [1]})]
-        with pytest.warns(FutureWarning, match="`concat_dim`"):
-            auto_combine(objs, concat_dim="x")
-
-    def test_auto_combine_with_merge_and_concat(self):
-        objs = [Dataset({"x": [0]}), Dataset({"x": [1]}), Dataset({"z": ((), 99)})]
-        with pytest.warns(FutureWarning, match="require both concatenation"):
-            auto_combine(objs)
-
-    def test_auto_combine_with_coords(self):
-        objs = [
-            Dataset({"foo": ("x", [0])}, coords={"x": ("x", [0])}),
-            Dataset({"foo": ("x", [1])}, coords={"x": ("x", [1])}),
-        ]
-        with pytest.warns(FutureWarning, match="supplied have global"):
-            auto_combine(objs)
-
-    def test_auto_combine_without_coords(self):
-        objs = [Dataset({"foo": ("x", [0])}), Dataset({"foo": ("x", [1])})]
-        with pytest.warns(FutureWarning, match="supplied do not have global"):
-            auto_combine(objs)
diff --git a/xarray/tests/test_concat.py b/xarray/tests/test_concat.py
index ee99ca02..00428f70 100644
--- a/xarray/tests/test_concat.py
+++ b/xarray/tests/test_concat.py
@@ -5,8 +5,7 @@ import pandas as pd
 import pytest

 from xarray import DataArray, Dataset, Variable, concat
-from xarray.core import dtypes
-
+from xarray.core import dtypes, merge
 from . import (
     InaccessibleArray,
     assert_array_equal,
@@ -18,6 +17,34 @@ from . import (
 from .test_dataset import create_test_data


+def test_concat_compat():
+    ds1 = Dataset(
+        {
+            "has_x_y": (("y", "x"), [[1, 2]]),
+            "has_x": ("x", [1, 2]),
+            "no_x_y": ("z", [1, 2]),
+        },
+        coords={"x": [0, 1], "y": [0], "z": [-1, -2]},
+    )
+    ds2 = Dataset(
+        {
+            "has_x_y": (("y", "x"), [[3, 4]]),
+            "has_x": ("x", [1, 2]),
+            "no_x_y": (("q", "z"), [[1, 2]]),
+        },
+        coords={"x": [0, 1], "y": [1], "z": [-1, -2], "q": [0]},
+    )
+
+    result = concat([ds1, ds2], dim="y", data_vars="minimal", compat="broadcast_equals")
+    assert_equal(ds2.no_x_y, result.no_x_y.transpose())
+
+    for var in ["has_x", "no_x_y"]:
+        assert "y" not in result[var]
+
+    with raises_regex(ValueError, "'q' is not present in all datasets"):
+        concat([ds1, ds2], dim="q", data_vars="all", compat="broadcast_equals")
+
+
 class TestConcatDataset:
     @pytest.fixture
     def data(self):
@@ -92,7 +119,7 @@ class TestConcatDataset:
             actual = concat(objs, dim="x", coords=coords)
             assert_identical(expected, actual)
         for coords in ["minimal", []]:
-            with raises_regex(ValueError, "not equal across"):
+            with raises_regex(merge.MergeError, "conflicting values"):
                 concat(objs, dim="x", coords=coords)

     def test_concat_constant_index(self):
@@ -103,8 +130,10 @@ class TestConcatDataset:
         for mode in ["different", "all", ["foo"]]:
             actual = concat([ds1, ds2], "y", data_vars=mode)
             assert_identical(expected, actual)
-        with raises_regex(ValueError, "not equal across datasets"):
-            concat([ds1, ds2], "y", data_vars="minimal")
+        with raises_regex(merge.MergeError, "conflicting values"):
+            # previously dim="y", and raised error which makes no sense.
+            # "foo" has dimension "y" so minimal should concatenate it?
+            concat([ds1, ds2], "new_dim", data_vars="minimal")

     def test_concat_size0(self):
         data = create_test_data()
@@ -134,6 +163,14 @@ class TestConcatDataset:
         data = create_test_data()
         split_data = [data.isel(dim1=slice(3)), data.isel(dim1=slice(3, None))]

+        with raises_regex(ValueError, "must supply at least one"):
+            concat([], "dim1")
+
+        with raises_regex(ValueError, "Cannot specify both .*='different'"):
+            concat(
+                [data, data], dim="concat_dim", data_vars="different", compat="override"
+            )
+
         with raises_regex(ValueError, "must supply at least one"):
             concat([], "dim1")

@@ -146,7 +183,7 @@ class TestConcatDataset:
             concat([data0, data1], "dim1", compat="identical")
         assert_identical(data, concat([data0, data1], "dim1", compat="equals"))

-        with raises_regex(ValueError, "encountered unexpected"):
+        with raises_regex(ValueError, "present in some datasets"):
             data0, data1 = deepcopy(split_data)
             data1["foo"] = ("bar", np.random.randn(10))
             concat([data0, data1], "dim1")
diff --git a/xarray/tests/test_dask.py b/xarray/tests/test_dask.py
index d1057654..76b3ed1a 100644
--- a/xarray/tests/test_dask.py
+++ b/xarray/tests/test_dask.py
@@ -825,7 +825,6 @@ def kernel(name):
     """Dask kernel to test pickling/unpickling and __repr__.
     Must be global to make it pickleable.
     """
-    print("kernel(%s)" % name)
     global kernel_call_count
     kernel_call_count += 1
     return np.ones(1, dtype=np.int64)
diff --git a/xarray/tests/test_merge.py b/xarray/tests/test_merge.py
index ed1453ce..c1e6c7a5 100644
--- a/xarray/tests/test_merge.py
+++ b/xarray/tests/test_merge.py
@@ -196,6 +196,8 @@ class TestMergeMethod:
         with raises_regex(ValueError, "compat=.* invalid"):
             ds1.merge(ds2, compat="foobar")

+        assert ds1.identical(ds1.merge(ds2, compat="override"))
+
     def test_merge_auto_align(self):
         ds1 = xr.Dataset({"a": ("x", [1, 2]), "x": [0, 1]})
         ds2 = xr.Dataset({"b": ("x", [3, 4]), "x": [1, 2]})
Obtaining file:///testbed
  Preparing metadata (setup.py): started
  Preparing metadata (setup.py): finished with status 'done'
Requirement already satisfied: numpy>=1.12 in /opt/miniconda3/envs/testbed/lib/python3.10/site-packages (from xarray==0.12.3+93.g2c8ba46b.dirty) (1.23.0)
Requirement already satisfied: pandas>=0.19.2 in /opt/miniconda3/envs/testbed/lib/python3.10/site-packages (from xarray==0.12.3+93.g2c8ba46b.dirty) (1.5.3)
Requirement already satisfied: python-dateutil>=2.8.1 in /opt/miniconda3/envs/testbed/lib/python3.10/site-packages (from pandas>=0.19.2->xarray==0.12.3+93.g2c8ba46b.dirty) (2.8.2)
Requirement already satisfied: pytz>=2020.1 in /opt/miniconda3/envs/testbed/lib/python3.10/site-packages (from pandas>=0.19.2->xarray==0.12.3+93.g2c8ba46b.dirty) (2023.3)
Requirement already satisfied: six>=1.5 in /opt/miniconda3/envs/testbed/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas>=0.19.2->xarray==0.12.3+93.g2c8ba46b.dirty) (1.16.0)
Installing collected packages: xarray
  Attempting uninstall: xarray
    Found existing installation: xarray 0.12.3+93.g2c8ba46b.dirty
    Uninstalling xarray-0.12.3+93.g2c8ba46b.dirty:
      Successfully uninstalled xarray-0.12.3+93.g2c8ba46b.dirty
  DEPRECATION: Legacy editable install of xarray==0.12.3+93.g2c8ba46b.dirty from file:///testbed (setup.py develop) is deprecated. pip 25.0 will enforce this behaviour change. A possible replacement is to add a pyproject.toml or enable --use-pep517, and use setuptools >= 64. If the resulting installation is not behaving as expected, try using --config-settings editable_mode=compat. Please consult the setuptools documentation for more information. Discussion can be found at https://github.com/pypa/pip/issues/11457
  Running setup.py develop for xarray
Successfully installed xarray
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.
============================= test session starts ==============================
platform linux -- Python 3.10.14, pytest-7.4.0, pluggy-1.5.0
rootdir: /testbed
configfile: setup.cfg
plugins: env-1.1.3, cov-5.0.0, hypothesis-6.108.5, xdist-3.6.1
collected 8 items

xarray/tests/test_merge.py ........                                      [100%]

=============================== warnings summary ===============================
xarray/core/dask_array_ops.py:12
xarray/core/dask_array_ops.py:12
  /testbed/xarray/core/dask_array_ops.py:12: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    if LooseVersion(dask.__version__) <= LooseVersion("0.18.2"):

xarray/core/npcompat.py:136
xarray/core/npcompat.py:136
  /testbed/xarray/core/npcompat.py:136: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    if LooseVersion(np.__version__) >= LooseVersion("1.13"):

xarray/core/dask_array_compat.py:45
xarray/core/dask_array_compat.py:45
  /testbed/xarray/core/dask_array_compat.py:45: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    if LooseVersion(dask_version) > LooseVersion("0.19.2"):

xarray/core/pdcompat.py:46
  /testbed/xarray/core/pdcompat.py:46: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    if LooseVersion(pd.__version__) < "0.25.0":

../opt/miniconda3/envs/testbed/lib/python3.10/site-packages/setuptools/_distutils/version.py:345
../opt/miniconda3/envs/testbed/lib/python3.10/site-packages/setuptools/_distutils/version.py:345
  /opt/miniconda3/envs/testbed/lib/python3.10/site-packages/setuptools/_distutils/version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    other = LooseVersion(other)

xarray/plot/utils.py:18
xarray/plot/utils.py:18
  /testbed/xarray/plot/utils.py:18: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    if LooseVersion(nc_time_axis.__version__) < LooseVersion("1.2.0"):

xarray/tests/__init__.py:60: 15 warnings
  /testbed/xarray/tests/__init__.py:60: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    return version.LooseVersion(vstring)

../opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pydap/lib.py:5
  /opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pydap/lib.py:5: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html
    from pkg_resources import get_distribution

../opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pkg_resources/__init__.py:2871
  /opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`.
  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
    declare_namespace(pkg)

../opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pkg_resources/__init__.py:2871
  /opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap.responses')`.
  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
    declare_namespace(pkg)

../opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pkg_resources/__init__.py:2350
../opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pkg_resources/__init__.py:2350
../opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pkg_resources/__init__.py:2350
  /opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pkg_resources/__init__.py:2350: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap')`.
  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
    declare_namespace(parent)

../opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pkg_resources/__init__.py:2871
  /opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap.handlers')`.
  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
    declare_namespace(pkg)

../opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pkg_resources/__init__.py:2871
  /opt/miniconda3/envs/testbed/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('pydap.tests')`.
  Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
    declare_namespace(pkg)

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
==================================== PASSES ====================================
=========================== short test summary info ============================
PASSED xarray/tests/test_merge.py::test_broadcast_dimension_size
PASSED xarray/tests/test_merge.py::test_assert_compat_valid
PASSED xarray/tests/test_merge.py::test_ordered_default_dict
PASSED xarray/tests/test_merge.py::test_merge_variables
PASSED xarray/tests/test_merge.py::test_expand_variable_dicts
PASSED xarray/tests/test_merge.py::test_determine_coords
PASSED xarray/tests/test_merge.py::test_merge_coords_for_inplace_math
PASSED xarray/tests/test_merge.py::test_merge_core
======================== 8 passed, 34 warnings in 3.49s ========================

